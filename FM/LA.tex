
\chapter{Lineare Algebra}
\section{Grundbegriffe}
\subsection{Norm}\index{Norm}
\begin{Definition}
Eine Abbildung $v\mapsto\|v\|$ von einem
$\K$-Vektorraum $V$ in die nichtnegativen reellen Zahlen heißt
\emdef{Norm}, wenn für alle $v,w\in V$ und $a\in\K$
die drei Axiome%
\begin{gather}
\|v\|=0 \implies v=0,\\
\|av\| = |a|\,\|v\|,\\
\|v+w\| \le \|v\|+\|w\|
\end{gather}
erfüllt sind.
\end{Definition}

Eigenschaften:
\begin{gather}
\|v\|=0\iff v=0,\\
\|-v\|=\|v\|,\\
\|v\|\ge 0.
\end{gather}
Umgekehrte Dreiecksungleichung:
\begin{equation}
|\|v\|-\|w\||\le \|v-w\|.
\end{equation}

\subsection{Skalarprodukt}\index{Skalarprodukt}
\subsubsection{Axiome}
Axiome für $v,w$ aus einem reellen Vektorraum und $\lambda$ ein Skalar:
\begin{gather}
\langle v,w\rangle = \langle w,v\rangle,\\
\langle v,\lambda w\rangle = \lambda\langle v,w\rangle,\\
\langle v,w_1+w_2\rangle = \langle v,w_1\rangle +\langle v,w_2\rangle,\\
\langle v,v\rangle\ge 0,\\
\langle v,v\rangle=0 \iff v=0.
\end{gather}
Axiome für $v,w$ aus einem komplexen Vektorraum und $\lambda$ ein Skalar:
\begin{gather}
\langle v,w\rangle = \overline{\langle w,v\rangle},\\
\langle \lambda v,w\rangle = \overline{\lambda}\langle v,w\rangle,\\
\langle v,\lambda w\rangle = \lambda\langle v,w\rangle,\\
\langle v,w_1+w_2\rangle = \langle v,w_1\rangle +\langle v,w_2\rangle,\\
\langle v,v\rangle\ge 0,\\
\langle v,v\rangle=0 \iff v=0.
\end{gather}

\subsubsection{Eigenschaften}
Das reelle Skalarprodukt ist eine symmetrische bilineare Abbildung.

\subsubsection{Winkel und Längen}
\begin{Definition}
Der Winkel $\varphi$ zwischen $v$ und $w$
ist definiert durch die Beziehung:
\begin{equation}
\langle v,w\rangle = \|v\|\,\|w\|\,\cos\varphi.
\end{equation}
\end{Definition}
\begin{Definition}
Orthogonal:\index{Orthogonal}
\begin{equation}
v\perp w \;:\Longleftrightarrow\; \langle v,w\rangle=0.
\end{equation}
Ein Skalarprodukt $\langle v,w\rangle$ induziert die Norm
\begin{equation}
\|v\| := \sqrt{\langle v,v\rangle}.
\end{equation}
\end{Definition}

\subsubsection{Orthonormalbasis}
\index{Orthogonalsystem}\index{Orthogonalbasis}
\index{Orthonormalsystem}\index{Orthonormalbasis}
Sei $B=(b_k)_{k=1}^n$ eine Basis eines endlichdimensionalen
Vektorraumes.

\begin{Definition}
Gilt $\langle b_i,b_j\rangle=0$
für alle $i,j$ mit $i\ne j$, so wird $B$ \emdef{Orthogonalbasis}
genannt. Ist $B$ nicht unbedingt eine Basis, so spricht man von einem 
\emdef{Orthogonalsystem}.
\end{Definition}
\begin{Definition}
Ist $B$ eine Orthogonalbasis und gilt
zusätzlich $\langle b_k,b_k\rangle=1$ für alle $k$, so wird
$B$ \emdef{Orthonormalbasis} (ONB) genannt. Ist $B$ nicht unbedingt
eine Basis,  so spricht man von einem \emdef{Orthonormalsystem}.
\end{Definition}

Sei $v=\sum_k v_kb_k$ und $w=\sum_k w_kb_k$.
Mit $\sum_k$ ist immer $\sum_{k=1}^n$ gemeint.

Ist $B$ eine Orthonormalbasis, so gilt:
\begin{equation}
\langle v,w\rangle = \sum_k \overline{v_k}\,w_k.
\end{equation}
Ist $B$ nur eine Orthogonalbasis, so gilt:
\begin{equation}
\langle v,w\rangle = \sum_k \langle b_k,b_k\rangle \overline{v_k}\,w_k
\end{equation}
Allgemein gilt:
\begin{equation}
\langle v,w\rangle = \sum_{i,j} g_{ij} \overline{v_i}\,w_j
\end{equation}
mit $g_{ij}=\langle b_i,b_j\rangle$. In reellen Vektorräumen
ist die komplexe Konjugation wirkungslos und kann somit entfallen.

Ist $B$ eine Orthogonalbasis und $v=\sum_k v_k b_k$, so gilt:
\begin{equation}
v_k = \frac{\langle b_k,v\rangle}{\langle b_k,b_k\rangle}.
\end{equation}
Ist $B$ eine Orthonormalbasis, so gilt speziell:
\begin{equation}
v_k = \langle b_k,v\rangle.
\end{equation}


\subsubsection{Orthogonale Projektion}
Orthogonale Projektion von $v$ auf $w$:
\begin{equation}
P[w](v) := \frac{\langle v,w\rangle}{\langle w,w\rangle}\,w.
\end{equation}
\subsubsection{Gram-Schmidt-Verfahren}
Für linear unabhängige Vektoren $v_1,\ldots,v_n$
wird durch%
\begin{equation}
w_k := v_k - \sum_{i=1}^{k-1} P[w_i](v_k)
\end{equation}
ein Orthogonalsystem $w_1,\ldots,w_n$ berechnet.

Speziell für zwei nicht kollineare Vektoren $v_1,v_2$ gilt
\begin{gather}
w_1=v_1,\\
w_2=v_2-P[w_1](v_2).
\end{gather}

\section{Matrizen}\index{Matrix}
\subsection{Quadratische Matrizen}\index{quadratische Matrix}
Eine quadratiche Matrix $A=(a_{ij})$ heißt symmetrisch,
falls gilt $a_{ij}=a_{ji}$ bzw. $A^T=A$.

Jede reelle symmetrische Matrix besitzt ausschließlich reelle
Eigenwerte und die algebraischen Vielfachheiten stimmen mit den
geometrischen Vielfachheiten überein.

Jede reelle symmetrische Matrix $A$ ist diagonalisierbar, d.\,h. es gibt
eine invertierbare Matrix $T$ und eine Diagonalmatrix $D$, so dass
$A=TDT^{-1}$ gilt.

Sei $V$ ein $K$-Vektorraum und $(b_k)_{k=1}^n$ eine Basis von $V$.
Für jede symmetrische Bilinearform $f\colon V^2\to K$ ist die
Darstellungsmatrix
\begin{equation}
A = (f(b_i,b_j))
\end{equation}
symmetrisch. Ist $A\in K^{n\times n}$ eine symmetrische Matrix, so
ist
\begin{equation}\label{eq:symmBf}
f(x,y) = x^T A y.
\end{equation}
eine symmetrische Bilinearform für  $x,y\in K^n$.
Ist $K=\R$ und $A$ positiv definit, so ist
\eqref{eq:symmBf} ein Skalarprodukt auf $\R^n$.

\subsection{Determinanten}\index{Determinante}
Für Matrizen $A,B\in K^{n\times n}$ und $r\in K$ gilt:
\begin{gather}
\det(AB) = \det(A)\det(B),\\
\det(A^T) = \det(A),\\
\det(rA) = r^n\det(A),\\
\det(A^{-1}) = \det(A)^{-1}.
\end{gather}
Für eine Diagonalmatrix $D=\diag(d_1,\ldots,d_n)$ gilt:
\begin{gather}
\det(D) = \prod_{k=1}^n d_k.
\end{gather}
Eine linke Dreiecksmatrix ist eine Matrix der Form
$(a_{ij})$ mit $a_{ij}=0$ für $i<j$. Eine rechte
Dreiecksmatrix ist die Transponierte einer linken
Dreiecksmatrix.

Für eine linke oder rechte Dreiecksmatrix $A=(a_{ij})$ gilt:
\begin{gather}
\det(A) = \prod_{k=1}^n a_{kk}.
\end{gather}

\subsection{Eigenwerte}
\strong{Eigenwertproblem:}\index{Eigenwert}
Für eine gegebene quadratische Matrix $A$ bestimme
\begin{equation}
\{(\lambda,v)\mid Av = \lambda v,\,v\ne 0\}.
\end{equation}
Das homogene lineare Gleichungssystem
\begin{equation}
Av=\lambda v \iff (A-\lambda E_n)v=0
\end{equation}
besitzt Lösungen $v\ne 0$ gdw.
\begin{equation}
p(\lambda)=\det(A-\lambda E_n)=0.
\end{equation}
Bei $p(\lambda)$ handelt es sich um ein normiertes Polynom
vom Grad $n$, das \emdef{charakeristisches Polynom}
\index{charakteristisches Polynom}
genannt wird.

\strong{Eigenraum:}\index{Eigenraum}
\begin{equation}
\Eig(A,\lambda):=\{v\mid Av=\lambda v\}.
\end{equation}
Die Dimension $\dim\Eig(A,\lambda)$ wird
\emdef{geometrische Vielfachheit}\index{geometrische Vielfachheit}
von $\lambda$ genannt.

\strong{Spektrum:}\index{Spektrum}
\begin{equation}
\sigma(A) := \{\lambda\mid \exists v\ne 0\colon Av=\lambda v\}.
\end{equation}

\section{Lineare Gleichungssysteme}
\index{lineares Gleichungssytem}
Ein lineares Gleichungssystem mit $m$ Gleichungen und $n$ Unbekannten
hat die Form:
\begin{equation}\label{eq:LGS}
\begin{split}
a_{11} x_1 + a_{12} x_2 + \ldots + a_{1n} x_n &= b_1,\\
a_{21} x_1 + a_{22} x_2 + \ldots + a_{2n} x_n &= b_2,\\
&\;\;\vdots\\
a_{m1} x_1 + a_{m2} x_2 + \ldots + a_{mn} x_n &= b_n.
\end{split}
\end{equation}
Das System lässt sich durch
\begin{equation}
A:=\begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1n}\\
a_{21} & a_{22} & \ldots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{m1} & a_{m1} & \ldots & a_{mn}
\end{bmatrix}
\end{equation}
und
\begin{equation}
x:=\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix},\quad
b:=\begin{bmatrix}
b_1 \\ b_2 \\ \vdots \\ b_n
\end{bmatrix}
\end{equation}
zusammenfassen.

Äquivalente Matrixform von \eqref{eq:LGS}:
\begin{equation}
Ax=b.
\end{equation}
Erweiterte Koeffizientenmatrix:\index{erweiterte Koeffizientenmatrix}
\begin{equation}
(A\,|\,b) := \left[\begin{array}{ccc|c}
a_{11} & \ldots & a_{1n} & b_1\\
\vdots & \ddots & \vdots & \vdots\\
a_{m1} & \ldots & a_{mn} & b_n
\end{array}\right].
\end{equation}
Lösungskriterium:
\begin{equation}
\exists x[Ax=b] \iff \rg(A)=\rg(A\,|\,b).
\end{equation}
Eindeutige Lösung (bei $n$ Unbekannten):
\begin{equation}
\exists! x[Ax=b] \iff\rg(A)=\rg(A\,|\,b)=n.
\end{equation}
Im Fall $m=n$ gilt:
\begin{equation}
\begin{split}
&\exists! x[Ax=b] \iff A\in\operatorname{GL}(n,K)\\
&\iff \rg(A)=n \iff \det(A)\ne 0.
\end{split}
\end{equation}

\section{Analytische Geometrie}
\subsection{Geraden}\index{Gerade}
\subsubsection{Parameterdarstellung}
\index{Parameterdarstellung!einer Geraden}

\strong{Punktrichtungsform:}\index{Punktrichtungsform}
\begin{equation}
p(t) = p_0+t\underline v,
\end{equation}
$p_0$: Stützpunkt, $\underline v$: Richtungsvektor.
Die Gerade ist dann die Menge $g=\{p(t)\mid t\in\R\}$.

Der Vektor $\underline v$ repräsentiert außerdem die Geschwindigkeit,
mit der diese Parameterdarstellung durchlaufen wird:
$p'(t)=\underline v$.

\strong{Gerade durch zwei Punkte:}
Sind zwei Punkte $p_1,p_2$ mit $p_1\ne p_2$ gegeben, so ist
durch die beiden Punkte eine Gerade gegeben. Für diese Gerade ist
\begin{equation}
p(t) = p_1+t(p_2-p_1)
\end{equation}
eine Punktrichtungsform\index{Punktrichtungsform}.
Durch Umformung ergibt sich die \strong{Zweipunkteform:}
\begin{equation}\label{eq:Zweipunkteform}
p(t) = (1-t)p_1+tp_2.
\end{equation}
Bei \eqref{eq:Zweipunkteform} handelt es sich um eine
Affinkombination. Gilt $t\in[0,1]$, so ist \eqref{eq:Zweipunkteform}
eine Konvexkombination: eine Parameterdarstellung für die Strecke
von $p_1$ nach $p_2$.

\subsubsection{Parameterfreie Darstellung}
\strong{Hesse-Form:}
\begin{equation}\label{eq:Hesse-Form}
g = \{p\mid\langle \uv n,p-p_0\rangle = 0\},
\end{equation}
$p_0$: Stützpunkt, $\uv n$: Normalenvektor.

Die Hesse-Form ist nur in der Ebene möglich.
Form \eqref{eq:Hesse-Form} hat in Koordinaten
die Form
\begin{equation}
\begin{split}
g &= \{(x,y)\mid n_x(x-x_0)+n_y(y-y_0)=0\}\\
&= \{(x,y)\mid n_x x+n_y y = n_x x_0+n_y y_0\}.
\end{split}
\end{equation}

\strong{Hesse-Normalform:} \eqref{eq:Hesse-Form} mit $|\uv n|=1$.


Sei $\uv v\wedge\uv w$ das äußere Produkt.

\strong{Plückerform:}
\begin{equation}
g = \{p\mid (p-p_0)\wedge \underline v=0\}.
\end{equation}
Die Größe $\underline m = p_0\wedge\underline v$ heißt Moment.
Beim Tupel $(\underline v:\underline m)$ handelt es sich um
Plückerkoordinaten für die Gerade.

In der Ebene gilt speziell:
\begin{equation}\label{eq:Gerade-Ebene}
g = \{(x,y)\mid (x-x_0)\Delta y = (y-y_0)\Delta x\}
\end{equation}
mit $\underline v=(\Delta x,\Delta y)$.

Sei $a:=\Delta y$ und $b:=-\Delta x$ und $c:=ax_0+by_0$.
Aus \eqref{eq:Gerade-Ebene} ergibt sich:
\begin{equation}
g = \{(x,y)\mid ax+by=c\}.
\end{equation}
Im Raum ergibt sich ein Gleichungssystem:
\begin{equation}
g = \{\begin{pmatrix}x\\ y\\ z\end{pmatrix}
\mid
\begin{vmatrix}
(x-x_0)\Delta y = (y-y_0)\Delta x\\
(y-y_0)\Delta z = (z-z_0)\Delta y\\
(x-x_0)\Delta z = (z-z_0)\Delta x
\end{vmatrix}\}
\end{equation}
mit $\underline v=(\Delta x,\Delta y,\Delta z)$.

\subsubsection{Abstand Punkt zu Gerade}
Sei $p(t):=p_0+t\underline v$ die Punktrichtungsform einer Geraden und
sei $q$ ein weiterer Punkt. Bei $\underline d(t):=p(t)-q$ handelt
es sich um den Abstandsvektor in Abhängigkeit von $t$.

Ansatz: Es gibt genau ein $t$, so dass gilt:
\begin{equation}
\langle\underline d,\underline v\rangle=0.
\end{equation}
Lösung:
\begin{equation}
t = \frac
  {\langle\underline v,q{-}p_0\rangle}
  {\langle\underline v,\underline v\rangle}.
\end{equation}

\subsection{Ebenen}\index{Ebene}
\subsubsection{Parameterdarstellung}
\index{Parameterdarstellung!einer Ebene}
Seien $\uv u, \uv v$ zwei nicht kollineare Vektoren.

Punktrichtungsform:
\begin{equation}\label{eq:Ebene-Punktrichtungsform}
p(s,t) = p_0+s\uv u+t\uv v.
\end{equation}

\subsubsection{Parameterfreie Darstellung}
Seien $\uv v, \uv w$ zwei nicht kollineare Vektoren.
Durch
\begin{equation}
E = \{p\mid (p-p_0)\wedge\uv v\wedge\uv w=0\}.
\end{equation}
wird eine Ebene beschrieben.

\strong{Hesse-Form:}
\begin{equation}
E = \{p\mid \langle\uv n,p-p_0\rangle=0\},
\end{equation}
$p_0$: Stützpunkt, $\uv n$: Normalenvektor. Die Hesse-Form einer
Ebene ist nur im dreidimensionalen Raum möglich.
Den Normalenvektor bekommt man aus \eqref{eq:Ebene-Punktrichtungsform}
mit $\uv n = \uv u\times\uv v$.

\subsubsection{Abstand Punkt zu Ebene}
Sei $p(s,t):=p_0+s\uv u+t\uv v$ die Punktrichtungsform einer Ebene
und sei $q$ ein weiterer Punkt. Bei $\uv d(s,t):=p-q$ handelt es sich um
den Abstandsvektor in Abhängigkeit von $(s,t)$.

Ansatz: Es gibt genau ein Tupel $(s,t)$, so dass gilt:
\begin{equation}
\langle\uv d,\uv u\rangle=0\enspace\text{und}\enspace
\langle\uv d,\uv v\rangle=0.
\end{equation}
Lösung: Es ergibt sich ein LGS:
\begin{equation}
\begin{bmatrix}
\langle\uv u,\uv v\rangle & \langle\uv v,\uv v\rangle\\
\langle\uv v,\uv v\rangle & \langle\uv u,\uv v\rangle
\end{bmatrix}
\begin{bmatrix}
s\\ t
\end{bmatrix}
= \begin{bmatrix}
\langle\uv v,q{-}p_0\rangle\\
\langle\uv u,q{-}p_0\rangle
\end{bmatrix}.
\end{equation}
Bemerkung: Die Systemmatrix $g_{ij}$ ist der metrische Tensor für die
Basis $B=(\uv u,\uv v)$. Die Lösung des LGS ist:
\begin{gather}
s = \frac
  {\langle g_{12}\uv v-g_{12}\uv u, q{-}p_0\rangle}
  {g_{11}^2-g_{12}^2},\\
t = \frac
  {\langle g_{12}\uv u-g_{12}\uv v, q{-}p_0\rangle}
  {g_{11}^2-g_{12}^2}.
\end{gather}

