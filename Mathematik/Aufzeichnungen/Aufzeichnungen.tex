\documentclass[a4paper,10pt,fleqn,twocolumn,twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{ngerman}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\definecolor{c1}{RGB}{00,40,80}
\usepackage[colorlinks=true,linkcolor=c1]{hyperref}
\usepackage{geometry}
\geometry{a4paper,left=25mm,right=10mm,top=24mm,bottom=32mm}
\setlength{\columnsep}{4mm}
\numberwithin{equation}{section}

\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\R}{\mathbb R}
\newcommand{\ee}{\mathrm e}

\begin{document}
\section*{Aufzeichnungen\\
zur Mathematik}
John Smith, 2016\\
Lizenz: Creative Commons CC0

\tableofcontents

\section{Logik}
\subsection{Zerlegung des Allquantors}
Eine periodische Funktion $f$ erfüllt die Gleichung
\begin{equation}
f(x) = f(x+T)
\end{equation}
definiert. Macht man eine Substitution $x:=u-T$, so ergibt sich
\begin{equation}
f(u-T) = f((u-T)+T) = f(u).
\end{equation}
Somit gilt auch $f(x)=f(x-T)$. Diese Folgerung war sehr kurz,
eigentlich zu kurz um wissenschaftlich zu sein.

Eine Funktion $f{:}\;\mathbb R\rightarrow\mathbb R$
heißt periodisch mit Periode $T$, falls die
Funktionalgleichung
\begin{equation}
\forall x{\in}\mathbb R{:}\;\; f(x) = f(x+T)
\end{equation}
erfüllt ist. Für die Gleichung ergibt sich zunächst
der folgende abstrakte Syntaxbaum (AST):
\begin{verbatim}
  (=
      (f x)
      (f (+ x T)))
\end{verbatim}
Aber was ist mit dem Allquantor? Der Allquantor ist eigentlich
eine Funktion in zwei Variablen. Das erste Argument ist eine Menge $M$,
das zweite ein Prädikat $p$. Der Allquantor überprüft nun, ob das
Prädikat $p$ für alle Elemente von $M$ gültig ist. Wir wandeln nun
die Schreibweise durch
\begin{equation}
\forall x{\in}M{:}\;\; p(x)\quad\longrightarrow\quad
\mathrm{all}(M,p)
\end{equation}
um. Die Funktionalgleichung wird als nun in der Form
\begin{equation}\label{periodisch-all}
\mathrm{all}(\mathbb R,\lambda x. f(x) = f(x+T))
\end{equation}
dargestellt. Dazu gehört folgender AST:
\begin{verbatim}
  (all IR
      (lambda (x)
          (=
              (f x)
              (f (+ x T)))))
\end{verbatim}
Unter einer \textit{freien Substitution} versteht man nun den
Austausch jedes Vorkommens einer \textit{freien Variable}
durch einen AST. Durch den $\lambda$-Term wird die
Variable $x$ nun gebunden, ist also nicht mehr frei.
Daher können wir nicht einfach eine freie Substitution
$[x{:=}u{-}T]$ durchführen.

Viel mehr gilt die folgende Regel. Ist $g$ eine passende
Bijektion, so gilt
\begin{equation}\label{all-Umformung}
\mathrm{all}(M,p) = \mathrm{all}(g^{-1}(M),\;p\circ g).
\end{equation}
Z.B. wählt man $M:=\{1,2,3\}$ und $p(x):=(x{<}4)$
sowie $g(x):=x/2$. Somit ergibt sich
\begin{gather*}
(\forall x{\in}\{1,2,3\}{:}\;\; x<4)\\
= (\forall x{\in}\{2,4,6\}{:}\;\; x/2<4).
\end{gather*}
Sei nun $g(x):=x-T$. Nun ist $g^{-1}(\mathbb R)=\mathbb R$.
Wendet man das nun auf (\ref{periodisch-all}) an, so ergibt
sich
\begin{equation}
\mathrm{all}(\mathbb R, \lambda x.f(x-T)=f(x)).
\end{equation}
Es handelt sich also eigentlich nicht um eine Substitution,
sondern um die Verkettung des Prädikats mit einer Funktion.
Sicherlich lässt sich eine solche durch eine Substitution
darstellen, aber das ist sehr schwammig. Und diese Schwammigkeit
fällt erst auf, wenn der Allquantor auch ausgeschrieben wird.
Im Allquantor ist ja, wie wir gesehen haben, eine Variablenbindung
enthalten.

Doch warum darf man (\ref{all-Umformung}) eigentlich anwenden?
Nun, hierzu zerlegen wir den Allquantor weiter. Es ist nämlich
\begin{equation}
\mathrm{all}(M,p) = (p(M)=\{\mathrm{true}\}).
\end{equation}
Sei $\mathrm{id}$ die identische Funktion. Nun ist aber
$p$ das selbe wie $p\circ \mathrm{id}$. Außerdem gilt ja
$\mathrm{id} = g\circ g^{-1}$. Somit ergibt sich
\begin{equation}
\begin{split}
& p(M) = (p\circ g\circ g^{-1})(M)\\
& = (p\circ g)(g^{-1}(M)).
\end{split}
\end{equation}
In der letzten Gleichung wurde $(f\circ g)(M)=f(g(M))$
verwendet. Das soll noch schnell gezeigt werden. Es gilt
\begin{equation}
\begin{split}
&(f\circ g)(M)\\
&= \{(f\circ g)(x)|\;x\in M\}\\
&= \{f(y)|\; y=g(x) \wedge x\in M\}\\
&= \{f(y)|\; y\in g(M)\}
= f(g(M)).
\end{split}
\end{equation}
Nun gut, das ist auch ein Pseudobeweis. Wir können hier noch
\begin{equation}
\bigcup_{i\in I} f(A_i)
= f(\bigcup_{i\in I} A_i)
\end{equation}
verwenden. Dann gilt aber
\begin{gather*}
(f\circ g)(M) = \bigcup_{x\in M} \{f(g(x))\}
= \bigcup_{x\in M} f(\{g(x)\})\\
= f(\bigcup_{x\in M}\{g(x)\})
= f(g(M)).
\end{gather*}

Für den Existenzquantor gilt analog
\begin{equation}
\exists x{\in}A{:}\;p(x)\quad\text{gdw.}\quad\mathrm{true}\in p(A).
\end{equation}
Für die Beschreibung von Mengen gilt
außerdem
\begin{equation}
\begin{split}
&\{x{\in}A|\;p(x)\} = \mathrm{filter}(A,p)\\
&= p^{-1}(\{\mathrm{true}\}).
\end{split}
\end{equation}
Die Beschreibung von Mengen lässt sich also als
Urbildmenge darstellen.

Für den Anzahlquantor gilt außerdem
\begin{equation}
\exists^{=n} x{\in}A{:}\; p(x)
\quad\text{gdw.}\quad \#\mathrm{filter}(A,p)=n.
\end{equation}

\subsection[Allquantifizierung über Produktmengen]%
{Allquantifizierung über\\Produktmengen}
Sei $M:=A\times B$. Sei $M_x:=\pi_1^{-1}(\{x\})$, das ist die
Faser von $x$ für die Projektion $\pi_1(x,y):=x$. Es gilt nun
\begin{equation}
M=\bigcup_{x\in A} M_x.
\end{equation}
Es gilt außerdem (was zu zeigen ist)
\begin{equation}\label{eq:forall-union}
\forall t{\in}\bigcup_{i\in I} A_i\,[P(t)]
\iff\forall i{\in}I\,\forall t{\in}A_i\,[P(t)].
\end{equation}
Daher ist
\begin{equation}
\begin{split}
&\forall (x,y){\in}M\,[P(x,y)]\\
&\iff\forall x{\in}A\,\forall (x,y){\in}M_x\,[P(x,y)].
\end{split}
\end{equation}
Wegen
\begin{equation}
\begin{split}
M_x &= \{x\}\times B\\
&= \{(x,y)\mid x\in\{x\}\land y\in B\}\\
&= \{(x,y)\mid y\in B\}
\end{split}
\end{equation}
gilt nun
\begin{equation}
(x,y)\in M_x\iff y\in B.
\end{equation}
Somit gilt
\begin{equation}
\begin{split}
&\forall (x,y){\in}M\,[P(x,y)]\\
&\iff\forall x{\in}A\;\forall y{\in}B\,[P(x,y)].
\end{split}
\end{equation}
Verwendet man $\pi_2$ anstelle von $\pi_1$, so gelangt man
zu der Erkenntnis $\forall x\forall y\iff\forall y\forall x$.

Betrachten wir nun \eqref{eq:forall-union} für endliche
Zerlegungen. Sei dazu $I(n):=\{1,...,n\}$ und $U_n:=U_{n-1}\cup A_n$
mit Anfang $U_1:=A_1$. Nun gilt $U_n=\bigcup_{i\in I(n)}A_i$.

Es ist nun
\begin{equation}
\begin{split}
&\forall t\in U_n\,[P(t)]\\
&\iff\forall t\in U_{n-1}\cup A_n\,[P(t)]\\
&\iff\forall t\in U_{n-1}\,[P(t)]\;\land\;\forall t\in A_n\,[P(t)]
\end{split}
\end{equation}
Falls \eqref{eq:forall-union} für $n-1$ gilt (Induktionsvoraussetzung),
so ist
\begin{equation}
\begin{split}
& \forall t\in U_{n-1}\,[P(t)]\\
& \iff\forall i{\in}I(n-1)\;\forall t{\in}A_i\,[P(t)].
\end{split}
\end{equation}
Zusammenführen, d.\,h.
\begin{equation}
\begin{split}
& \forall t{\in}A_n\,[P(t)]\;\land\;
\forall i{\in}I(n-1)\;\forall t{\in}A_i\,[P(t)]\\
&\iff\forall i{\in}I(n)\;\forall t{\in}A_i\,[P(t)],
\end{split}
\end{equation}
bringt \eqref{eq:forall-union} für $n$.
Die Induktion beginnen wir zunächst bei $n=1$, um möglichen
Problemen mit der leeren Menge aus dem Weg zu gehen.

Im allgemeinen Fall verwenden wir zunächst die prädikatenlogische
Definition der Vereinigungsmenge:
\begin{equation}\label{eq:union-def}
\bigcup_{i\in I}A_i := \{x\mid\exists i{\in}I\,[x\in A_i]\}.
\end{equation}
Nun gilt
\begin{equation}
\begin{split}
&\forall t\in\bigcup_{i\in I} A_i\,[P(t)]\\
&\iff\forall t[t\in\bigcup_{i\in I}A_i\implies P(t)]\\
&\iff\forall t[\exists i\,[t\in A_i]\implies P(t)]\\
&\iff\forall t[\forall i\,[t\in A_i\implies P(t)]]\\
&\stackrel{?!}{\iff}\forall i\,\forall t[t\in A_i\implies P(t)]\\
&\iff\forall i\forall t{\in}A_i[P(t)].
\end{split}
\end{equation}
Was noch zu zeigen ist, ist die Regel
\begin{equation}
\forall x\forall y\,[P(x,y)]
\iff \forall y\forall x\,[P(x,y)].
\end{equation}


\newpage
\section{Analysis}
\subsection{Halley-Verfahren}
Im Folgenden wird die Herleitung des Halley"=Verfahrens unter
Verwendung einer Padé"=Approximation dargestellt.
Die Padé"=Approximation \(R[m,0]\) ist die Taylorreihe. Sei
\begin{equation}
a_k = \frac{1}{k!} f^{(k)}(x_0).
\end{equation}
Die Taylorreihe ist
\begin{equation}
R[m,0] = \sum_{k=0}^m a_k(x-x_0)^k.
\end{equation}
Sei\\
\(A=\begin{vmatrix}
a_{m-n+1} & a_{m-n+2} & \ldots & a_{m+1}\\
\ldots & \ldots & \ldots & \ldots\\
a_m & a_{m+1} & \ldots & a_{m+n}\\
\sum\limits_{i=n}^m a_{i-n}x^i &
\sum\limits_{i=n-1}^m a_{i-n+1}x^i
&\ldots & \sum\limits_{i=0}^m a_i x^i
\end{vmatrix}\)\\
und\\
\(B=\begin{vmatrix}
a_{m-n+1} & a_{m-n+2} & \ldots & a_{m+1}\\
\ldots &\ldots &\ldots &\ldots\\
a_m & a_{m+1} &\ldots & a_{m+n}\\
x^n & x^{n-1} &\ldots & x^0
\end{vmatrix}\).\\
Es ist \(R[m,n]=A/B\).
Ersetze dann \(x\) gegen\\
\(x-x_0\). Es ist
\begin{equation}
\begin{split}
& R[1,1] = \frac{a_1(a_0+a_1 x)-a_0a_2x}{a_1-a_2x}\\
& = \frac{a_0a_1+(a_1^2-a_0a_2)x}{a_1-a_2x}.
\end{split}
\end{equation}
Die Nullstelle von \(f\) ist gesucht, und die Approximation
muss dort auch ungefähr null sein. Setzt man also \(R[1,1](x)=0\),
so folgt
\begin{equation}
0=a_0a_1+(a_1^2-a_0a_2)x.
\end{equation}
Ersetzt man noch \(x\) gegen \(x-x_0\) und formt danach um,
so erhält man
\begin{equation}
x=x_0-\frac{a_0a_1}{a_1^2-a_0a_2}.
\end{equation}
Ausgeschrieben bekommt man also \(x=\varphi(x_0)\) mit
\begin{equation}
\varphi(x) = x-\frac{2f(x)f'(x)}{2f'(x)^2-f(x)f''(x)}.
\end{equation}
Das Halley"=Verfahren ist dann die Fixpunktiteration
\(x_{n+1}=\varphi(x_n)\).
Man sieht, dass das Halley"=Verfahren in das Newton"=Verfahren
übergeht, wenn man \(f(x)f''(x)\) verschwinden lässt.

\newpage
\subsection{Integral der Potenzfunktion}
Ich will eine ungewöhnliche Methode zur Berechnung des Integrals
\begin{equation}
\int x^n\,\mathrm dx
\end{equation}
vorführen. Wähle die Substitution $x=\ee^u$. Es gilt nun
\begin{equation}
\frac{\mathrm dx}{\mathrm du}=\frac{\mathrm d}{dx}\ee^u=\ee^u
\end{equation}
und somit $\mathrm dx = \ee^u\,\mathrm du$. Man hätte auch
\begin{equation}
x^n = \ee^{\ln(x)n} = \ee^{un}
\end{equation}
rechnen können, was vielleicht eher
\begin{equation}
\frac{\mathrm du}{\mathrm dx} = \frac{\mathrm d}{\mathrm dx}\ln(x) = \frac{1}{x}
\end{equation}
suggeriert hätte. Hier muss man bedenken, dass die Substitution
nochmals ausgeführt werden kann, dass also
\begin{equation}
\frac{1}{x} = \frac{1}{\ee^u}
\end{equation}
gilt. Nach Substitutionsregel ergibt sich nun
\begin{equation}
\begin{split}
&\int x^n\,\mathrm dx = \int \ee^{nu}\ee^u\mathrm du
= \int \ee^{(n+1)u}\,\mathrm du\\
&= \frac{1}{n+1}\ee^{(n+1)u} = \frac{x^{n+1}}{n+1}.
\end{split}
\end{equation}
Es geht noch weiter. Im Fall $n=-1$ gilt
\begin{equation}
\int \frac{1}{x}\,\mathrm dx
= \int\frac{1}{\ee^u}\,\ee^u\mathrm du
= \int\mathrm du = u.
\end{equation}
Aber Umformen der Substitution ergibt ja $u=\ln x$.

\newpage
\section{Lineare Algebra}
\subsection{Hodge-Stern-Operator}
\subsubsection{Orientierung}
Sei $V$ ein $\R$-Vektorraum und sei $B=(b_k)_{k=1}^n$ eine Basis
von $V$, die wir positiv orientiert nennen. Ist $B'$ eine andere
Basis, so gibt es eine Basiswechselmatrix $T_{B'}^B$. Sei nun
\begin{equation}
s := \sgn(\det(T_{B'}^B)).
\end{equation}
Die Basis $B'$ heißt nun \emph{positiv orientiert}, wenn
$s>0$ ist und \emph{negativ orientiert}, wenn $s<0$ ist.

Im Koordinatenraum soll natürlich die Standardbasis positiv
orientiert sein.

Beachte, dass für das äußere Produkt gilt:
\begin{equation}
\bigwedge_{k=1}^n b_k' = \det(T_B^{B'}) \bigwedge_{k=1}^n b_k.
\end{equation}
Angenommen, die Basis $B'$ ist nun eine Permutation von $B=(b_k)$,
also die Festlegung
\begin{equation}
B' := (b_{\sigma(k)})_{k=1}^n, \quad \sigma\in\operatorname{Sym}(n).
\end{equation}
Die Basiswechselmatrix ist nun die Permutationsmatrix und somit gilt:
\begin{equation}
\det(T_{B'}^B) = \sgn(\sigma).
\end{equation}

\subsection{Skalarprodukträume}
Ist $V$ ein Skalarproduktraum und ist
$B=(e_k)_{k=1}^n$ eine Orthonormalbasis von $V$,
so ist der Hodge-Stern-Operator eine lineare Abbildung,
definiert durch%
\begin{equation}
\begin{split}
&*(e_{\sigma(1)}\wedge\ldots\wedge e_{\sigma(k)})\\
&:= \sgn(\sigma)\,s(B)\,e_{\sigma(k+1)}\wedge\ldots\wedge e_{\sigma(n)}
\end{split}
\end{equation}
mit $*\colon \Lambda^k(V)\to\Lambda^{n-k}(V)$,
wobei $0\le k\le n$ gilt.

Dabei ist $s(B)=+1$, wenn $B$ positiv orientiert ist,
und $s(B)=-1$, wenn $B$ negativ orientiert ist.

Sei $B=(b_k)$ nun eine Orthogonalbasis. Man definiert nun
$g_{ij}:=\langle b_i,b_j\rangle$. Jetzt lassen sich den
Basisvektoren durch
\begin{equation}
e_k := \frac{b_k}{\|b_k\|} = \frac{b_k}{\sqrt{g_{kk}}}
\end{equation}
normierte Basisvektoren zuordnen. Damit ergibt sich
\begin{equation}
\begin{split}
&*(b_{\sigma(1)}\wedge\ldots\wedge b_{\sigma(k)}) =\\
&= r \sgn(\sigma)\,s(B)\,b_{\sigma(k+1)}\wedge\ldots\wedge b_{\sigma(n)}
\end{split}
\end{equation}
mit
\begin{equation}
\begin{split}
r &= \frac{\sqrt{g_{\sigma(1)\sigma(1)}\ldots g_{\sigma(k)\sigma(k)}}}
{\sqrt{g_{\sigma(k+1)\sigma(k+1)}\ldots g_{\sigma(n)\sigma(n)}}}\\
&=\frac{g_{\sigma(1)\sigma(1)}\ldots g_{\sigma(k)\sigma(k)}}{\sqrt{\det g}}.
\end{split}
\end{equation}

\end{document}


