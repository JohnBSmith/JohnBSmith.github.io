\documentclass[a4paper,10pt,fleqn,twocolumn,twoside,dvipdfmx]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{microtype}

% Bugfix. Siehe https://komascript.de/node/2295
\unsettoc{toc}{onecolumn}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{mdframed}

\usepackage{libertine}
\usepackage[libertine,smallerops]{newtxmath}
\usepackage[scaled=0.8]{DejaVuSans}

\usepackage[automark]{scrlayer-scrpage}
\ohead{\pagemark}
\ihead{\headmark}
\chead{}
\cfoot{}
\ofoot{}

\usepackage{color}
\definecolor{c1}{RGB}{00,00,00}
\usepackage[colorlinks=true,linkcolor=c1]{hyperref}

\usepackage{geometry}
\geometry{a4paper,left=24mm,right=14mm,top=23mm,bottom=27mm}
\setlength{\columnsep}{5mm}
% \addtokomafont{disposition}{\rmfamily}

\newtheoremstyle{rmbox}%
  {0pt}% space above
  {0pt}% space below
  {}% bodyfont
  {}% indent
  {\sffamily\bfseries}% head font
  {\;}% punctuation between head and body
  {0pt}% space after theorem head
  {\thmname{#1}\thmnote{\;(#3)}.}

\theoremstyle{rmbox}
\newtheorem{Rezept}{Rezept}[section]

\definecolor{boxmargin}{rgb}{0.4,0.2,0.1}
\definecolor{boxbg}{rgb}{0.95,0.9,0.7}

\surroundwithmdframed[topline=false,rightline=false,bottomline=false,%
  linecolor=boxmargin, backgroundcolor=boxbg,
  linewidth=3.5pt, innerleftmargin=4pt,%
  innertopmargin=3pt, innerbottommargin=4pt,%
  innerrightmargin=6pt%
]{Rezept}

\newcommand{\strong}[1]{\textsf{\textbf{#1}}}
\newcommand{\bv}[1]{\mathbf{#1}}
\newcommand{\ee}{\mathrm{e}}
\newcommand{\ui}{\mathrm{i}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Q}{\mathbb Q}
\newcommand{\R}{\mathbb R}
\newcommand{\C}{\mathbb C}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\defiff}{\;:\Longleftrightarrow\;}

\begin{document}

\section*{\LARGE Rezepte zur Mathematik}

\tableofcontents

\clearpage
\section{Grundlagen}

\subsection{Natürliches Schließen}

\begin{Rezept}
Natürliche Schließen systematisiert gewohnte Beweisführung.
\end{Rezept}
\strong{Beispiel 1.}
Wir wollen zeigen, dass das Quadrat einer geraden Zahl ebenfalls
gerade ist. Die Definition
\[2\mid a \defiff \exists n\in\Z\colon a = 2n.\]
bringt die Behauptung zunächst in die Form
\[(\exists n\in\Z\colon a = 2n) \implies (\exists n'\in\Z\colon a^2 = 2n').\]
Nun führen wir den Beweis mittels natürlichem Schließen. Aufgrund
der Prämisse haben wir einen Zeugen $n\in\Z$, über den nichts
weiter bekannt ist, außer dass $a=2n$. Sei $n':=2n^2$. Wir dürfen
rechnen
\[a^2 = (2n)^2 = 4n^2 = 2n'.\]
Mit $n'$ wurde ein Zeuge für die rechte Existenzaussage konstruiert.
Somit wurde die Konklusion aus der Prämsse abgeleitet. Der Beweis
ist erbracht.

\section{Lineare Algebra}

\subsection{Basiswechsel}

\begin{Rezept}
Umständliche Rechnungen zum Basiswechsel kann man mit der
Gleichung $B\bv v_B = A\bv v_A$ zu einer kurzen Matrizenrechnung
reduzieren.
\end{Rezept}

\noindent
Sei $K$ ein Körper und $V$ ein Vektorraum über $K$.
Eine Basis $B=(\bv b_1,\ldots, \bv b_n)$ von $V$ induziert die
bijektive lineare Abbildung
\[\Phi_B\colon K^n\to V,\quad \Phi_B(\bv e_k) := \bv b_k,\]
die ihrerseits die Basis eindeutig charakterisiert. Mit
$E=(\bv e_1,\ldots,\bv e_n)$ ist hierbei die Standardbasis gemeint.
Man bezeichnet $\Phi_B$ als das durch $B$ induzierte Koordinatensystem.
Es ordnet jedem Koordinatenvektor $\bv v_B$ eins"=zu"=eins einen Vektor
$\bv v = \Phi_B(\bv v_B)$ zu.

Für $V=K^n$ kann man $\Phi_B=B$ setzen, indem $B$ als die Matrix
betrachtet wird, deren Spalten die Basisvektoren $\bv b_k$ sind,
und zwar genau auf die Weise, wie $B$ als geordnete Basis angegeben
wurde.

Seien nun zwei Basen $A,B$ des $K^n$ gegeben. Aufgabe ist es, den
Koordinatenvektor $\bv v_A$ bezüglich Basis $B$ darzustellen.
Man erhält die Lösung gemäß
\[\bv v = \Phi_B(\bv v_B) = \Phi_A(\bv v_A) \iff
\bv v_B = \Phi_B^{-1}(\Phi_A(\bv v_A)),\]
bzw. als Matrizenrechnung
\[\bv v = B\bv v_B = A\bv v_A\iff
\bv v_B = B^{-1}A\bv v_A.\]
Die Verkettung $T_B^A := B^{-1}A = \Phi_B^{-1}\circ\Phi_A$
ist immer eine lineare Abbildung zwischen
Koordinatenräumen, und somit unter allen Umständen als Matrix
darstellbar. Man bezeichnet sie als \emph{Transformationsmatrix}.

Mit diesen Überlegungen ist eigentlich auch bereits fast alles
Wichtige gesagt, um Darstellungsmatrizen und ihre Transformation
bei Basiswechsel verstehen zu können.

\newpage
\subsection{Darstellungsmatrizen}
\begin{Rezept}
Beim Basiswechsel ist die Darstellungsmatrix zu transformieren,
was man mittels Matrizenrechnung kurzfassen kann.
\end{Rezept}

\noindent
Gegeben seien Basen $A,B$ von $V$ und $C,D$ von $W$. Es gilt%
\begin{gather*}
\bv v = A\bv v_A = B\bv v_B,\\
\bv w = C\bv w_C = D\bv w_D.
\end{gather*}
Für eine lineare Abbildung $f\colon V\to W$ gilt%
\begin{align*}
\bv w = f(\bv v)&\iff C\bv w_C = f(A\bv v_A)\\
&\iff \bv w_C = (C^{-1}\circ f\circ A)\bv v_A.
\end{align*}
Die Darstellungsmatrix definiert man daher als%
\[M_C^A(f) := C^{-1}\circ f\circ A.\]
Man betrachte nun die Kette%
\[\bv v_B\stackrel{A^{-1}B}\longmapsto \bv v_A
\stackrel{M_C^A(f)}\longmapsto \bv w_C
\stackrel{D^{-1}C}\longmapsto \bv w_D.\]
Demzufolge gilt
\[M_D^B(f) = D^{-1}C\,M_C^A(f)A^{-1}B = T_D^C M_C^A(f) T_A^B.\]

\subsection{Lineare Unabhängigkeit}
\begin{Rezept}
Zum Vektorraum $K^n$ ist $B=(\bv b_1,\ldots,\bv b_n)$ genau dann eine
Basis, wenn $\det(B)\ne 0$ gilt.
\end{Rezept}
Das System $B=(\bv b_1,\ldots,\bv b_n)$ ist gemäß Definition genau
dann linear unabhängig, wenn
\[x_1 \bv b_1+\ldots + x_n\bv b_n = 0\implies \forall k\colon x_k = 0.\]
Kompakt geschrieben lautet diese Aussage
\[B\bv x = 0\implies \bv x=0\]
mit $\bv x = (x_1,\ldots,x_n)$. Äquivalent zu ihr ist
\[\operatorname{Kern}(B) = \{0\}.\]
Wir haben andererseits
\[\operatorname{Kern}(B) = \{0\}\iff B\;\text{bijektiv}\iff \det(B)\ne 0.\]
Der Test auf lineare Unabhängigkeit ist damit auf die Berechnung
einer Determinante zurückgeführt, was für ein konkretes $B$
automatisch durchgeführt werden kann.

\newpage
\section{Analysis}

\subsection{Polarkoordinaten}

\begin{Rezept}
Bei der Umrechnung in Polarkoordinaten wird das Auftreten
umständlicher Fallunterscheidungen verhindert,
indem man $x=r\cos\varphi$ nach $\varphi$ umstellt
und das Vorzeichen $s(y)$ hinzufügt.
\end{Rezept}

\noindent
Die Umrechnung von Polarkoordinaten in kartesische Koordinaten
ist mit
\[\begin{pmatrix}x\\ y\end{pmatrix}
= \begin{pmatrix}r\cos\varphi\\ r\sin\varphi\end{pmatrix}\]
mühelos durchführbar. Zur händischen Umrechnung von kartesischen
Koordinaten in Polarkoordinaten nutzt man am besten die Formel
\[\varphi = s(y)\arccos\Big(\frac{x}{r}\Big),\quad r=\sqrt{x^2+y^2},\]
wobei $s(0)=1$ ist und sonst $s(y)=\sgn(y)$ gilt. Auf diese
Weise entgeht man komplizierten Fallunterscheidungen.

\subsection{Ableitungen verifizieren}

\begin{Rezept}
Es ist zuträglich, nach dem Bestimmen der Ableitung im Anschluss
mit einem Plotter oder einem CAS die Probe durchzuführen.
\end{Rezept}

\noindent
Die Ableitung einer reellen Funktion $f$ ist definiert als%
\[f'(x) := \lim_{h\to 0}\frac{f(x+h)-f(x)}{h}.\]
Möchte man erfahren, ob sie unter Anwendung
der Ableitungsregeln richtig ermittelt wurde, kann man die Probe
machen, indem der Differenzenquotient%
\[D_h f(x) = \frac{f(x+h)-f(x)}{h}\]
an einer konkreten Stelle $x$ für ein kleines $h$ numerisch
berechnet und mit $f'(x)$ verglichen wird.

Da dies recht umständlich ist, ist es sinnvoll, diese Aufgabe einem
Funktionenplotter zu überlassen. Numerisch günstiger ist es, den
Differenzenquotient nicht naiv gemäß der Definition zu berechnen,
sondern als%
\[D_h f(x) = \frac{f(x+h)-f(x-h)}{2h}.\]
Mit dem Plotter kann man schließlich
\[10^n (D_h f(x) - f'(x)) \approx 0\]
für $n\in\{0,1,2,\ldots\}$ prüfen.

\newpage
\subsection{Fixpunktiteration}

\begin{Rezept}
Wann immer eine Rekurrenz der Form $x_{n+1} = \varphi(x_n)$
auftritt, betrachte man die Fixpunktgleichung $x = \varphi(x)$. Ob
$x_n$ gegen den Fixpunkt $x$ konvergiert, hängt allerdings von den
Eigenschaften von $\varphi$ ab.
\end{Rezept}

\noindent
Man kann sich die Frage stellen, welchen Wert der endlose
Kettenbruch
\[a+\frac{b}{a+\frac{b}{a+\frac{b}{a+\ldots}}}\]
für feste Zahlen $a,b$ besitzt. Zunächst muss geklärt sein, was
damit gemeint sein soll. Man beginnt bei einem Startwert $x_0$
und betrachtet die Rekurrenz
\[x_{n+1} = a+\frac{b}{x_n}.\]
Wir abstrahieren durch Einführung der Funktion $\varphi$, so dass
\[x_{n+1} = \varphi(x_n)\]
gilt. Wie man mühelos bestätigen kann, handelt es sich bei $\varphi$
für $a>0$ und $b>0$ um eine stetige Selbstabbildung auf $\R_{>0}$.
Nehmen wir nun an, die Folge $(x_n)$ konvergiert gegen eine Zahl $x$.
Dann muss gelten
\begin{align*}
x &= \lim_{n\to\infty} x_{n} = \lim_{n\to\infty} x_{n+1}
= \lim_{n\to\infty}\varphi(x_n)\\
&= \varphi(\lim_{n\to\infty} x_n) = \varphi(x),
\end{align*}
wobei $\varphi$ aufgrund der Stetigkeit aus der Grenzwertbildung
herausgezogen werden durfte. Die Gleichung
\[x = \varphi(x)\]
bezeichnet man als \emph{Fixpunktgleichung}. Ihre Lösungen
bezeichnet man als \emph{Fixpunkte} von $\varphi$. Speziell für den
Kettenbruch ergibt sich die quadratische Gleichung
\[x^2 - ax - b = 0.\]
So erhält man $1+\sqrt{2}$ für $a=2$ und $b=1$.
Für $a=1$ und $b=1$ erhält man den goldenen Schnitt.


\subsection{Konformität als Holomorphie}

\begin{Rezept}
Zur Konstruktion konformer Abbildungen der Ebene
nutze man holomorphe Funktionen.
\end{Rezept}

\noindent
Jeder komplexen Zahl ist gemäß
\[\Phi(a+b\ui) = \begin{pmatrix}a & -b\\ b & a\end{pmatrix}\]
oder äquivalent
\[\Phi(r\ee^{\ui\varphi}) = r\begin{pmatrix}
\cos\varphi & -\sin\varphi\\
\sin\varphi & \cos\varphi
\end{pmatrix}\]
genau eine Matrix zugeordnet. Die Abbildung $\Phi$ ist ein
Isomorphismus vom Körper der komplexen Zahlen in einen Körper,
der eine Unterstruktur des Matrizenrings darstellt.

Diese Beziehung schafft eine Verbindung zwischen dem
Rechnen mit komplexen Zahlen und Konzepten der linearen Algebra.
Sie sagt aus, dass eine komplexe Zahl als eine als \emph{Drehskalierung}
bezeichnete lineare Abbildung betrachtet werden kann. Die Multiplikation
von komplexen Zahlen entspricht der Verkettung der ihnen entsprechenden
Abbildungen.

Man nennt $f\colon\R^2\to\R^2$ eine \emph{konforme} Abbildung, wenn sie
differenzierbar ist und ihre Ableitung $Df$ an jeder Stelle
eine nichtverschwindende Drehskalierung ist. Nun ist die
Ableitung von $f(x,y)=(u(x,y),v(x,y))$ ja genau dann eine
Drehskalierung, wenn
\[Df = \begin{pmatrix}
\partial_x u & \partial_y u\\
\partial_x v & \partial_y v
\end{pmatrix} = \begin{pmatrix}
\partial_x u & -\partial_x v\\
\partial_x v & \partial_x u
\end{pmatrix}.\]
Diese Gleichung zwischen Matrizen beschreibt aber nichts anderes als
die Cauchy"=Rieman"=Gleichungen. Wir setzen daher $z:=x+y\ui$ und
$f(z):=u+v\ui$. Das heißt, $f$ ist genau dann eine konforme
Abbidlung, wenn $f$ holomorph ist und ihre Ableitung keine
Nullstellen besitzt.

\subsection{Vollständige Induktion}

\begin{Rezept}
Eine Aussage $A(n)$ für $n\in\Z, n\ge n_0$ sei offensiv
hergeleitet worden unter Nutzung von höheren oder
heuristischen Methoden, deren Gültigkeit nicht zweifelsfrei
klar ist. Gelingt anschließend ein Beweis durch vollständige
Induktion, braucht man sich darum nicht weiter kümmern
und kann voranschreiten.
\end{Rezept}
Das Verfahren der vollständigen Induktion verläuft folgendermaßen.
Im \emph{Induktionsschritt} schlussfolgert man, dass die
\emph{Induktionsvoraussetzung} $A(n)$ die Aussage $A(n+1)$ nach sich
zieht. Kann man zudem einen \emph{Induktionsanfang}
$A(n_0)$ bestätigen, was oft trivial ist, gilt die Aussage $A(n)$
für alle $n\ge n_0$ als bewiesen.

\strong{Beispiel 1.} Die Summation kann man definieren durch%
\[\sum_{k=0}^0 a_k := a_0,\quad
\sum_{k=0}^{n+1} a_k := a_{n+1}+\sum_{k=0}^n a_n.
\]
Zu beweisen sei die Aussage $\sum_{k=0}^n ca_k = c\sum_{k=0}^n a_k$
für alle $n\ge 0$, wobei
$c$ eine Konstante ist. Der Induktionsanfang ist mit
$\sum_{k=0}^0 ca_k = ca_0 = c\sum_{k=0}^0 a_k$
trivial erbracht. Im Schritt rechnet man%
\begin{gather*}
\sum_{k=0}^{n+1} ca_k = ca_{n+1}+\sum_{k=0}^n ca_k
\!\!\stackrel{\text{(IV)}}=\! c(a_n + \sum_{k=0}^n a_k)
= c\!\sum_{k=0}^n a_k,
\end{gather*}
wobei (IV) gemäß Induktionsvoraussetzung gilt.

\strong{Beispiel 2.} Zu beweisen sei $\sum_{k=0}^n k = \tfrac{n}{2}(n+1)$.
Der Anfang ist trivial. Im Schritt rechnet man
\begin{gather*}
\sum_{k=0}^{n+1} k = n+1+\sum_{k=0}^n k
\stackrel{\text{(IV)}} = n+1+\tfrac{n}{2}(n+1)\\
= (n+1)(1+\tfrac{n}{2})
= \tfrac{n+1}{2}(n+2).
\end{gather*}
Zur Auffindung von Formeln kann man alle erdenklichen Methoden
zum Einsatz bringen, obgleich diese unzuverlässig oder heuristisch
sind. Man darf Umformungen benutzen, bei denen nicht klar ist,
unter welchen Prämissen diese gültig sind. Sogar vage und skizzenhafte
Argumentationen sind nicht unbedingt tabu.

\subsection{Lineare Rekurrenzen}

\begin{Rezept}
Jede lineare Rekurrenz lässt sich durch Hilfsvariablen und homogene
Koordinaten in die Form eines homogenen linearen Systems erster Ordnung
bringen, das in vektorieller Form als $\mathbf v_{n+1}=A\mathbf v_n$
ausgedrückt werden kann. Die Lösung ist daher
$\mathbf v_n = A^n\mathbf v_0$, wobei die Matrixpotenz $A^n$ mittels
Eigenwerttheorie bzw. Theorie der Matrixfunktionen bestimmt werden
kann.
\end{Rezept}

\noindent
Die lineare Rekurrenz $x_{n+1}=ax_n+b$ wird 
mittels homogener Koordinaten zum System%
\[\begin{pmatrix}
x_{n+1}\\ 1
\end{pmatrix}
= \begin{pmatrix}a & b\\ 0 & 1\end{pmatrix}
\begin{pmatrix}
x_n\\ 1
\end{pmatrix}.\]
Bei der linearen Rekurrenz $x_{n+1}=ax_n+bx_{n-1}+c$ setzen wir
$y_n = x_{n-1}$ und erhalten somit das System
\[\begin{pmatrix}
x_{n+1}\\ y_{n+1}
\end{pmatrix} = \begin{pmatrix}
a & b\\ 1 & 0
\end{pmatrix}\begin{pmatrix}
x_n\\ y_n
\end{pmatrix}+\begin{pmatrix}
c\\ 0
\end{pmatrix}.\]
Mittels homogener Koordinaten wird dieses zu
\[\begin{pmatrix}
x_{n+1}\\ y_{n+1}\\ 1
\end{pmatrix} = \begin{pmatrix}
a & b & c\\
1 & 0 & 0\\
0 & 0 & 1
\end{pmatrix}\begin{pmatrix}
x_n\\ y_n\\ 1
\end{pmatrix}.\]

\subsection{Majorantenkriterium}

\begin{Rezept}
Kann man zu einer Reihe von Elementen eines Banachraums eine
konvergente Majorante finden, dann muss die Reihe absolut konvergent
sein. 
\end{Rezept}

\noindent
Sei $(a_k)$ eine Folge von Elementen eines Banachraums.
Eine Reihe $\sum_{k=0}^n b_k$ reeller Zahlen $b_k\ge 0$ mit
$\|a_k\|\le b_k$ für alle $k\ge k_0$ nennt man eine \emph{Majorante}
der Reihe $\sum_{k=0}^n a_k$.

\strong{Beispiel.} Die absolute Konvergenz der Reihe
\[\sum_{k=1}^n\frac{1}{4k^2-2k}\]
bestätigt die Majorante mit $b_k=1/k^2$, denn
\begin{gather*}
\Big|\frac{1}{4k^2-2k}\Big| \le \frac{1}{k^2}\iff k^2\le |4k^2-2k|\\
\iff k\le |4k-2| \iff k^2 \le (4k-2)^2\\
\iff 0\le 15k^2-16k+4.
\end{gather*}
Diese Ungleichung ist für $k\ge 1$ erfüllt.

\newpage
\subsection{Bisektion}

\begin{Rezept}
Bisektion ist ein allgemeines und automatisierbares
Verfahren zur Ermittlung von Nullstellen.
\end{Rezept}

\noindent
Gegeben ist eine Bestimmungsgleichung $T_1(x)=T_2(x)$ in der
Variable $x$, deren Lösungen zu ermitteln sind. Die Problemstellung
ist gleichbedeutend damit, die Nullstellen der reellen Funktion
$f(x)=T_2(x)-T_1(x)$ zu ermitteln. Angenommen, $f$ ist stetig
und für $a<b$ ist $f(a)f(b)<0$, das heißt, $f(a)$ und $f(b)$ haben
unterschiedliches Vorzeichen. Dann muss $f$ gemäß dem Zwischenwertsatz
mindestens eine Nullstelle im Intervall $[a,b]$ besitzen.

Wir betrachten nun den folgenden Vorgang. Der Mittelwert $c := (a+b)/2$
wird berechnet. Ist $f(a)f(c)<0$, muss eine Nullstelle in $[a,c]$
liegen und es wird $c:=b$ gesetzt, andernfalls in $[c,b]$ und es
wird $c:=a$ gesetzt. Auf das neue Intervall $[a,b]$ wird der Vorgang
abermals angewendet, solange bis der Abstand $b-a$ hinreichend klein
geworden ist. Nun ist $c\approx a\approx b$ eine Näherung für die
Nullstelle.

Was ist nun, wenn die Funktion mehrere Nullstellen besitzt?
Dann betrachtet man eine hinreichend feine Aufteilung von $[a,b]$ in
die $n$ Teilintervalle $[a_k,b_k]$ mit $a_k=a+hk$ und $b_k=a_k+h$, wobei
$h=(b-a)/n$. Ist $f(a_k)f(b_k)<0$, kommt Bisektion zur
Anwendung.

Bisher erfuhren solcherlei Funktionen keine Beachtung, die eine
Nullstelle besitzen, obwohl kein Vorzeichenwechsel vorliegt.
Setzen wir dafür voraus, dass $f$ einmal differenzierbar ist und
eine stetige Ableitung besitzt. Wenn kein Vorzeichenwechsel
vorhanden ist, schneidet $f$ die waagerechte Achse nicht,
sondern berührt sie nur, womit $f$ bei der Nullstelle einen
Extrempunkt besitzt. An einem solchen muss aber zwingend $f'(x)=0$
gelten, und zwar mit Vorzeichenwechsel. Ergo kann man wie folgt
vorgehen. Man ermittelt zusätzlich die Nullstellen der Ableitung $f'$
mittels Bisektion und schöpft davon alle die ab, die ebenfalls
Nullstellen von $f$ sind.

Wir beuten das Verfahren nun aus, indem wir es zum Zeichnen einer
durch $f(x,y)=0$ beschriebenen Kurve nutzen. Dazu wird die
$y$-Achse durch die $y_k=y_0+kd$ fein abgetastet. Zu jedem $y_k$
bestimmt man Lösungen $x$ der Gleichung $f(x,y_k)=0$ wie beschrieben
mittels Bisektion. Zur Verbesserung überlagert man die Zeichnung mit
der analogen Zeichnung bezüglich Abtastung der $x$-Achse mit jeweils
Lösungen $y$ der Gleichung $f(x_k,y)=0$.




\newpage
\section{Wahrscheinlichkeitsrechnung}

\subsection{Zufallsgrößen}

\begin{Rezept}
Eine Zufallsgröße $X\colon\Omega\to\Omega'$ schafft eine kausale
Verbindung, dergestalt dass ein Ergbnis $\omega\in\Omega$ zum
Ergebnis $X(\omega)$ wird. Ein Ereignis $A'\subseteq\Omega'$
tritt daher genau dann ein, wenn das Urbild $X^{-1}(A')$ eintritt.
Dieser Umstand induziert die Definition
\[P_X(A') := P(X^{-1}(A')) = P(X\in A').\]
\end{Rezept}

\noindent
Eine Zufallsgröße, was ist das? Eine Zufallsgröße kann man sich
zunächst einfach als eine Abbildung $X\colon\Omega\to\Omega'$ zwischen
Ergebnismengen vorstellen. Sei bspw.
\[\Omega := \{(w_1,w_2)\mid w_1,w_2\in\{1,\ldots,6\}\}\]
die Menge der Ergebnisse des Wurfs zweier gewöhnlicher
Würfel. Das heißt, wurde mit dem ersten Würfel eine Drei
gewürfelt, und mit dem zweiten eine Fünf, ist das Ergebnis $(3, 5)$.
Jedes elementare Ereignis $\{(w_1,w_2)\}$ besitzt offenbar
die gleiche Wahrscheinlichkeit%
\[P(\{(w_1,w_2)\}) = \frac{1}{|\Omega|} = \frac{1}{36}.\]
Für ein beliebiges Ereignis $A$ gilt daher%
\[P(A) = \frac{|A|}{|\Omega|}.\]
Ein gutes Beispiel für eine Zufallsgröße ist die Summe der
Augenzahlen, also%
\[X((w_1,w_2)) := w_1 + w_2.\]
Des Pudels Kern liegt nun in der Beantwortung der Frage, wie
wahrscheinlich ein aus Funktionswerten von $X$ bestehendes
Ereignis ist.

Ein elementares Ereignis $\{x\}$ tritt doch genau dann ein,
wenn $x$ der Funktionswert $x=X(\omega)$ zum Ergebnis
$\omega$ ist. Wurde bspw. das Ergebnis $\omega=(3, 5)$
gewürfelt, ist das elementare Ereignis%
\[\{X(\omega)\} = \{3 + 5\} = \{8\}\]
eingetreten.
Ein elementares Ereignis $\{x\}$ tritt also genau dann ein, wenn das
Ergebnis $\omega$ im Urbild $X^{-1}(x)$ liegt, für das sich die
Schreibweise%
\[X^{-1}(x)=\{X=x\}\]
eingebürgert hat. Demnach stimmt die Wahrscheinlichkeit von $\{x\}$
mit der des Urbildereignisses $\{X=x\}$ überein. Das heißt, es gilt%
\[P_X(\{x\}) = P(X^{-1}(x)) = P(\{X=x\}) = P(X=x).\]
Beispielsweise ist
\[\{X=8\} = \{(2,6), (6,2), (3,5), (5,3), (4,4)\}.\]
Damit ergibt sich
\[P(X=8) = \frac{|\{X=8\}|}{|\Omega|} = \frac{5}{36}\]
als Wahrscheinlichkeit der Augensumme acht.


\vfill\noindent
Dieses Heft steht unter der Lizenz\\
Creative Commons CC0 1.0.

\end{document}

