
\chapter{Analytische Geometrie}

Die analytische Geometrie stellt eine Weiterentwicklung der
klassischen euklidischen Geometrie dar. Diese Entwicklung erfolgte
in zwei Schritten. Im ersten Schritt wurden Koordinatensysteme
eingeführt, um eine Synthese von rechnerischen und geometrischen
Methoden zu ermöglichen. Geometrische Aufgabenstellungen ließen
sich hiermit in Gleichungen und Gleichungssysteme übersetzen.
In einem zweiten Schritt erfolgte die Einführung der Vektorrechnung,
welche eine Übersetzung geometrischer Sachverhalte in rechnerische
Ausdrücke erlaubt, die anschaulicher und prägnanter als Ausdrücke
mit Koordinaten sind. Mit der Vektorrechnung verbunden sind neue
mathematische Objekte, die Vektoren, das sind Verschiebungspfeile
die sich addieren und skalieren lassen. Wesentliches Werkzeug sind
außerdem neuartige Rechenoperationen mit geometrischer Deutung: das
Skalarprodukt, das äußere Produkt, das Vektorprodukt
und das Clifford"=Produkt. Als weiteres maßgebliches Werkzeug kam
später die Matrizenrechnung hinzu. Zwischen all diesen Operationen
gibt es vielfältige Beziehungen.

Die Vektorrechnung wurde später selbst weiterentwickelt zur linearen
Algebra, wo die Matrizen als Darstellungen linearer Abbildungen
zwischen Vektorräumen gedeutet werden konnten. Die Vektorrechnung
ist in der linearen Algebra als Spezialfall enthalten, bei dem die
Vektoren aus dem reellen euklidischen Vektorraum entstammen.
Neben diesem kommen in der linearen Algebra noch viele andere
Vektorräume vor. Um Übersicht zu behalten, ermittelt man in der
linearen Algebra abstrakte Regeln und Gesetzmäßigkeiten, die in allen
Vektorräumen gültig sind.

Abgerundet wird die analytische Geometrie durch Isomorphien,
das sind eins"=zu"=eins-Beziehungen zwischen unterschiedlichen
Rechenformalismen. Z.\,B. lassen sich Vektoren in der Ebene auch als
komplexe Zahlen betrachten. Komplexe Zahlen sind wiederum als
spezielle Matrizen darstellbar.

Zu Bemerken ist noch, dass die analytische Geometrie nicht
als Ersatz für die klassische euklidische Geometrie gedacht ist,
sondern als \emph{Vervollständigung}. Die Sätze, Methoden und Beweise
der euklidischen Geometrie behalten ihre Gültigkeit, allerdings
kommen neue Methoden hinzu. Einige Sachverhalte sind etwas leichter
mit klassischer Geometrie verständlich, andere sind besonders
elegant mit Vektorrechnung formulierbar.

\section{Rechnen mit Koordinaten}
\subsection{Die Koordinatenebene}

Die Koordinatenebene ist das kartesische Produkt der reellen
Zahlen mit sich selbst, besteht also aus allen geordneten Paaren,
deren Komponenten reelle Zahlen sind, kurz%
\[\R\times\R := \{(x,y)\mid x\in\R\;\text{und}\;y\in\R\}.\]
Man kann nun jedem Punkt der euklidischen Ebene ein Koordinatenpaar
zuordnen, indem man ein Koordinatensystem in die Ebene einzeichnet.
Aus Gründen der Einfachheit sollte dieses Koordinatensystem
\emph{kartesisch} sein, das heißt die Koordinatenachsen sollten
rechtwinklig aufeinander stehen und die Skaleneinheit sollte genau
einer Längeneinheit entsprechen.

Die Beschreibung von waagerechten und senkrechten Geraden
erfolgt gemäß Einschränkung der Koordinatenebene auf Teilmengen.
Eine waagerechte Gerade wird durch eine feste Koordinate $y_0$
beschrieben, während die Koordinate $x$ frei variieren darf:%
\[\R\times\{y_0\} = \{(x,y_0)\mid x\in\R\}.\]
Entsprechend ist bei einer senkrechten Gerade die Koordinate
$x_0$ fest, während $y$ frei variieren darf:%
\[\{x_0\}\times\R = \{(x_0,y)\mid y\in\R\}.\]
Man bezeichnet mit $\R^+ = \R_{>0}$ die positiven und mit
$\R^- = \R_{<0}$ die negativen reellen Zahlen. Hiermit lassen
sich vier Halbebenen beschreiben:%
\[\R^+\times\R,\;\; \R^-\times\R,\;\; \R\times\R^+,\;\; \R\times\R^-.\]
Außerdem gibt es vier Quadranten:
\[\R^+\times\R^+,\;\; \R^-\times\R^+,\;\; \R^-\times\R^-,\;\; \R^+\times\R^-.\]
Man kann die Betrachtung auch auf Rechtecke einschränken:%
\[\begin{split}
&[a,b]\times [c,d]\\
&= \{(x,y)\mid a\le x\le b\;\mathrm{und}\;c\le y\le d\}.
\end{split}\]
Entsprechend gibt es offene Rechtecke:
\[\begin{split}
&(a,b)\times (c,d)\\
&= \{(x,y)\mid a<x<b\;\mathrm{und}\;c<y<d\}.
\end{split}\]

\subsection{Geraden}

Durch je zwei unterschiedliche Punkte verläuft genau eine Gerade $g$.
Gegeben seien daher zwei Punkte $P_1=(x_1,y_1)$ und $P_2=(x_2,y_2)$
mit $P_1\ne P_2$. Es gilt%
\[P_1\ne P_2 \iff x_1\ne x_2\;\text{oder}\; y_1\ne y_2.\]
Gibt es nun einen weiteren Punkt $P_0=(x_0,y_0)$, möchte man wissen, ob
$P_0$ auf der Geraden $g$ liegt, kurz $P_0\in g$ gilt. Ein solche
Situation wird klassisch als \emph{Inzidenz} bezeichnet. Zur Lösung
dieser Aufgabe ist die Bestimmung einer beschreibenden Gleichung
der Geraden notwendig. Wie findet man diese Gleichung?

Betrachten wir den Punkt $P_1$. Eine Entfernung $\Delta x$ von $x_1$
führt dann zu einer Entfernung $\Delta y$ von $y_1$. Nach den
Strahlensätzen muss aber das Verhältnis $m=\frac{\Delta y}{\Delta x}$
eine Konstante sein. Dieses feste $m$ wird als \emph{Anstieg}
bezeichnet. D.\,h., für einen beliebigen weiteren
Punkt $P=(x,y)$ auf der Geraden muss die Beziehung%
\begin{equation}\label{eq:Anstieg}
m = \frac{\Delta y}{\Delta x} = \frac{y_2-y_1}{x_2-x_1}
= \frac{y-y_1}{x-x_1}
\end{equation}
erfüllt sein. Zum Zeichnen der Gerade ist es ggf. günstig,
diese Gleichung nach $y$ umzuformen, dann ergibt sich eine
Funktion $f$ die jedem $x$ ein $y=f(x)$ zuordnet. Man bekommt%
\[f(x) = y_1+\frac{y_2-y_1}{x_2-x_1}(x-x_1),\]
bzw. kurz
\[f(x) = y_1+m\cdot (x-x_1).\]
Hier besteht allerdings die Einschränkung $x_1\ne x_2$, d.\,h. die
Punkte $P_1$ und $P_2$ dürfen nicht senkrecht aufeinander liegen,
sonst bekommt man eine unerlaubte Division durch null. Entgehen dieser
Einschränkung ist möglich, indem die Gleichung \eqref{eq:Anstieg}
durch Umformung von allen Divisionen befreit wird, das ergibt%
\[(y_2-y_1)(x-x_1) = (y-y_1)(x_2-x_1).\]
Die Inzidenz lässt sich nun leicht prüfen, indem einfach
$x:=x_0$ und $y:=y_0$ eingesetzt wird. Die Gleichung ist nur
dann erfüllt, wenn $P_0$ auf der Geraden liegt.

Eine weitere Umformung der Gleichung führt zu
\[(y_2-y_1)x-(x_2-x_1)y
= (y_2-y_1)x_1-(x_2-x_1)y_1.\]
Definiert man nun
\[a_1:=y_2-y_1,\;\; a_2:=x_1-x_2,\;\; b:=a_1x_1+a_2y_1,\]
dann bekommt die Gleichung die kurze Form
\begin{equation}\label{eq:Gerade-kurz}
a_1 x + a_2 y = b.
\end{equation}
Diese Form der Gleichung benutzen wir später als Ausgangspunkt
zur Beschreibung der Schnittmenge von zwei Geraden.

\subsection{Kreise}

Viele Kurven sind mit Gleichungen beschreibbar, darunter fallen
auch Kreise. Ein Kreis ist eine Menge von Punkten, die alle
den gleichen Abstand $r$ vom Mittelpunkt haben. Der Mittelpunkt
falle zunächst mit dem Koordinatenursprung zusammen. Ist nun
$P=(x,y)$ ein beliebiger Punkt auf dem Kreis, dann ergibt sich
ein rechtwinkliges Dreieck mit den Kathetenlängen $|x|$ und $|y|$
und der Hypotenusenlänge $r$. Gemäß des Satzes von Pythagoras
muss also $|x|^2+|y|^2=r^2$ sein. Da negative Vorzeichen beim
Quadrieren verschwinden, können die Betragsstriche entfallen.
Der Kreis ist demnach die Punktmenge%
\[K(r) := \{(x,y)\mid x^2+y^2=r^2\}.\]
Liegt der Mittelpunkt nicht im Koordinatenursprung, sondern im
Punkt $P_0=(x_0,y_0)$, kann man die Differenzen $\Delta x := x-x_0$
und $\Delta y := x-y_0$ bilden, dann sind $|\Delta x|$ und $|\Delta y|$
wieder die Längen der Katheten. Somit ergibt sich%
\begin{equation}\label{eq:Kreis-allgemein}
(x-x_0)^2+(y-y_0)^2=r^2
\end{equation}
als allgemeine Gleichung.

\subsection{Schnittmengen}

Wir haben gesehen, dass sich bestimmte geometrische Objekte durch
Gleichungen beschreiben lassen. Eine Gleichung ist eine Relation
$R(x,y)$. Jede Relation beschreibt eine Punktmenge%
\[R := \{(x,y)\mid R(x,y)\}.\]
Eine wichtige Aufgabe in der Geometrie besteht nun darin, für
zwei solcher Punktmengen $R_1$ und $R_2$ die Schnittmenge
$R_1\cap R_2$ zu bestimmen. Der Schnittmenge entspricht
eine und"=Verknüpfung der beiden Relationen, d.\,h.%
\[R_1\cap R_2 = \{(x,y)\mid R_1(x,y)\;\text{und}\; R_2(x,y)\},\]
bzw.
\[(x,y)\in R_1\cap R_2 \Leftrightarrow R_1(x,y)\;\text{und}\; R_2(x,y).\]
Eine solche und"=Verknüpfung wird als \emph{System} von
Relationen bezeichnet. Handelt es sich bei den Relationen um
Gleichungen, spricht man von einem \emph{Gleichungssystem}.

Zunächst ein ganz einfaches Beispiel. Wir wollen eine waagerechte
Gerade $\R\times\{y_0\}$ und eine senkrechte Gerade $\{x_0\}\times\R$
schneiden. Dass sich als Schnittmenge nur ein einziger Punkt ergibt,
und zwar $\{(x_0,y_0)\}$, sollte klar sein, im Zweifelsfall fertige
man eine Skizze an. Das kann man nun auch
genau nachrechnen:%
\begin{gather*}
(x,y)\in\R\times\{y_0\}\cap \{x_0\}\times\R\\
\Leftrightarrow x\in\R\;\text{und}\; y=y_0
\;\text{und}\;y\in\R\;\text{und}\;x=x_0\\
\Leftrightarrow x=x_0\;\text{und}\;y=y_0\\
\Leftrightarrow (x,y)=(x_0,y_0).
\end{gather*}

\subsection{Schnitt von Geraden}

Die Schnittmenge $g_1\cap g_2$ von zwei Geraden
\[\begin{split}
g_1 &:= \{(x,y)\mid a_{11}x+a_{12}y = b_1\},\\
g_2 &:= \{(x,y)\mid a_{21}x+a_{22}y = b_2\}
\end{split}\]
ist die Lösungsmenge des Gleichungssystems
\begin{equation}
\begin{split}\label{eq:LGS}
a_{11}x+a_{12}y &= b_1,\\
a_{21}x+a_{22}y &= b_2.
\end{split}
\end{equation}
Der Anschauung nach ist schon klar, dass drei verschiedene
Umstände bestehen können: die beiden Geraden haben genau
einen Schnittpunkt, stehen entfernt parallel zueinander
oder stimmen überein. Diese Umstände müssen sich in der Lösungsmenge
des Gleichungssystems manifestieren.

Umformung der ersten Gleichung bringt
\[x = \frac{b_1-a_{12}y}{a_{11}}.\]
Einsetzen von $x$ in die zweite Gleichung
und Umformung nach $y$ bringt dann%
\[y = \frac{a_{11}b_2-a_{21}b_1}{a_{11}a_{22}-a_{21}a_{12}}.\]
Nun besteht noch das Problem, dass bei der Umformung eine Division
durch null aufgetreten sein könnte, die Umformung daher ungültig ist.
War das nicht der Fall, gab es nur Äquivalenzumformungen, das
System besitzt demnach dann genau eine Lösung.

Zwar ließe sich die Umformung auch so ausführen, dass eine Division
umgangen wird, allerdings ist die Multiplikation mit null ebenfalls
keine Äquivalenzumformung.

Es ist allerdings so, dass man auf vier Wegen zur gleichen Lösung
gelangt, je nachdem welche Variable welcher Gleichung eingesetzt
wird. Demnach lässt sich eine Division durch null immer vermeiden,
wenn nicht gleichzeitig alle Koeffizienten
$a_{11}, a_{12}, a_{21}, a_{22}$
verschwinden. Bei der Geradengleichung \eqref{eq:Gerade-kurz}
können aber nicht gleichzeitig beide Koeffizienten verschwinden.
Demnach verbleibt nur noch die Division durch die Zahl%
\[D := a_{11}a_{22}-a_{21}a_{12},\]
die als \emph{Determinante} bezeichnet wird. Wenn $D\ne 0$ ist,
muss es eine eindeutige Lösung geben. Betrachten wir daher
nun den Fall $D=0$, das führt zur Gleichung%
\[a_{11}a_{22} = a_{21}a_{12}.\]
Unter der Prämisse $a_{12}\ne 0$ und $a_{22}\ne 0$ ist die Gleichung
äquivalent zu%
\[-\frac{a_{11}}{a_{12}} = -\frac{a_{21}}{a_{22}}.\]
Die Terme auf den beiden Seiten sind die Anstiege der Geraden
$g_1$ und $g_2$ für den funktionalen Zusammenhang $y(x)$, was
nach Umformung von \eqref{eq:LGS} nach $y$ ersichtlich ist:%
\begin{align*}
y &= f_1(x) = \frac{b_1}{a_{12}} - \frac{a_{11}}{a_{12}}x,\\
y &= f_2(x) = \frac{b_2}{a_{22}} - \frac{a_{21}}{a_{22}}x.
\end{align*}
Die Geraden müssen also zwangsläufig parallel stehen. Parallelität
schließt allerdings nicht aus, dass die Geraden auch übereinstimmen
könnten. Das ist genau dann der Fall, wenn $f_1=f_2$, wenn also
zusätzlich%
\[\frac{b_1}{a_{12}} = \frac{b_2}{a_{22}}\]
gilt. Nun kann die Prämisse noch verletzt sein, dann ist aber
$a_{11}\ne 0$ und $a_{21}\ne 0$, und man kann stattdessen den
funktionalen Zusammenhang $x(y)$ betrachten.

\subsection{Schnitt von Kreis und Gerade}

Man kann Schnittmengen aller möglichen durch Gleichungen
beschreibbaren Kurven betrachten, was zu unterschiedlichsten
Gleichungssystemen führt. Wir könnten uns ewig mit dieser
Thematik aufhalten. Beim Schnitt von Kreis und Gerade treten
allerdings die wichtigen Begriffe Passante, Sekante und Tangente
auf. Eine Konzepterweiterung des Begriffs Tangente lässt sich
mittels Differentialrechnung ermitteln. Aus diesem Grund wollen wir
uns hier mit dem Ursprung dieses Begriffs am Kreis beschäftigen.

Ohne Beschränkung der Allgemeinheit lässt sich das Koordinatensystem
so wählen, dass der Mittelpunkt des Kreises im Koordinatenursprung
liegt. Der Einfachheit halber wollen wir vertikale Geraden
ausschließen, dann ist der Schnitt beschrieben durch die Lösungsmenge
des Gleichungssystems%
\begin{align}\label{eq:GS-Kreis}
& x^2 + y^2 = r^2,\\
& y = n+mx.
\end{align}
Einsetzen von $y$ in die erste Gleichung liefert%
\begin{gather*}
x^2+(n+mx)^2 = r^2\\
\iff x^2+n^2+2mnx+m^2 x^2 = r^2\\
\iff (m^2+1)x^2 + 2mnx + n^2-r^2 = 0.
\end{gather*}
Dies ist unter allen Umständen eine quadratische Gleichung in $x$,
denn $m^2+1\ge 1$, also insbesondere $m^2+1\ne 0$. Mit%
\[p = \frac{2mn}{m^2+1},\;\; q = \frac{n^2-r^2}{m^2+1}\]
bekommt man die Diskriminante
\[D = p^2-4q = \frac{4r^2}{m^2+1}-\Big(\frac{2n}{m^2+1}\Big)^2.\]
Bei $D<0$ gibt es keine Lösung, es liegt eine Passante vor. Bei
$D>0$ gibt es zwei Lösungen, es Sekante vor. Bei $D=0$ sind beide
Lösungen zu einer einzigen zusammengefallen, man bekommt eine
Tangente. Es ergibt sich das Kriterium%
\begin{equation}\label{eq:Tangentenkriterium}
D=0\iff r^2(m^2+1)=n^2.
\end{equation}
Wir wollen nun aufzeigen, dass die mittels Differentialrechnung
ermittelten Tangenten tatsächlich dieses Kriterium erfüllen.
Umformung von Gleichung \eqref{eq:GS-Kreis} nach $y$ führt zum
funktionalen Zusammenhang%
\[y = f(x) = \sqrt{r^2-x^2}.\]
An einer Stelle $x_0$ mit $y_0=f(x_0)$ ist die Tangente nun
beschrieben durch die Funktion%
\begin{align*}
T(x) &= f(x_0)+f'(x_0)(x-x_0)\\
&= \underbrace{f(x_0)-f'(x_0)x_0}_{n}+\underbrace{f'(x_0)}_{m}x.
\end{align*}
Die Ableitung der Funktion $f$ ist%
\[f'(x) = \frac{-x}{\sqrt{r^2-x^2}} = \frac{-x}{f(x)}.\]
Es ergibt sich
\[m^2+1 = f'(x_0)^2+1 = \frac{r^2}{r^2-x_0^2} = \frac{r^2}{f(x_0)^2}.\]
Man bekommt
\begin{gather*}
\frac{n^2}{m^2+1}
= \frac{(f(x_0)-f'(x_0)x_0)^2}{f'(x_0)^2+1}\\
= \frac{f(x_0)^2}{r^2}(f(x_0)-f'(x_0)x_0)^2\\
= \frac{1}{r^2}(f(x_0)^2-f(x_0)f'(x_0)x_0)^2\\
= \frac{1}{r^2}(f(x_0)^2+x_0^2)^2
= \frac{1}{r^2}(r^2-x_0^2+x_0^2)^2\\
= \frac{r^4}{r^2} = r^2.
\end{gather*}
Tatsächlich ergibt sich das gewünschte Resultat
\eqref{eq:Tangentenkriterium}.

\subsection{Abstände}

Der Abstand $d(P_1,P_2)$ von zwei Punkten
$P_1=(x_1,y_1)$ und $P_2=(x_2,y_2)$ lässt sich leicht berechnen,
denn hier gilt der Satz des Pythagoras:%
\[\begin{split}
d(P_1,P_2)^2 &= (\Delta x)^2+(\Delta y)^2\\
&= (x_2-x_1)^2+(y_2-y_1)^2.
\end{split}\]
Mit dieser Abstandsfunktion
\[d\colon \R^2\times\R^2\to\R\]
lässt sich ein Kreis um den Punkt $P_0$ mit Radius $r$
auch kurz so beschreiben:%
\[K(P_0,r) := \{P\mid d(P_0,P)=r\}.\]
Der Kreis ist also die Menge der Punkte $P$, welche konstanten
Abstand $r$ vom Punkt $P_0$ haben. Dieser Abstand heißt \emph{Radius}
des Kreises.

\subsection{Translation des Koordinatensystems}

Ein wichtiges Werkzeug in der Geometrie sind
Koordinatentransformationen. Die einfachste solche Transformation
ist die Verschiebung des Koordinatensystems, auch Translation
genannt.
Gegeben sei eine Kurve, beschrieben als Punktmenge%
\[\{(x,y)\mid F(x,y)=0\}.\]
Verschiebung des Koordinatenursprungs zum Punkt $P_0=(x_0,y_0)$
führt zu einem neuen Koordinatensystem, in welchen die Kurve durch
eine andere Gleichung beschrieben ist. Diese Gleichung wollen wir
ermitteln.

Die Translation ist die Transformation
\[\begin{split}
x &= x'+x_0\\
y &= y'+y_0.
\end{split}\]
Setzt man $x'=0$ und $y'=0$, ergibt das wie gewünscht
$x=x_0$ und $y=y_0$. Man bekommt nun
\[F(x,y)=0 \iff F(x'+x_0, y'+y_0)=0.\]
Möchte man nicht das Koordinatensystem verschieben, sondern
die Kurve, muss die Verschiebung in die umgekehrte Richtung
ausgeführt werden. Das ist ein allgemeines Prinzip. Bewegt man
sich z.\,B. Vorwärts im Raum, ist das das gleiche als würde man
stillstehen und der Raum sich rückwärts bewegen. Dreht man sich
in eine Richtung, ist das das gleiche als würde man stillstehen
und der Raum sich in die andere Richtung drehen.

Zu einer Kurve $F(x,y)=0$ ist die neue Kurve demnach beschrieben
durch%
\[F(x-x_0, y-y_0) = 0.\]
Wird der Mittelpunkt des durch $x^2+y^2=r^2$ beschriebener Kreises
also zum Punkt $(x_0,y_0)$ verschoben, hat der neue Kreis die
Gleichung
\[(x-x_0)^2 + (y-y_0)^2 = r^2.\]
Eine andere Überlegung hat bei \eqref{eq:Kreis-allgemein}
schon zu dieser Gleichung geführt.

Der Graph $G$ einer reellen Funktion $f\colon\R\to\R$ beschreibt
ebenfalls eine Kurve, es gilt
\[G = \{(x,y)\mid y=f(x)\}.\]
Für die nach $(x_0,y_0)$ verschobene Kurve gilt
\[y-y_0 = f(x-x_0).\]
Umformung nach $y$ bringt die neue Vorschrift
\[x\mapsto y_0+f(x-x_0).\]

\section{Vektorrechnung}

\subsection{Vektoren}

Ein Vektor $\bvec v$ ist ein Verschiebungspfeil, der aus einer
Richtung und einer Länge besteht. Ein beliebiger Punkt $P$ wird damit
verschoben zu einem Punkt $P'$. Man schreibt $P' = P+\bvec v$. Die
Schreibweise dieser Operation als Addition ist aus folgendem Grund
gerechtfertigt. Die Verschiebung $\bvec v$ lässt sich zerlegen in
eine Komponente $v_1$ und eine Komponente $v_2$. Ist
$P=(x,y)$ und $P'=(x',y')$, dann gilt $x'=x+v_1$ und $y'=y+v_2$.
Das kann man auch so schreiben:%
\[P+\bvec v = \begin{pmatrix}x\\ y\end{pmatrix}
+\begin{pmatrix}v_1\\ v_2\end{pmatrix}
:= \begin{pmatrix}x+v_1\\ y+v_2\end{pmatrix} = P'.\]
Zu zwei Punkten $P_1=(x_1,y_2)$ und $P_2=(x_2,y_2)$ definiert man
entsprechend die Differenz
\[P_2-P_1 = \begin{pmatrix}x_2\\ y_2\end{pmatrix}
-\begin{pmatrix}x_1\\ y_1\end{pmatrix}
:= \begin{pmatrix}x_2-x_1\\ y_2-y_1\end{pmatrix}.\]
Diese Differenz ist genau der Verschiebungsvektor von $P_1$ nach
$P_2$, denn es gilt
\[P_1+(P_2-P_1) = P_2,\]
wie eine kurze Rechnung bestätigt:
\begin{gather*}
\begin{pmatrix}
x_1\\ y_1
\end{pmatrix}
+ \begin{pmatrix}
x_2-x_1\\
y_2-y_1
\end{pmatrix}
= \begin{pmatrix}
x_1+x_2-x_1\\
y_1+y_2-y_1
\end{pmatrix}
= \begin{pmatrix}
x_2\\
y_2
\end{pmatrix}.
\end{gather*}
Vektoren schreibt man fettgedruckt ($\bvec v$), unterstrichen
($\underline v$) oder mit Pfeil ($\vec v$). Die Darstellung von
Fettdruck kann in der Handschrift auch durch Unterstreichen geschehen.
Für Vektoren werden meistens die Buchstaben $\bvec u,\bvec v,\bvec w$
und $\bvec a,\bvec b,\bvec c,\bvec d,\bvec e$ benutzt.

Die Addition von zwei Vektoren ist wie folgt definiert:%
\[\mathbf a+\mathbf b = \begin{pmatrix}a_1\\ b_2\end{pmatrix}
+\begin{pmatrix}b_1\\ b_2\end{pmatrix}
:= \begin{pmatrix}a_1+b_1\\ a_2+b_2\end{pmatrix}.\]
Die Addition von Vektoren entspricht der Hintereinanderausführung
der Verschiebungen, denn es gilt%
\[P+(\bvec a+\bvec b) = (P+\bvec a)+\bvec b,\]
wie man leicht nachrechnen kann:
\begin{align*}
&\begin{pmatrix}x\\ y\end{pmatrix}
+ \begin{pmatrix}a_1+b_1\\ a_2+b_2\end{pmatrix}
= \begin{pmatrix}x+a_1+b_1\\ y+a_2+b_2\end{pmatrix}\\
&= \begin{pmatrix}x+a_1\\ y+a_2\end{pmatrix}
+ \begin{pmatrix}b_1\\ b_2\end{pmatrix}.
\end{align*}
Genauso leicht zu bestätigen sind das Kommutativ-
und das Assoziativgesetz:%
\begin{gather*}
\bvec a+\bvec b = \bvec b+\bvec a,\\
\bvec a+(\bvec b+\bvec c) = (\bvec a+\bvec b)+\bvec c.
\end{gather*}
Multipliziert man jede Komponente eines Vektors mit einer reellen
Zahl $r$, dann wird der Vektor um diese Zahl skaliert. Daher
wird $r$ in diesem Kontext als \emph{Skalar} bezeichnet. Man
definiert die \emph{Skalarmultiplikation}%
\[r\bvec a = r\begin{pmatrix}a_1\\ a_2\end{pmatrix}
:= \begin{pmatrix}ra_1\\ ra_2\end{pmatrix}.\]
Wie bei Zahlen gilt dann z.\,B.
\[\bvec a+\bvec a = 2\bvec a.\]
Das ist auch anschaulich klar: Zweifache Anwendung der gleichen
Verschiebung entspricht genau der Verschiebung um die doppelte Länge.

Die speziellen Vektoren
\[
\bvec e_1 := \begin{pmatrix}1\\ 0\end{pmatrix},\;\;
\bvec e_2 := \begin{pmatrix}0\\ 1\end{pmatrix}
\]
bezeichnet man als \emph{Standardbasis}. Man kann einen
Vektor nun wie folgt zerlegen:%
\begin{align*}
\bvec a &= \begin{pmatrix}a_1\\ a_2\end{pmatrix}
= \begin{pmatrix}a_1\\ 0\end{pmatrix}
+ \begin{pmatrix}0\\ a_2\end{pmatrix}
= a_1\begin{pmatrix}1\\ 0\end{pmatrix}
+ a_2\begin{pmatrix}0\\ 1\end{pmatrix}\\
&= a_1\bvec e_1+a_2\bvec e_2.
\end{align*}
Man sagt, jeder Vektor $\bvec a$ ist als \emph{Linearkombination}
der Standardbasis darstellbar. Der Anteil $a_1\bvec e_1$
entspricht der waagerechten Verschiebung, der Anteil
$a_2\bvec e_2$ der senkrechten.

Die Länge eines Vektors $\bvec a$ wird Betrag
genannt und $|\bvec a|$ geschrieben. Der Zerlegung
$\bvec a=a_1\bvec e_1+a_2\bvec e_2$ in waagerechte und
senkrechte Verschiebung bildet ein rechtwinkliges Dreieck. Der Satz des
Pythagoras erlaubt demnach die Berechnung des Betrags:%
\[|\bvec a|^2 = |a_1\bvec e_1|^2+|a_2\bvec e_2|^2 = a_1^2+a_2^2.\]
Aufgrund der trigonometrischen Betrachtung des rechtwinkligen
Dreiecks können wir außerdem folgendes sagen. Ist ein Vektor $\bvec a$
durch einen Betrag $r$ und einen Richtungswinkel $\varphi$ gegeben,
gilt%
\[\bvec a = \begin{pmatrix}
r\cos\varphi\\
r\sin\varphi
\end{pmatrix}.\]
Gemäß trigonometrischem Pythagoras gilt dann%
\begin{align*}
|\bvec a|^2 &= (r\cos\varphi)^2+(r\sin\varphi)^2\\
&= r^2(\cos(\varphi)^2+\sin(\varphi)^2)\\
&= r^2\cdot 1 = r^2.
\end{align*}
Also ist $|\bvec a|=r$, wie gewünscht.

\subsection{Das Skalarprodukt}

Das \emph{Standardskalarprodukt} von zwei Vektoren%
\[\bvec a = \begin{pmatrix}a_1\\ a_2\end{pmatrix},\;\;
\bvec b = \begin{pmatrix}b_1\\ b_2\end{pmatrix}\]
ist definiert als
\[\langle\bvec a,\bvec b\rangle = a_1b_1+a_2b_2.\]
Das Skalarprodukt eines Vektors mit sich selbst
ergibt das Quadrat des Betrages:
\[\langle\bvec a,\bvec a\rangle = a_1^2+a_2^2 = |\bvec a|^2.\]
Das Skalarprodukt ist kommutativ:
\[\langle\bvec a,\bvec b\rangle
= \langle\bvec b,\bvec a\rangle,\]
denn
\[a_1 b_1 + a_2 b_2 = b_1 a_1 + b_2 a_2.\]
Das Skalarprodukt erfüllt folgende Regeln:
\begin{gather*}
\langle r\bvec a,\bvec b\rangle = \langle\bvec a,r\bvec b\rangle
= r\langle\bvec a,\bvec b\rangle,\\
\langle\bvec a,\bvec b+\bvec c\rangle
= \langle\bvec a,\bvec b\rangle+\langle\bvec a,\bvec c\rangle,\\
\langle\bvec a+\bvec b,\bvec c\rangle
= \langle\bvec a,\bvec c\rangle+\langle\bvec b,\bvec c\rangle.
\end{gather*}
Diese drei Regeln bezeichnet man als \emph{Bilinearität}.

Bilinearität erlaubt das Ausmultiplizieren, ganz analog zur
gewöhnlichen Multiplikation. Man betrachte z.\,B. die
erste binomische Formel
\[(a+b)^2 = a^2+2ab+b^2.\]
Zu dieser besteht ein entsprechendes Analogon:
\begin{gather*}
|\bvec a+\bvec b|^2
= \langle\bvec a+\bvec b,\bvec a+\bvec b\rangle\\
= \langle\bvec a,\bvec a+\bvec b\rangle
+ \langle\bvec b,\bvec a+\bvec b\rangle\\
= \langle\bvec a,\bvec a\rangle
+ \langle\bvec a,\bvec b\rangle
+ \langle\bvec b,\bvec a\rangle
+ \langle\bvec b,\bvec b\rangle\\
= |\bvec a|^2 + 2\langle\bvec a,\bvec b\rangle + |\bvec b|^2.
\end{gather*}

\subsection{Die orthogonale Projektion}

Eine der wichtigen Operationen des euklidischen Vektorraums ist
die orthogonale Projektion. Darunter versteht man folgendes.
Durch einen Vektor $\bvec v$ ist die Richtung einer Ursprungsgeraden
gegeben. Ein zweiter Vektor $\bvec a$ ist nun dergestalt auf die
Gerade zu projizieren, dass die bei der Projektion zurückgelegte
Strecke rechtwinklig auf der Geraden steht.

Das Ergebnis der Projektion ist ein Vektor, welcher durch
$P_{\bvec v}(\bvec a)$ oder $P[\bvec v](\bvec a)$ symbolisiert wird.
Weil dieser auf der Geraden liegt, muss er zu $\bvec v$ kollinear
sein, das heißt es gibt eine Zahl $\lambda$, so dass
\[P_{\bvec v}(\bvec a) = \lambda\bvec v\]
gilt. Außerdem wurde gefordert, dass $\bvec a-\lambda\bvec v$
rechtwinklig auf auf $\bvec v$ steht. Dies bedeutet
\[0 = \langle\bvec v,\bvec a-\lambda\bvec v\rangle
= \langle\bvec v,\bvec a\rangle - \lambda\langle\bvec v,\bvec v\rangle.\]
Umstellen dieser Gleichung bringt uns das gesuchte $\lambda$. Wir
erhalten
\[P_{\bvec v}(\bvec a) = \frac{\langle\bvec v,\bvec a\rangle}
{\langle\bvec v,\bvec v\rangle}\bvec v.\]
Aus der Bilinearität des Skalarproduktes folgen unmittelbar
die beiden Eigenschaften
\begin{gather*}
P_{\bvec v}(\bvec a+\bvec b) = P_{\bvec v}(\bvec a)+P_{\bvec v}(\bvec a),\\
P_{\bvec v}(r\bvec a) = rP_{\bvec v}(\bvec a).
\end{gather*}
Somit handelt es sich bei der Projektion um eine lineare
Abbildung.

Eine Anwendung der Projektion ist die Spiegelung an einer
Ursprungsgeraden. Sei $\bvec v$ ein Vektor in Richtung dieser
Geraden. Sei $\bvec a$ ein beliebiger Ortsvektor.
Der Verbindungsvektor von $\bvec a$ nach $P_{\bvec v}(\bvec a)$
ist Ziel minus Quelle, das ist
\[P_{\bvec v}(\bvec a) - \bvec a.\]
Wendet man das doppelte dieser Verschiebung auf $\bvec a$ an,
erhält man den gespiegelten Punkt. Die Spiegelung, symbolisieren
wir sie durch $S_{\bvec v}(\bvec a)$, ist demnach
beschrieben durch
\[S_{\bvec v}(\bvec a) = \bvec a + 2(P_{\bvec v}(\bvec a) - \bvec a)
= 2P_{\bvec v}(\bvec a) - \bvec a.\]
Auch für die Spiegelung gelten die Eigenschaften
\begin{gather*}
S_{\bvec v}(\bvec a+\bvec b) = S_{\bvec v}(\bvec a)+S_{\bvec v}(\bvec a),\\
S_{\bvec v}(r\bvec a) = rS_{\bvec v}(\bvec a).
\end{gather*}
Demzufolge ist auch die Spiegelung eine lineare Abbildung.

\section{Matrizenrechnung}
\subsection{Lineare Abbildungen}

Die Multiplikation einer quadratischen Matrix mit einem
Koordinatenvektor definieren wir als
\[\begin{pmatrix}
a_{11} & a_{12}\\
a_{21} & a_{22}
\end{pmatrix}\begin{pmatrix}x \\ y\end{pmatrix}
:= \begin{pmatrix}a_{11}x+a_{12}y \\ a_{21}x+a_{22}y\end{pmatrix}.\]
Man kann sich das so merken: Die Matrix auf der linken Seite wird
waagerecht mit dem linken Finger überstrichen, während der rechte
Finger gleichzeitig senkrecht den Vektor auf der rechten Seite
überstreicht. Weil man dies für jeweils eine von zwei Zeilen der Matrix
tun kann, besitzt das Ergebnis ebenfalls zwei Zeilen.

Zur Multiplikation einer Matrix mit einer Matrix multipliziert man
die linke Matrix mit jeweils einem Spaltenvektor der rechten
Matrix. Demzufolge ist die Multiplikation vermittels
\[\begin{pmatrix}
a_{11} &\!\! a_{12}\\
a_{21} &\!\! a_{22}
\end{pmatrix}\begin{pmatrix}
b_{11} &\!\! b_{12}\\
b_{21} &\!\! b_{22}
\end{pmatrix} = \begin{pmatrix}
a_{11}b_{11}+a_{12}b_{21} &\! a_{11}b_{12}+a_{12}b_{22}\\
a_{21}b_{11}+a_{22}b_{21} &\! a_{21}b_{12}+a_{22}b_{22}
\end{pmatrix}\]
gegeben. Es ist dasselbe allgemeine Schema. Der linke Finger
überstreicht waagerecht die linken Zeilen, der rechte währenddessen
senkrecht die rechten Spalten.

Warum diese Multiplikation definiert wurde, wird mit der nächsten
Betrachtung klar werden.

Seien $V,W$ zwei Vektorräume, hier $V=W=\R^2$. Eine Abbildung
$L\colon V\to W$ heißt linear, falls die beiden Regeln
\begin{gather}\label{eq:homogen}
L(\bvec a + \bvec b) = L(\bvec a) + L(\bvec b),\\
\label{eq:additiv}
L(\lambda\bvec a) = \lambda L(\bvec a)
\end{gather}
für beliebige Vektoren $\bvec a,\bvec b$ und skalare $\lambda$ erfüllt
sind.

Sei $L$ linear. Für einen Vektor $\bvec v = v_1\bvec e_1+v_2\bvec e_2$
gilt dann
\[L(\bvec v) = v_1 L(\bvec e_1) + v_2 L(\bvec e_2)\]
unter Nutzung der Regeln \eqref{eq:homogen} und \eqref{eq:additiv}.
Das bedeutet aber dass die Abbildung bereits bei Kenntnis der
Bilder der Basisvektoren eindeutig bestimmt ist. Sei daher
\[\begin{pmatrix}a_{11}\\ a_{21}\end{pmatrix} := L(\bvec e_1),
\quad \begin{pmatrix}a_{12}\\ a_{22}\end{pmatrix} := L(\bvec e_2).\]
Sodann gilt
\[L(\bvec v) = v_1\begin{pmatrix}a_{11}\\ a_{21}\end{pmatrix}
+ v_2\begin{pmatrix}a_{12}\\ a_{22}\end{pmatrix}
= \begin{pmatrix}a_{11}v_1+a_{12}v_2\\ a_{21}v_1+a_{22}v_2\end{pmatrix}.\]
Auf der linken Seite der Gleichung steht die Regel für die
Multiplikation einer Matrix mit einem Vektor.
Die Spalten der Matrix $A$ seien die Bilder der Basisvektoren, kurz
\[A := (L(\bvec e_1),L(\bvec e_2)).\]
Aufgrund der gezeigten Rechnung gilt also
\[L(\bvec v) = A\bvec v.\]
Jeder linearen Abbildung entspricht somit genau eine Matrix. Es stellt
sich heraus, dass die Matrizen im Allgemeinen genau die linearen
Abbildungen zwischen den Koordinatenräumen sind.

Man kann nachrechnen, dass für zwei beliebigen Matrizen $A,B$ und
einen beliebigen Vektor $\bvec v$ das Assoziativgesetz
\[B(A\bvec v) = (BA)\bvec v\]
immer erfüllt ist. Dies bedeutet, dass die Matrizenmultiplikation
$BA$ genau der Hintereinanderausführung der linearen Abbildungen,
$B$ nach $A$, entspricht.

\subsection{Matrizen als Vektoren}

Die Skalarmultiplikation einer reellen Zahl $r$ mit einer Matrix
definiert man als
\[r\cdot\begin{pmatrix}
a_{11} & a_{12}\\
a_{21} & a_{22}\end{pmatrix}
:= \begin{pmatrix}
ra_{11} & ra_{12}\\
ra_{21} & ra_{22}\end{pmatrix}.\]
Die Addition von zwei Matrizen definiert man als
\[\begin{pmatrix}
a_{11} & a_{12}\\
a_{21} & a_{22}
\end{pmatrix} + \begin{pmatrix}
b_{11} & b_{12}\\
b_{21} & b_{22}
\end{pmatrix} = \begin{pmatrix}
a_{11}+b_{11} & a_{12}+b_{12}\\
a_{21}+b_{21} & a_{22}+b_{22}
\end{pmatrix}.\]
Die Matrizen sind auf diese Weise auch Vektoren, welche allerdings
aus einem vierdimensionalen Raum entstammen. 

\section{Parameterkurven}
\subsection{Begriff}
Zur Konstruktion einer Kurve kann man auch einen Punkt $P$ abhängig
von einem Parameter $t$ machen, der Punkt ist dann eine Funktion
von $t$. Sei $I\subseteq\R$ und%
\[c\colon I\to\R^2, \quad c(t):=\begin{pmatrix}x(t)\\ y(t)\end{pmatrix}.\]
Dann wird jedem $t$ ein Punkt $P=c(t)$ zugeordnet. Von einer
Parameterkurve spricht man im strengen Sinn dann, wenn diese
Funktion stetig ist, beim Durchlaufen der Kurve also nirgends
Sprünge vorkommen. Demnach sollte $I$ auch nicht eine beliebige
Teilmenge sein, sondern zusammenhängend, also ein Intervall.

Parameterkurven stellen unter einer geometrischen Sichtweise eine
Verallgemeinerung von reellen Funktionen dar, denn jede reelle
Funktion $f\colon I\to\R$ ist beschreibbar als%
\[c\colon I\to \R^2,\;\; c(t):=\begin{pmatrix}t\\ f(t)\end{pmatrix}.\]

\subsection{Differentiation}

Gegeben sei eine Parameterkurve $f\colon\R\to\R^2$. Unter einem
Mikroskop wachsender Vergrößerung betrachtet wird eine gutartige
Parameterkurve an einer festen Stelle $t$ immer mehr aussehen wie eine
Gerade, die Tangente der Kurve an dieser Stelle. D.\,h. für
ein kleines $h\approx 0$ gibt es einen Vektor $\bvec v$, so dass%
\[f(t+h) \approx f(t)+h\bvec v.\]
Umformung nach $\bvec v$ bringt
\[\bvec v \approx \frac{f(t+h)-f(t)}{h}.\]
Für $h\to 0$ wird sich $\bvec v$ immer mehr dem Vektor annähern,
der sich tangential an die Kurve anschmiegt.

\strong{Definition.} Eine Parameterkurve $f\colon\R\to\R^2$ heißt
differenzierbar an der Stelle $t$, wenn der Grenzwert%
\[f'(t) := \lim_{h\to 0}\frac{f(t+h)-f(t)}{h}\]
existiert.

Sind $f,g$ differenzierbare Parameterkurven und $r\in\R$ eine Zahl,
dann gelten die Regeln%
\begin{gather*}
(f+g)'(t) = f'(t)+g'(t),\\
(f-g)'(t) = f'(t)-g'(t),\\
(rf)'(t) = rf'(t).
\end{gather*}
Diese Regeln sind identisch zu den Ableitungsregeln für reelle
Funktionen. Auch der Nachweis kann ohne Änderungen übernommen
werden.

Ist $f$ differenzierbar und $\bvec a$ ein fester Vektor, dann gilt%
\[(f\bvec a)' = f'(t)\,\bvec a.\]
Der Nachweis:
\begin{align*}
(f\bvec a)'(t) &= \lim_{h\to 0}\frac{f(t+h)\,\bvec a-f(t)\,\bvec a}{h}\\
&= \lim_{h\to 0}\Big(\frac{f(t+h)-f(t)}{h}\,\bvec a\Big)\\
&= \Big(\lim_{h\to 0}\frac{f(t+h)-f(t)}{h}\Big)\,\bvec a
= f'(t)\,\bvec a.
\end{align*}
Für $f(t):=(f_1(t),f_2(t))$ mit differenzierbaren Komponenten
$f_1,f_2$ gilt demnach
\begin{align*}
f'(t) &= (f_1\bvec e_1+f_2\bvec e_2)'(t)\\
&= (f_1\bvec e_1)'(t)+(f_2\bvec e_2)'(t)\\
&= f_1'(t)\,\bvec e_1+f_2'(t)\,\bvec e_2 = (f_1'(t),f_2'(t)).
\end{align*}

\subsection{Tangenten und Normalen}

Ist eine Parameterkurve $c$ an der Stelle $t_0$ differenzierbar,
dann ist der Vektor $c'(t_0)$ tangential, daher ist die Tangente
gemäß
\[T(t) := c(t_0)+(t-t_0)c'(t_0)\]
als Parametergerade gegeben. Für die Normale braucht der
Tangentialvektor lediglich um $90^\circ$ gedreht werden,
was durch Anwendung der Rotationsmatrix
$(\begin{smallmatrix}0 & -1\\ 1 & 0\end{smallmatrix})$
erfolgt. Die Normale ist also gegeben gemäß
\[N(t) := c(t_0)+(t-t_0)\begin{pmatrix}0 & -1\\ 1 & 0\end{pmatrix}c'(t_0).\]
Falls sich der Leser noch nicht mit der Multiplikation einer
Matrix mit einem Vektor beschäftigt hat: Es gilt
\[\begin{pmatrix}
0 & -1\\ 1 & 0
\end{pmatrix}
\begin{pmatrix}
v_1\\ v_2
\end{pmatrix}
= \begin{pmatrix}
-v_2\\ v_1
\end{pmatrix}.\]
Haben zwei Parameterkurven $f,g$ jeweils die Tangentialvektoren
$f'(t_1)\ne 0$ und $g'(t_2)\ne 0$, ist der Winkel $\varphi$ zwischen
den Tangenten bestimmbar gemäß
\[\cos\varphi = \frac{\langle f'(t_1),g'(t_2)\rangle}{|f'(t_1)|\cdot |g'(t_2)|},\]
dies ist eine Anwendung der Formel
\[\langle\bvec a,\bvec b\rangle = |\bvec a|\cdot|\bvec b|\cdot\cos\varphi.\]
Gilt zusätzlich $f(t_1)=g(t_2)$, dann spricht man vom
\emph{Schnittwinkel der Kurven}. Die Forderung $t_1=t_2$ muss hierbei
nicht erfüllt sein. Diese entspricht dem gleichzeitigen Eintreffen
von sich bewegenden Punkten.

Als Spezialfall betrachte man zwei bezüglich $y=m_1 x+n_1$ und
$y=m_2 x+n_2$ gegebene Geraden. Diese lassen sich als Parameterkurven
darstellen gemäß
\[
f(t) := \begin{pmatrix}t\\ m_1 t+n_1\end{pmatrix},\;\;
g(t) := \begin{pmatrix}t\\ m_2 t+n_2\end{pmatrix}.
\]
Hierzu gehören die Tangentialvektoren
\[
f'(t) := \begin{pmatrix}1\\ m_1\end{pmatrix},\;\;
g'(t) := \begin{pmatrix}1\\ m_2\end{pmatrix}.
\]
Die Geraden stehen rechtwinklig zueinander, wenn%
\begin{gather*}
0 = \langle f'(t_1),g'(t_2)\rangle = 1\cdot 1+m_1\cdot m_2\\
\iff m_1 m_2 = -1.
\end{gather*}
In diesem speziellen Fall ist eigentlich keine Differentialrechnung
notwendig, denn
\[\begin{pmatrix}t\\ m t+n\end{pmatrix}
= \begin{pmatrix}1\\ m\end{pmatrix}t+\begin{pmatrix}0\\ n\end{pmatrix}\]
hat die Form einer Parametergerade, der Richtungsvektor lässt sich
daran direkt ablesen.


\subsection{Beziehung zu impliziten Kurven}

Für manche Parameterkurven ist eine Darstellung als implizite
Kurve möglich. Für manche implizite Kurven ist umgekehrt eine
Darstellung als Parameterkurve möglich.

Ein einfaches Beispiel ist die Kreiskurve
\[c\colon{[0,2\pi)}\to\R^2,\;\;c(t):=\begin{pmatrix}
r\cos t\\
r\sin t
\end{pmatrix}.\]
Die Komponenten $x(t)=r\cos t$  und $y(t)=r\sin t$ erfüllen hier die
Gleichung $x(t)^2+y(t)^2=r^2$. Demnach durchläuft $c(t)$ einen
Kreis mit Radius $r$. Außerdem ist $t$ in diesem Fall als Winkel
interpretierbar.

Die \emph{Lemniskate von Gerono} ist definiert als
\[c\colon{[0,2\pi)}\to\R^2,\;\;c(t):=\begin{pmatrix}
\cos t\\
\sin(2t)/2
\end{pmatrix}.\]
Hier ist es schon schwieriger, eine Gleichung zu finden, die
$x(t)=\cos t$ und $y(t)=\sin(2t)/2$ erfüllen.

Gemäß den Additionstheoremen gilt
\[\sin(2t) = 2\cos t\sin t.\]
Demnach ist $y=x\sin t$. Dann gilt auch $y^2=x^2\sin(t)^2$.
Laut trigonometrischem Pythagoras ist $\sin(t)^2=1-\cos(t)^2$,
das führt zu
\begin{equation}\label{eq:Gerono-implizit}
y^2=x^2(1-x^2)\iff x^4+y^2 = x^2.
\end{equation}


\subsection{Bogenlänge}

Gegeben sei eine differenzierbare Kurve $c: [a,b]\to\R^2$ mit
stetiger Ableitung. Jede solche Kurve besitzt eine Bogenlänge, die sich
mit Integralrechnung ermitteln lässt. Es folgt eine Herleitung.
Zunächst wird das Intervall $[a,b]$ zerlegt in $n$ gleich große Teile
\[[a+kh,\,a+(k{+}1)h],\quad k\in\{0,\ldots,n-1\}\]
mit $h=(b-a)/n$. Für ein hinreichend kleines $h$ ist die Kurve in
jedem Intervall nahezu gerade, für die Länge gilt gemäß dem Satz des
Pythagoras dann
\[L_k \approx \sqrt{(\Delta x)^2+(\Delta y)^2}.\]
Hierbei gilt
\begin{align*}
\Delta x &= x(a+(k{+}1)h)-x(a+kh),\\
\Delta y &= y(a+(k{+}1)h)-y(a+kh).
\end{align*}
Für hinreichend kleines $h$ gilt zudem
\begin{align*}
x(t+h) \approx x(t)+x'(t)h,\\
y(t+h) \approx y(t)+y'(t)h.
\end{align*}
Somit ist
\begin{align*}
\Delta x &\approx x'(a+kh)h,\\
\Delta y &\approx y'(a+kh)h.
\end{align*}
Das führt zu
\begin{align*}
L_k &\approx \sqrt{x'(a+kh)^2 h^2+y'(a+kh)^2 h^2}\\
&= \sqrt{x'(a+kh)^2+y'(a+kh)^2}\cdot h\\
&= |c'(a+kh)|\cdot h.
\end{align*}
Also ist
\[L = \sum_{k=0}^{n-1} L_k
\approx \sum_{k=0}^{n-1} \big|c'(a+k\tfrac{b-a}{n})\big|\cdot\tfrac{b-a}{n}.\]
Für $n\to\infty$ ergibt sich die Formel für das bestimmte
Integral einer auf $[a,b]$ stetigen Funktion. D.\,h.%
\[L = \int_a^b |c'(t)|\,\mathrm dt.\]
Probieren wir das gleich am Einheitskreis
\[c(t):=(\cos(t),\sin(t)),\;\;t\in{[0,2\pi)}\]
aus. Wie gewünscht ergibt sich
\begin{align*}
L &= \int_0^{2\pi} \left|\begin{pmatrix}-\sin t\\ \cos t\end{pmatrix}\right|\,\mathrm dt\\
&= \int_0^{2\pi} \sqrt{(-\sin t)^2+(\cos t)^2}\,\mathrm dt\\
&= \int_0^{2\pi} 1\,\mathrm dt = [t]_{0}^{2\pi} = 2\pi.
\end{align*}
Aufgrund der auftretenden Wurzel ist die analytische Bestimmung
des Integrals nur in sehr speziellen Fällen durchführbar.
Gleichwohl lässt sich für eine konkrete Kurve mittels
Quadraturverfahren am Computer schnell ein hochgenauer Wert ermitteln.

\section{Implizite Kurven}
\subsection{Differenzieren impliziter Kurven}

Betrachten wir eine gutartige Funktion $f$, bei der die Lösungsmenge
der Gleichung $f(x,y)=0$ eine Kurve in der Ebene ist. Sei außerdem
$p=(x_0,y_0)$ ein Punkt der Kurve. Ist es machbar, die Gleichung
der Tangente am Punkt $p$ aufzustellen, ohne die Gleichung der Kurve
bezüglich $x$ oder $y$ in eine explizite Form zu bringen?

Angenommen wir würden eine die Kurve durchlaufende Parameterkurve
$c$ kennen, so dass $c(0)=p$ ist und $c'(0)$ der Tangentialvektor,
dessen Richtung mit der Tangente am Punkt $p$ übereinstimmt.
Demnach gilt $f(c(t))=0$ für alle Parameterwerte $t$. Infolge
verschwindet auch die Ableitung nach $t$ an allen Stellen. Auch
für die Verkettung $f\circ c$ gilt eine bestimmte Form der
Kettenregel, so dass wir erhalten%
\[0 = (f\circ c)'(t) = \langle\nabla f(p), c'(t)\rangle.\]
Der Vektor $\nabla f(p)$ ist hierbei der Gradient der Funktion
$f$ an der Stelle $p$. Weil das Skalarprodukt verschwindet,
muss der Gradient also rechtwinklig zum Tangentialvektor stehen.

Die Kurve $c$ vergessen wir nun wieder, sie ist nicht länger
von Bedeutung. Rotieren wir den Gradient mit
$R(90^\circ)=\big(\begin{smallmatrix}0 & -1\\ 1 & 0\end{smallmatrix}\big)$
um $90^\circ$ gegen den Uhrzeigersinn, bringt uns dies als
Ergebnis einen Tangentialvektor. Die Tangente am Punkt $p$ ist
demzufolge gemäß%
\[t\mapsto p + tR(90^\circ)\nabla f(p)\]
als Parametergerade beschrieben.

Alternativ können wir wie folgt vorgehen. Wir suchen einen zum
Gradient rechtwinkligen Vektor $\mathbf v$, so dass gilt%
\[\langle\nabla f(p),\mathbf v\rangle = 0.\]
Mit $\nabla f(p) = \begin{pmatrix}\partial_1 f(p)\\ \partial_2 f(p)\end{pmatrix}$
und $\mathbf v = \begin{pmatrix}v_1\\ v_2\end{pmatrix}$ erhalten wir
\[\langle\nabla f(p),\mathbf v\rangle
= \partial_1 f(p)v_1 + \partial_2 f(p)v_2.\]
Man darf sich nun $v_1,v_2$ so aussuchen, dass die Gleichung erfüllt
ist. Wir wählen $v_1:=1$, dann ist $v_2$ nämlich der Anstieg der
Geraden. Wir bekommen
\[v_2 = -\frac{\partial_1 f(p)}{\partial_2 f(p)}.\]
Die Gerade ist beschrieben durch
\[y = y_0 + v_2(x-x_0).\]
Als Beispiel diene Geronos Lemniskate
\eqref{eq:Gerono-implizit}, also
\[f(x,y) := x^4+y^2-x^2.\]
Die partiellen Ableitungen sind
\begin{align*}
\partial_1 f(x,y) &= 4x^3-2x,\\
\partial_2 f(x,y) &= 2y.
\end{align*}
Das resultiert in
\[v_2 = -\frac{\partial_1 f(x,y)}{\partial_2 f(x,y)}
= \frac{x-2x^3}{y}.\]
Benötigt wird nun ein Punkt der Kurve. Dazu stellen wir
\eqref{eq:Gerono-implizit} nach $y$ um und erhalten
\[y = \pm\sqrt{x^2-x^4}.\]
Wir können den Anstieg nun in Abhängigkeit von $x$ angeben, das ist
\[v_2(x) = \frac{x-2x^3}{\pm\sqrt{x^2-x^4}}.\]
Die Gerade ist
\[y = y(x_0) + v_2(x_0)(x-x_0).\]
Weil bei diesem Beispiel mit die Findung einer expliziten Form für $y$
möglich war, können wir nun $\frac{\mathrm dy}{\mathrm dx}$ alternativ
mit üblicher Differentialrechnung bestimmen und gelangen so zum
gleichen Ergebnis für $v_2$.

