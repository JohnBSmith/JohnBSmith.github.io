
\chapter{Lineare Algebra}
\section{Grundbegriffe}
\subsection{Norm}\index{Norm}
\begin{definition}[Norm]\mbox{}\newline
Eine Abbildung $v\mapsto\|v\|$ von einem
Vektorraum $V$ über dem Körper $K$ in die nichtnegativen reellen
Zahlen heißt \emdef{Norm}, wenn für alle $v,w\in V$ und $a\in K$
die drei Axiome%
\begin{gather}
\|v\|=0 \implies v=0,\\
\|av\| = |a|\,\|v\|,\\
\|v+w\| \le \|v\|+\|w\|
\end{gather}
erfüllt sind.
\end{definition}

\noindent
Eigenschaften:
\begin{gather}
\|v\|=0\iff v=0,\\
\|-v\|=\|v\|,\\
\|v\|\ge 0.
\end{gather}
Dreiecksungleichung nach unten:
\begin{equation}
|\|v\|-\|w\||\le \|v-w\|.
\end{equation}

\subsection{Skalarprodukt}\index{Skalarprodukt}

Ein Vektorraum über dem Körper $\R$ heißt \emdef{reeller Vektorraum},
einer über dem Körper $\C$ heißt \emdef{komplexer Vektorraum}.

\subsubsection{Definition}
Sei $V$ ein reeller Vektorraum. Eine Abbildung $f\colon V^2\to\R$
mit $f(x,y)=\langle x,y\rangle$ heißt \emdef{Skalarprodukt}, wenn
folgende Axiome erfüllt sind. Für $v,w\in V$ und $\lambda\in\R$ gilt:
\begin{gather}
\langle v,w\rangle = \langle w,v\rangle,\\
\langle v,\lambda w\rangle = \lambda\langle v,w\rangle,\\
\langle v,w_1+w_2\rangle = \langle v,w_1\rangle +\langle v,w_2\rangle,\\
\langle v,v\rangle\ge 0,\\
\langle v,v\rangle=0 \iff v=0.
\end{gather}
Sei $V$ ein komplexer Vektorraum und $f\colon V^2\to\C$.\\
Für $v,w\in V$ und $\lambda\in\R$ gilt:
\begin{gather}
\langle v,w\rangle = \overline{\langle w,v\rangle},\\
\langle \lambda v,w\rangle = \overline{\lambda}\langle v,w\rangle,\\
\langle v,\lambda w\rangle = \lambda\langle v,w\rangle,\\
\langle v,w_1+w_2\rangle = \langle v,w_1\rangle +\langle v,w_2\rangle,\\
\langle v,v\rangle\ge 0,\\
\langle v,v\rangle=0 \iff v=0.
\end{gather}

\subsubsection{Eigenschaften}
Das reelle Skalarprodukt ist eine symmetrische bilineare Abbildung.

\subsubsection{Winkel und Längen}
\begin{definition}[Winkel, orgthogonale Vektoren]\mbox{}\newline
Der \emdef{Winkel} $\varphi$ zwischen $v$ und $w$
ist definiert durch die Beziehung:
\begin{equation}
\langle v,w\rangle = \|v\|\,\|w\|\,\cos\varphi.
\end{equation}
\emdef{Orthogonal}:\index{Orthogonal}
\begin{equation}
v\perp w \;:\Longleftrightarrow\; \langle v,w\rangle=0.
\end{equation}
\end{definition}

Ein Skalarprodukt $\langle v,w\rangle$ induziert die Norm
\begin{equation}
\|v\| := \sqrt{\langle v,v\rangle}.
\end{equation}

\subsubsection{Orthonormalbasis}\label{sec:ONB}
\index{Orthogonalsystem}\index{Orthogonalbasis}
\index{Orthonormalsystem}\index{Orthonormalbasis}
Sei $B=(b_k)_{k=1}^n$ eine Basis eines endlichdimensionalen
Vektorraumes über den reellen oder komplexen Zahlen.

\begin{definition}[Orthogonalbasis]\mbox{}\newline
Gilt $\langle b_i,b_j\rangle=0$
für alle $i,j$ mit $i\ne j$, so wird $B$ \emdef{Orthogonalbasis}
genannt. Ist $B$ nicht unbedingt eine Basis, so spricht man von einem 
\emdef{Orthogonalsystem}.
\end{definition}

\begin{definition}[Orthonormalbasis]\mbox{}\newline
Ist $B$ eine Orthogonalbasis und gilt
zusätzlich $\langle b_k,b_k\rangle=1$ für alle $k$, so wird
$B$ \emdef{Orthonormalbasis} (ONB) genannt. Ist $B$ nicht unbedingt
eine Basis,  so spricht man von einem \emdef{Orthonormalsystem}.
\end{definition}

Sei $v=\sum_k v_kb_k$ und $w=\sum_k w_kb_k$.
Mit $\sum_k$ ist immer $\sum_{k=1}^n$ gemeint.

Ist $B$ eine Orthonormalbasis, so gilt:
\begin{equation}
\langle v,w\rangle = \sum_k \overline{v_k}\,w_k.
\end{equation}
Ist $B$ nur eine Orthogonalbasis, so gilt:
\begin{equation}
\langle v,w\rangle = \sum_k \langle b_k,b_k\rangle \overline{v_k}\,w_k
\end{equation}
Allgemein gilt:
\begin{equation}
\langle v,w\rangle = \sum_{i,j} g_{ij} \overline{v_i}\,w_j
\end{equation}
mit $g_{ij}=\langle b_i,b_j\rangle$. In reellen Vektorräumen
ist die komplexe Konjugation wirkungslos und kann somit entfallen.

Ist $B$ eine Orthogonalbasis und $v=\sum_k v_k b_k$, so gilt:
\begin{equation}
v_k = \frac{\langle b_k,v\rangle}{\langle b_k,b_k\rangle}.
\end{equation}
Ist $B$ eine Orthonormalbasis, so gilt speziell:
\begin{equation}
v_k = \langle b_k,v\rangle.
\end{equation}


\subsubsection{Orthogonale Projektion}
Orthogonale Projektion von $v$ auf $w$:
\begin{equation}
P[w](v) := \frac{\langle v,w\rangle}{\langle w,w\rangle}\,w.
\end{equation}
\subsubsection{Gram-Schmidt-Verfahren}
Für linear unabhängige Vektoren $v_1,\ldots,v_n$
wird durch%
\begin{equation}
w_k := v_k - \sum_{i=1}^{k-1} P[w_i](v_k)
\end{equation}
ein Orthogonalsystem $w_1,\ldots,w_n$ berechnet.

Speziell für zwei nicht kollineare Vektoren $v_1,v_2$ gilt
\begin{gather}
w_1=v_1,\\
w_2=v_2-P[w_1](v_2).
\end{gather}

\newpage
\subsubsection{Musikalische Isomorphismen}
\begin{definition}[Musikalische Isomorphismen]%
\index{musikalische Isomorphismen}\mbox{}\newline
Sei $V$ ein eindlichdimensionaler Vektorraum
mit Skalarprodukt und $V^*$ sein Dualraum.
Die lineare Abbildung%
\begin{equation}
\Phi\colon V\to V^*,\quad \Phi(u)(v):=\langle u,v\rangle
\end{equation}
ist ein kanonischer Isomorphismus.%
\index{kanonischer Isomorphismus!musikalische Isomorphismen}
Man nennt $u^\flat:=\Phi(u)$
und $\omega^\sharp:=\Phi^{-1}(\omega)$ die \emdef{musikalischen
Isomorphismen}.
\end{definition}

\section{Koordinatenvektoren}
\subsection{Koordinatenraum}
Addition von $a,b\in K^n$:
\begin{equation}\label{eq:Koordinatenraum-Addition}
\begin{bmatrix}
a_1\\
\vdots\\
a_n
\end{bmatrix}
+\begin{bmatrix}
b_1\\
\vdots\\
b_n
\end{bmatrix}
:= \begin{bmatrix}
a_1+b_1\\
\vdots\\
a_n+b_n
\end{bmatrix}.
\end{equation}
Subtraktion:
\begin{equation}
\begin{bmatrix}
a_1\\
\vdots\\
a_n
\end{bmatrix}
-\begin{bmatrix}
b_1\\
\vdots\\
b_n
\end{bmatrix}
:= \begin{bmatrix}
a_1-b_1\\
\vdots\\
a_n-b_n
\end{bmatrix}.
\end{equation}
Skalarmultiplikation von $\lambda\in K$ mit $a\in K^n$:
\begin{align}\label{eq:Koordinatenraum-Skalarmultiplikation}
\lambda\begin{bmatrix}
a_1\\
\vdots\\
a_n
\end{bmatrix}
:= \begin{bmatrix}
\lambda a_1\\
\vdots\\
\lambda a_n
\end{bmatrix}.
\end{align}
Ist $K$ ein Körper, so bildet die Menge
\begin{equation}
K^n = \{(a_1,\ldots,a_n)\mid \forall k\colon a_k\in K\}
\end{equation}
bezüglich der Addition \eqref{eq:Koordinatenraum-Addition}
und der Multiplikation \eqref{eq:Koordinatenraum-Skalarmultiplikation}
einen Vektorraum, der \emdef{Koordinatenraum} genannt wird.
Das Tupel $E_n=(e_1,\ldots,e_n)$ mit
\begin{equation}\label{eq:kanonische-Basis}
\begin{split}
e_1 &:= (1,0,0,0,\ldots, 0),\\
e_2 &:= (0,1,0,0,\ldots, 0),\\
e_3 &:= (0,0,1,0,\ldots, 0),\\
\ldots\\
e_n &:= (0,0,0,0,\ldots, 1)
\end{split}
\end{equation}
bildet eine geordnete Basis von $K^n$, die \emdef{kanonische Basis}
genannt wird. Es gilt
\begin{equation}
a = (a_1,\ldots,a_n) = a_1 e_1+\ldots+a_n e_n.
\end{equation}

\newpage
\subsection{Kanonisches Skalarprodukt}
\begin{definition}[Kanonisches Skalarprodukt]\mbox{}\newline
Für $a,b\in\R^n$:
\begin{equation}
\langle a,b\rangle := \sum_{k=1}^n a_k b_k.
\end{equation}
Für $a,b\in\C^n$:
\begin{equation}
\langle a,b\rangle := \sum_{k=1}^n \overline{a_k}\,b_k.
\end{equation}
\end{definition}
\noindent
Die kanonische Basis \eqref{eq:kanonische-Basis} ist eine
Orthonormalbasis bezüglich diesem Skalarprodukt, s. \ref{sec:ONB}.
Das Skalarprodukt induziert die Norm
\begin{equation}
|a| := \sqrt{\langle a,a\rangle} = \sqrt{\textstyle \sum_{k=1}^n |a_k|^2},
\end{equation}
die \emdef{Vektorbetrag}\index{Vektorbetrag} genannt wird.

Jedem Koordinatenvektor $a\ne 0$ lässt sich ein Einheitsvektor\index{Einheitsvektor}
$\hat a:=\frac{a}{|a|}$ zuordnen, der in Richtung von $a$ zeigt
und die Eigenschaft $|\hat a|=1$ besitzt.

Es gilt
\begin{align}
a\perp b &\iff \langle a,b\rangle=0,\\
a\uparrow\uparrow b &\iff \langle a,b\rangle = |a|\,|b|,\\
a\uparrow\downarrow b &\iff \langle a,b\rangle = -|a|\,|b|.
\end{align}
Allgemein gilt
\begin{equation}
\langle a,b\rangle = |a|\,|b|\cos\varphi.\qquad(\varphi=\angle (a,b))
\end{equation}

\subsection{Vektorprodukt}
Für $a,b\in\R^3$:\index{Vektorprodukt}
\begin{equation}
a\times b = \begin{bmatrix}
a_x\\ a_y\\ a_z
\end{bmatrix}\times\begin{bmatrix}
b_x\\ b_y\\ b_z
\end{bmatrix}
= \begin{vmatrix}
e_x & a_x & b_x\\
e_y & a_y & b_y\\
e_z & a_z & b_z
\end{vmatrix}
= \begin{bmatrix}
a_y b_z - a_z b_y\\
a_z b_x - a_x b_z\\
a_x b_y - a_y b_x
\end{bmatrix}.
\end{equation}
Rechenregeln für $a,b,c\in\R^3$ und $r\in\R$:
\begin{gather}
a\times (b+c) = a\times b+a\times c,\\
(a+b)\times c = a\times c+b\times c,\\
(ra)\times b = r(a\times b) = a\times(rb),\\
a\times b = -b\times a,\\
a\times a = 0.
\end{gather}
Für den Betrag gilt:
\begin{equation}
|a\times b| = |a|\,|b|\,\sin\varphi.\qquad(\varphi=\angle (a,b))
\end{equation}
Beziehung zur Determinante:
\begin{equation}
\langle a,b\times c\rangle = \det(a,b,c).
\end{equation}
Jacobi-Identität:%
\index{Identität!Jacobi-Identität}\index{Jacobi-Identität}
\begin{equation}
a\times(b\times c) = b\times (a\times c) - c\times (a\times b). 
\end{equation}
Graßmann-Identität:%
\index{Identität!Graßmann-Identität}\index{Graßmann-Identität}
\begin{equation}
a\times(b\times c) = b\langle a,c\rangle - c\langle a,b\rangle.
\end{equation}
Cauchy-Binet-Identität:
\index{Identität!Cauchy-Binet-Identität}\index{Cauchy-Binet-Identität}
\begin{equation}
\langle a\times b, c\times d\rangle
= \langle a,c\rangle\langle b,d\rangle
- \langle b,c\rangle\langle a,d\rangle.
\end{equation}
Lagrange-Identität:%
\index{Identität!Lagrange-Identität}\index{Lagrange-Identität}
\begin{equation}
|a\times b|^2 = |a|^2 |b|^2 - \langle a,b\rangle^2.
\end{equation}

\newpage
\section{Matrizen}\index{Matrix}
\subsection{Quadratische Matrizen}%
\index{Matrix!quadratische}\index{quadratische Matrix}
\subsubsection{Matrizenring}%
\index{Ring!Matrizenring}\index{Matrizenring}
Mit $K^{n\times n}$ wird die Menge quadratischen Matrizen
\begin{equation}
(a_{ij}) = \begin{bmatrix}
a_{11} & \ldots & a_{1n}\\
\ldots & \ddots & \ldots\\
a_{n1} & \ldots & a_{nn}
\end{bmatrix}
\end{equation}
mit Einträgen $a_{ij}$ aus dem Körper $K$ bezeichnet.

Die Menge $K^{n\times n}$ bildet bezüglich Addition
und Multiplikation von Matrizen einen Ring (s. \ref{sec:Strukturen}).

Das neutrale Element der Multiplikation
ist die Einheitsmatrix
\begin{equation}
E_n = (\delta_{ij}),\quad
\delta_{ij}:=\begin{cases}
1 & \text{wenn}\;i=j,\\
0 & \text{sonst}.
\end{cases}
\end{equation}
Das sind
\begin{equation}
E_2 = \begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix},\quad
E_3 = \begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{bmatrix},
\quad\text{usw.}
\end{equation}

\subsubsection{Symmetrische Matrizen}
Eine quadratiche Matrix $A=(a_{ij})$ heißt
symmetrisch\index{symmetrische Matrix}\index{Matrix!symmetrische},
falls gilt $a_{ij}=a_{ji}$ bzw. $A^T=A$.

Jede reelle symmetrische Matrix besitzt ausschließlich reelle
Eigenwerte und die algebraischen Vielfachheiten stimmen mit den
geometrischen Vielfachheiten überein.

Jede reelle symmetrische Matrix $A$ ist diagonalisierbar, d.\,h. es gibt
eine invertierbare Matrix $T$ und eine Diagonalmatrix $D$, so dass
$A=TDT^{-1}$ gilt.

Sei $V$ ein $K$-Vektorraum und $(b_k)_{k=1}^n$ eine Basis von $V$.
Für jede symmetrische Bilinearform\index{symmetrische Bilinearform}
$f\colon V^2\to K$ ist die
Darstellungsmatrix
\begin{equation}
A = (f(b_i,b_j))
\end{equation}
symmetrisch. Ist $A\in K^{n\times n}$ eine symmetrische Matrix, so
ist
\begin{equation}\label{eq:symmBf}
f(x,y) = x^T A y.
\end{equation}
eine symmetrische Bilinearform für  $x,y\in K^n$.
Ist $K=\R$ und $A$ positiv definit, so ist
\eqref{eq:symmBf} ein Skalarprodukt auf $\R^n$.

\subsubsection{Reguläre Matrizen}\index{inverse Matrix}
\begin{definition}[Reguläre Matrix, singuläre Matrix]\mbox{}\newline
Eine quadratische Matrix $A\in K^{n\times n}$ heißt
\emdef{regulär}\index{reguläre Matrix}\index{Matrix!reguläre}
oder \emdef{invertierbar}, wenn es eine inverse Matrix $A^{-1}$ gibt,
so dass
\begin{equation}
A^{-1}A = E_n \quad (\iff AA^{-1} = E_n)
\end{equation}
gilt, wobei mit $E_n$ die Einheitsmatrix gemeint ist.
Eine quadratische Matrix die nicht regulär ist, heißt
\emdef{singulär}\index{singuläre Matrix}\index{Matrix!singuläre}.
\end{definition}

\strong{Kriterien.} Für eine Matrix $A\in K^{n\times n}$ gilt:
\begin{gather}
A\text{ ist regulär}\iff \exists B (BA=E_n)\\
\iff \det(A)\ne 0 \iff \operatorname{rk}(A)=n\\
\iff 0\text{ ist kein Eigenwert von }A\\
\iff \ker(A)=\{0\}.
\end{gather}

\noindent
\strong{Eigenschaften.}
Jede reguläre Matrix besitzt genau eine inverse Matrix. 
Die Menge der regulären Matrizen bildet bezüglich Matrizenmultiplikation
eine Gruppe, die
\emdef{allgemeine lineare Gruppe}\index{allgemeine lineare Gruppe}
\begin{equation}
\operatorname{GL}(n,K) := \{A\in K^{n\times n}\mid\det(A)\ne 0\}.
\end{equation}
Ist $V$ ein Vektorraum über dem Körper $K$, so bilden die
Automorphismen bezüglich Verkettung eine Gruppe, die
\emph{Automorphismengruppe}
\begin{equation}
\operatorname{GL}(V) = \operatorname{Aut}(V).
\end{equation}
Ein \emdef{Endomorphismus}\index{Endomorphismus!auf einem Vektorraum}
ist eine lineare Abbildung, welche eine Selbstabbildung ist.
Ein \emdef{Automorphismus}\index{Automorphismus!auf einem Vektorraum}
ist eine bijektiver Endomorphismus.

Wählt man auf $V$ eine Basis
$B$, so ist die Zuordnung der Darstellungsmatrix
\begin{equation}
M_B^B\colon \operatorname{Aut}(V)\to\operatorname{GL}(\dim V,K)
\end{equation}
eine Gruppenisomorphismus.

Endomorphismen die nicht bijektiv sind, bzw. singuläre Matrizen,
lassen die Dimension ihrer Definitionsmenge schrumpfen:
\begin{equation}
f{\in}\operatorname{End}(V){\setminus}\operatorname{Aut}(V)
\Longleftrightarrow \dim f(V)<\dim V.
\end{equation}
Für Matrizen $A\in K^{n\times n}$ bedeutet das, dass sie nicht
den vollen Rang besitzen:
\begin{equation}
\det A=0\iff \operatorname{rk}(A) < n = \dim K^n.
\end{equation}
\strong{Bestimmung der inversen Matrix.}

Für eine Matrix $A\in K^{2\times 2}$ gilt:
\begin{equation}
A^{-1} = \begin{bmatrix}
a & b\\
c & d
\end{bmatrix}^{-1}
= \frac{1}{ad-bc}\begin{bmatrix}
d & -b\\
-c & a
\end{bmatrix},
\end{equation}
wenn $\det A\ne 0$ mit $\det A = ad-bc$.

\begin{definition}[Streichungsmatrix]\mbox{}\newline
Wird in der Matrix $A$ die Zeile $i$ und die Spalte $j$ entfernt,
so entsteht eine neue Matrix $[A]_{ij}$, die
\emdef{Streichungsmatrix}\index{Streichungsmatrix}
von $A$ genannt wird.
\end{definition}
Laplacescher Entwicklungssatz:
\begin{align}
\det A = \sum_{i=1}^n (-1)^{i+j}a_{ij}\det([A]_{ij}),\\
\det A = \sum_{j=1}^n (-1)^{i+j}a_{ij}\det([A]_{ij}).
\end{align}

\subsubsection{Determinanten}\index{Determinante}
Für Matrizen $A,B\in K^{n\times n}$ und $r\in K$ gilt:
\begin{gather}
\det(AB) = \det(A)\det(B),\\
\det(A^T) = \det(A),\\
\det(rA) = r^n\det(A),\\
\det(A^{-1}) = \det(A)^{-1}.
\end{gather}
Für eine Diagonalmatrix $D=\diag(d_1,\ldots,d_n)$ gilt:
\begin{gather}
\det(D) = \prod_{k=1}^n d_k.
\end{gather}
Eine linke Dreiecksmatrix ist eine Matrix der Form
$(a_{ij})$ mit $a_{ij}=0$ für $i<j$. Eine rechte
Dreiecksmatrix ist die Transponierte einer linken
Dreiecksmatrix.

Für eine linke oder rechte Dreiecksmatrix $A=(a_{ij})$ gilt:
\begin{gather}
\det(A) = \prod_{k=1}^n a_{kk}.
\end{gather}

\newpage
\subsubsection{Eigenwerte}
\strong{Eigenwertproblem:}\index{Eigenwert}
Für eine gegebene quadratische Matrix $A$ bestimme
\begin{equation}
\{(\lambda,v)\mid Av = \lambda v,\,v\ne 0\}.
\end{equation}
Das homogene lineare Gleichungssystem
\begin{equation}
Av=\lambda v \iff (A-\lambda E_n)v=0
\end{equation}
besitzt Lösungen $v\ne 0$ gdw.
\begin{equation}
p(\lambda):=\det(A-\lambda E_n)=0.
\end{equation}
Bei $p(\lambda)$ handelt es sich um ein normiertes Polynom
vom Grad $n$, das \emdef{charakeristisches Polynom}
\index{charakteristisches Polynom}
genannt wird.

\strong{Eigenraum:}\index{Eigenraum}
\begin{equation}
\Eig(A,\lambda):=\{v\mid Av=\lambda v\}.
\end{equation}
Die Dimension $\dim\Eig(A,\lambda)$ wird
\emdef{geometrische Vielfachheit}\index{geometrische Vielfachheit}
von $\lambda$ genannt.

\strong{Spektrum:}\index{Spektrum}
\begin{equation}
\sigma(A) := \{\lambda\mid \exists v\ne 0\colon Av=\lambda v\}.
\end{equation}

\subsubsection{Nilpotente Matrizen}
\begin{definition}[Nilpotente Matrix]\mbox{}\newline
Eine quadratische Matrix $A\in K^{n\times n}$ heißt \emdef{nilpotent},
wenn es eine Zahl $k\in\N, k\ge 1$ gibt, so dass gilt:
\begin{equation}
A^k=0.
\end{equation}
Die erste solche Zahl heißt \emdef{Nilpotenzgrad} der Matrix $A$.

Eine äquivalente Bedingung ist:
\begin{equation}
p_A(\lambda):=\det(\lambda E-A)=\lambda^n.
\end{equation}
\end{definition}

\noindent
\strong{Eigenschaften.}
Sei $A$ eine nilpotente Matrix. Es gilt:
\begin{itemize}[itemsep=0pt, leftmargin=3em]
\bitem $A$ besitzt nur den Eigenwert $\lambda=0$.
\bitem $\det(A)=\tr(A)=0$.
\bitem $E-A$ ist invertierbar.
\end{itemize}

\subsection{Matrixfunktionen}
\subsubsection{Matrixexponential}
\begin{definition}[Matrixexponential]\mbox{}\\*
Für eine beliebige Matrix  $X\in \C^{n\times n}$ konvergiert%
\begin{equation}
\exp(X) := \sum_{k=0}^\infty \frac{X^k}{k!}.
\end{equation}
\end{definition}
Für jede Matrix $X$ und $a,b\in\C$ gilt
\begin{gather}
\exp(-X) = \exp(X)^{-1},\\
\exp(X^H) = \exp(X)^H,\\
\exp((a+b)X) = \exp(aX)\exp(bX),\\
\exp(\diag(d_1,\ldots,d_n)) = \diag(\ee^{d_1},\ldots,\ee^{d_n}),\\
\det(\exp(X)) = \ee^{\tr(X)}.
\end{gather}
Für $XY=XY$ gilt
\begin{equation}
\exp(X+Y)=\exp(X)\exp(Y).
\end{equation}
Das Exponential einer Matrix ist immer invertierbar und
jede Matrix aus $\operatorname{GL}(n,\C)$ kann als Matrixexponential
dargestellt werden. D.\,h.
$\exp\colon \C^{n\times n}\to\operatorname{GL}(n,\C)$
ist surjektiv.

\newpage
\subsubsection{Allgemein}

Matrizen bilden bezüglich Matrizenmultiplikation zusammen
mit der Frobeniusnorm oder einer Operatornorm eine assoziative
Banachalgebra mit Einselement.
Man betrachte nun die formale Potenzreihe%
\begin{equation}
f(X) := \sum_{k=0}^\infty a_k X^k,\quad a_k\in \C.
\end{equation}
Besitzt die Einsetzung $f(z)$ für $z\in\C$ den Konvergenzradius
$r>0$ und ist $A$ ein Element einer Banachalgebra
mit Einselement mit $\|A\|<r$,
dann ist $f(A)$ absolut konvergent. Gemäß%
\begin{equation}
f\colon \{\|A\|<r\mid A\in\C^{n\times n}\}\to\C^{n\times n}
\end{equation}
ist daher eine Matrixfunktion definiert.

Ist $A$ diagonalisierbar mit $A=TDT^{-1}$, dann gilt
\begin{equation}
f(A) = Tf(D)T^{-1},
\end{equation}
wobei $f(D)$ gemäß
\begin{equation}
f(\diag(d_1,\ldots,d_n)) = \diag(f(d_1),\ldots,f(d_n))
\end{equation}
berechnet wird.

\strong{Sylvesters Formel.} Allgemein gilt%
\begin{equation}\label{eq:Sylvesters-Formel}
f(A) = \sum_{i=1}^s A_i \sum_{k=0}^{m_i-1} \frac{f^{(k)}(\lambda_i)}{k!}(A-\lambda_i E)^k
\end{equation}
mit
\begin{equation}
A_i := \prod_{j=1,\,j\ne i}^s \frac{1}{\lambda_i-\lambda_j}(A-\lambda_j E).
\end{equation}
Hierbei ist $s$ die Anzahl der unterschiedlichen Eigenwerte und $m_i$
die algebraische Vielfachheit von $\lambda_i$.

Bei einer diagonalisierbaren Matrix vereinfacht sich die Formel zu
\begin{equation}
f(A) = \sum_{i=1}^n A_i f(\lambda_i).
\end{equation}
Speziell für $2\times 2$-Matrizen gilt
\begin{equation}
f(A) = pA+qE
\end{equation}
mit
\begin{align}
p &= \frac{f(\lambda_2)-f(\lambda_1)}{\lambda_2-\lambda_1},\\
q &= f(\lambda_1)-p\lambda_1 = f(\lambda_2)-p\lambda_2.
\end{align}
Im Fall $\lambda_1=\lambda_2$ ist $p=f'(\lambda_1)$.

\strong{Als Cauchy-Integral.} Sei $f\colon U\to\C$ holomorph
und $G\subseteq U$ abgeschlossen und einfach zusammenhängend.
Liegen alle Eigenwerte von $A$ im Inneren von $G$, dann gilt%
\begin{equation}\label{eq:Matrixfunktion-Cauchy}
f(A) = \frac{1}{2\pi\ui}\int_{\partial G} f(z)(zE-A)^{-1}\,\mathrm dz.
\end{equation}
Die Formeln \eqref{eq:Sylvesters-Formel} und
\eqref{eq:Matrixfunktion-Cauchy} liefern außerdem
zwei miteinander verträgliche Verallgemeinerungen der Definition
der Matrixfunktion.

\newpage
\section{Lineare Gleichungssysteme}
\index{lineares Gleichungssytem}
Ein lineares Gleichungssystem mit $m$ Gleichungen und $n$ Unbekannten
hat die Form:
\begin{equation}\label{eq:LGS}
\begin{split}
a_{11} x_1 + a_{12} x_2 + \ldots + a_{1n} x_n &= b_1,\\
a_{21} x_1 + a_{22} x_2 + \ldots + a_{2n} x_n &= b_2,\\
&\;\;\vdots\\
a_{m1} x_1 + a_{m2} x_2 + \ldots + a_{mn} x_n &= b_n.
\end{split}
\end{equation}
Das System lässt sich durch
\begin{equation}
A:=\begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1n}\\
a_{21} & a_{22} & \ldots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{m1} & a_{m1} & \ldots & a_{mn}
\end{bmatrix}
\end{equation}
und
\begin{equation}
x:=\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix},\quad
b:=\begin{bmatrix}
b_1 \\ b_2 \\ \vdots \\ b_n
\end{bmatrix}
\end{equation}
zusammenfassen.

Äquivalente Matrixform von \eqref{eq:LGS}:
\begin{equation}
Ax=b.
\end{equation}
Erweiterte Koeffizientenmatrix:\index{erweiterte Koeffizientenmatrix}
\begin{equation}
(A\,|\,b) := \left[\begin{array}{ccc|c}
a_{11} & \ldots & a_{1n} & b_1\\
\vdots & \ddots & \vdots & \vdots\\
a_{m1} & \ldots & a_{mn} & b_n
\end{array}\right].
\end{equation}
Lösungskriterium:
\begin{equation}
\exists x[Ax=b] \iff \rg(A)=\rg(A\,|\,b).
\end{equation}
Eindeutige Lösung (bei $n$ Unbekannten):
\begin{equation}
\exists! x[Ax=b] \iff\rg(A)=\rg(A\,|\,b)=n.
\end{equation}
Im Fall $m=n$ gilt:
\begin{equation}
\begin{split}
&\exists! x[Ax=b] \iff A\in\operatorname{GL}(n,K)\\
&\iff \rg(A)=n \iff \det(A)\ne 0.
\end{split}
\end{equation}

% \newpage
\section{Multilineare Algebra}
\subsection{Äußeres Produkt}
Sei $V$ ein Vektorraum und sei $v_k\in V$ für alle $k$.

Sind $a=\sum_{k=1}^n a_k v_k$
und $b=\sum_{k=1}^n b_k v_k$ beliebige
Linearkombinationen, so gilt
\begin{equation}
\begin{split}
a\wedge b &= \sum_{i,j} a_i b_j\,v_i\wedge v_j\\
&= \sum_{1\le i<j\le n} (a_i b_j-a_j b_i)\,v_i\wedge v_j
\end{split}
\end{equation}
und
\begin{equation}
\begin{split}
a\wedge b &= a\otimes b-b\otimes a\\
&= \sum_{i,j} (a_i b_j-a_j b_i)\,v_i\otimes v_j\\
&= \sum_{i,j} a_i b_j (v_i\otimes v_j-v_j\otimes v_i).
\end{split}
\end{equation}
\subsubsection{Alternator}\index{Alternator}
Für $a_k\in V$ ist
$\operatorname{Alt}_p\colon T^p(V)\to T^p(V)$
mit
\begin{equation}
\begin{split}
& \operatorname{Alt}_p (a_1\otimes\ldots\otimes a_p)\\
&:= \frac{1}{p!}\sum_{\sigma\in S_{\scriptstyle p}}
\sgn(\sigma)\,(a_{\sigma(1)}\otimes\ldots\otimes a_{\sigma(p)}).
\end{split}
\end{equation}
Mit $A^p(V)$ wird die Bildmenge des Alternators bezeichnet.
Der Raum $\Lambda^p(V)$ wird kanonisch mit $A^p(V)$ identifiziert, indem
\begin{equation}
a_1\wedge\ldots\wedge a_p
= p!\operatorname{Alt}_p(a_1\otimes\ldots\otimes a_p)
\end{equation}
gesetzt wird. Hierdurch wird ein kanonischer Isomorphismus%
\index{kanonischer Isomorphismus!Alternator} zwischen
den Algebren $\Lambda(V)$ und $A(V)$ induziert.

Speziell gilt
\begin{equation}
\operatorname{Alt}_2 (a\otimes b) := \frac{1}{2}(a\otimes b-b\otimes a).
\end{equation}
und
\begin{equation}
a\wedge b = 2\operatorname{Alt}_2(a\otimes b).
\end{equation}

\subsubsection{Äußere Algebra}\index{aussere Algebra@äußere Algebra}
Darstellung als Quotientenraum:
\begin{equation}
\Lambda^2(V) = T^2(V)/\{v\otimes v\mid v\in V\}.
\end{equation}
Dimension: Ist $\dim(V)=n$, so gilt
\begin{equation}
\dim(\Lambda^k(V)) = \binom{n}{k}.
\end{equation}

\clearpage
\section{Analytische Geometrie}
\subsection{Geraden}\index{Gerade}
\subsubsection{Parameterdarstellung}
\index{Parameterdarstellung!einer Geraden}

Die \strong{Punktrichtungsform}\index{Punktrichtungsform} ist
\begin{equation}
p(t) = p_0+t\bv v,
\end{equation}
wobei $p_0$ der Stützpunkt und $\bv v$ der Richtungsvektor ist.
Die Gerade ist dann die Menge $g=\{p(t)\mid t\in\R\}$.
Der Vektor $\bv v$ repräsentiert außerdem die Geschwindigkeit,
mit der diese Parameterdarstellung durchlaufen wird:
$p'(t)=\bv v$.

\strong{Gerade durch zwei Punkte:}
Sind zwei Punkte $p_1,p_2$ mit $p_1\ne p_2$ gegeben, so ist
durch die beiden Punkte eine Gerade gegeben. Für diese Gerade ist
\begin{equation}
p(t) = p_1+t(p_2-p_1)
\end{equation}
eine Punktrichtungsform\index{Punktrichtungsform}.
Durch Umformung ergibt sich die \strong{Zweipunkteform:}
\begin{equation}\label{eq:Zweipunkteform}
p(t) = (1-t)p_1+tp_2.
\end{equation}
Bei \eqref{eq:Zweipunkteform} handelt es sich um eine
Affinkombination. Gilt $t\in[0,1]$, so ist \eqref{eq:Zweipunkteform}
eine Konvexkombination: eine Parameterdarstellung für die Strecke
von $p_1$ nach $p_2$.

\subsubsection{Parameterfreie Darstellung}
\strong{Hesse-Form:}
\begin{equation}\label{eq:Hesse-Form}
g = \{p\mid\langle\bv n, p-p_0\rangle = 0\},
\end{equation}
$p_0$: Stützpunkt, $\bv n$: Normalenvektor.

Die Hesse-Form ist nur in der Ebene möglich.
Form \eqref{eq:Hesse-Form} hat in Koordinaten
die Form
\begin{equation}
\begin{split}
g &= \{(x,y)\mid n_x(x-x_0)+n_y(y-y_0)=0\}\\
&= \{(x,y)\mid n_x x+n_y y = n_x x_0+n_y y_0\}.
\end{split}
\end{equation}

\strong{Hesse-Normalform:} \eqref{eq:Hesse-Form} mit $|\bv n|=1$.


Sei $\bv v\wedge\bv w$ das äußere Produkt.

\strong{Plückerform:}
\begin{equation}
g = \{p\mid (p-p_0)\wedge\bv v=0\}.
\end{equation}
Die Größe $\bv m = p_0\wedge\bv v$ heißt Moment.
Beim Tupel $(\bv v:\bv m)$ handelt es sich um
Plückerkoordinaten für die Gerade.

In der Ebene gilt speziell:
\begin{equation}\label{eq:Gerade-Ebene}
g = \{(x,y)\mid (x-x_0)\Delta y = (y-y_0)\Delta x\}
\end{equation}
mit $\underline v=(\Delta x,\Delta y)$.

Sei $a:=\Delta y$ und $b:=-\Delta x$ und $c:=ax_0+by_0$.
Aus \eqref{eq:Gerade-Ebene} ergibt sich:
\begin{equation}
g = \{(x,y)\mid ax+by=c\}.
\end{equation}
Im Raum ergibt sich ein Gleichungssystem:
\begin{equation}
g = \{\begin{pmatrix}x\\ y\\ z\end{pmatrix}
\mid
\begin{vmatrix}
(x-x_0)\Delta y = (y-y_0)\Delta x\\
(y-y_0)\Delta z = (z-z_0)\Delta y\\
(x-x_0)\Delta z = (z-z_0)\Delta x
\end{vmatrix}\}
\end{equation}
mit $\underline v=(\Delta x,\Delta y,\Delta z)$.

\subsubsection{Abstand Punkt zu Gerade}
Sei $p(t):=p_0+t\bv v$ die Punktrichtungsform einer Geraden und
sei $q$ ein weiterer Punkt. Bei $\bv d(t):=p(t)-q$ handelt
es sich um den Abstandsvektor in Abhängigkeit von $t$.

Ansatz: Es gibt genau ein $t$, so dass gilt:
\begin{equation}
\langle\bv d,\bv v\rangle=0.
\end{equation}
Lösung:
\begin{equation}
t = \frac
  {\langle\bv v,q{-}p_0\rangle}
  {\langle\bv v,\bv v\rangle}.
\end{equation}

\subsection{Ebenen}\index{Ebene}
\subsubsection{Parameterdarstellung}
\index{Parameterdarstellung!einer Ebene}
Seien $\bv u, \bv v$ zwei nicht kollineare Vektoren.

Punktrichtungsform:
\begin{equation}\label{eq:Ebene-Punktrichtungsform}
p(s,t) = p_0+s\bv u+t\bv v.
\end{equation}

\subsubsection{Parameterfreie Darstellung}
Seien $\bv v, \bv w$ zwei nicht kollineare Vektoren.
Durch
\begin{equation}
E = \{p\mid (p-p_0)\wedge\bv v\wedge\bv w=0\}.
\end{equation}
wird eine Ebene beschrieben.

\strong{Hesse-Form:}
\begin{equation}
E = \{p\mid \langle\bv n,p-p_0\rangle=0\},
\end{equation}
$p_0$: Stützpunkt, $\bv n$: Normalenvektor. Die Hesse-Form einer
Ebene ist nur im dreidimensionalen Raum möglich.
Den Normalenvektor bekommt man aus \eqref{eq:Ebene-Punktrichtungsform}
mit $\bv n = \bv u\times\bv v$.

Es gilt:
\begin{equation}
\langle\bv n,p-p_0\rangle\iff \langle\bv n,p\rangle = \langle\bv n,p_0\rangle.
\end{equation}
Über den Zusammenhang $\bv n=(a,b,c)$, $p=(x,y,z)$ und $d=\langle\bv n,p_0\rangle$
ergibt sich die

\strong{Koordinatenform:}
\begin{equation}
E = \{(x,y,z)\mid ax+by+cz = d\}.
\end{equation}

\subsubsection{Abstand Punkt zu Ebene}
Sei $p(s,t):=p_0+s\bv u+t\bv v$ die Punktrichtungsform einer Ebene
und sei $q$ ein weiterer Punkt. Bei $\bv d(s,t):=p-q$ handelt es sich um
den Abstandsvektor in Abhängigkeit von $(s,t)$.

Ansatz: Es gibt genau ein Tupel $(s,t)$, so dass gilt:
\begin{equation}
\langle\bv d,\bv u\rangle=0\enspace\text{und}\enspace
\langle\bv d,\bv v\rangle=0.
\end{equation}
Dieser Ansatz führt zum LGS
\begin{equation}
\begin{bmatrix}
\langle\bv u,\bv v\rangle & \langle\bv v,\bv v\rangle\\
\langle\bv v,\bv v\rangle & \langle\bv u,\bv v\rangle
\end{bmatrix}
\begin{bmatrix}
s\\ t
\end{bmatrix}
= \begin{bmatrix}
\langle\bv v,q{-}p_0\rangle\\
\langle\bv u,q{-}p_0\rangle
\end{bmatrix}.
\end{equation}
Bemerkung: Die Systemmatrix $g_{ij}$ ist der metrische Tensor für die
Basis $B=(\bv u,\bv v)$. Die Lösung des LGS ist
\begin{gather}
s = \frac
  {\langle g_{12}\bv v-g_{12}\bv u, q{-}p_0\rangle}
  {g_{11}^2-g_{12}^2},\\
t = \frac
  {\langle g_{12}\bv u-g_{12}\bv v, q{-}p_0\rangle}
  {g_{11}^2-g_{12}^2}.
\end{gather}

