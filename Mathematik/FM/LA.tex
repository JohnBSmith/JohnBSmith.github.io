
\chapter{Lineare Algebra}
\section{Grundbegriffe}
\subsection{Norm}\index{Norm}
\begin{Definition}
Eine Abbildung $v\mapsto\|v\|$ von einem
Vektorraum $V$ über dem Körper $K$ in die nichtnegativen reellen
Zahlen heißt \emdef{Norm}, wenn für alle $v,w\in V$ und $a\in K$
die drei Axiome%
\begin{gather}
\|v\|=0 \implies v=0,\\
\|av\| = |a|\,\|v\|,\\
\|v+w\| \le \|v\|+\|w\|
\end{gather}
erfüllt sind.
\end{Definition}

Eigenschaften:
\begin{gather}
\|v\|=0\iff v=0,\\
\|-v\|=\|v\|,\\
\|v\|\ge 0.
\end{gather}
Dreiecksungleichung nach unten:
\begin{equation}
|\|v\|-\|w\||\le \|v-w\|.
\end{equation}

\subsection{Skalarprodukt}\index{Skalarprodukt}

\subsubsection{Axiome}
{\definition}
Eine Abbildung heißt \emdef{Skalarprodukt},
wenn folgende Axiome erfüllt sind.

Axiome für $v,w$ aus einem reellen Vektorraum und $\lambda$ ein Skalar:
\begin{gather}
\langle v,w\rangle = \langle w,v\rangle,\\
\langle v,\lambda w\rangle = \lambda\langle v,w\rangle,\\
\langle v,w_1+w_2\rangle = \langle v,w_1\rangle +\langle v,w_2\rangle,\\
\langle v,v\rangle\ge 0,\\
\langle v,v\rangle=0 \iff v=0.
\end{gather}
Axiome für $v,w$ aus einem komplexen Vektorraum und $\lambda$ ein Skalar:
\begin{gather}
\langle v,w\rangle = \overline{\langle w,v\rangle},\\
\langle \lambda v,w\rangle = \overline{\lambda}\langle v,w\rangle,\\
\langle v,\lambda w\rangle = \lambda\langle v,w\rangle,\\
\langle v,w_1+w_2\rangle = \langle v,w_1\rangle +\langle v,w_2\rangle,\\
\langle v,v\rangle\ge 0,\\
\langle v,v\rangle=0 \iff v=0.
\end{gather}

\subsubsection{Eigenschaften}
Das reelle Skalarprodukt ist eine symmetrische bilineare Abbildung.

\subsubsection{Winkel und Längen}
{\definition}
Der \emdef{Winkel} $\varphi$ zwischen $v$ und $w$
ist definiert durch die Beziehung:
\begin{equation}
\langle v,w\rangle = \|v\|\,\|w\|\,\cos\varphi.
\end{equation}
{\definition}
\emdef{Orthogonal}:\index{Orthogonal}
\begin{equation}
v\perp w \;:\Longleftrightarrow\; \langle v,w\rangle=0.
\end{equation}
Ein Skalarprodukt $\langle v,w\rangle$ induziert die Norm
\begin{equation}
\|v\| := \sqrt{\langle v,v\rangle}.
\end{equation}

\subsubsection{Orthonormalbasis}\label{sec:ONB}
\index{Orthogonalsystem}\index{Orthogonalbasis}
\index{Orthonormalsystem}\index{Orthonormalbasis}
Sei $B=(b_k)_{k=1}^n$ eine Basis eines endlichdimensionalen
Vektorraumes über den reellen oder komplexen Zahlen.

{\definition}
Gilt $\langle b_i,b_j\rangle=0$
für alle $i,j$ mit $i\ne j$, so wird $B$ \emdef{Orthogonalbasis}
genannt. Ist $B$ nicht unbedingt eine Basis, so spricht man von einem 
\emdef{Orthogonalsystem}.

{\definition}
Ist $B$ eine Orthogonalbasis und gilt
zusätzlich $\langle b_k,b_k\rangle=1$ für alle $k$, so wird
$B$ \emdef{Orthonormalbasis} (ONB) genannt. Ist $B$ nicht unbedingt
eine Basis,  so spricht man von einem \emdef{Orthonormalsystem}.

Sei $v=\sum_k v_kb_k$ und $w=\sum_k w_kb_k$.
Mit $\sum_k$ ist immer $\sum_{k=1}^n$ gemeint.

Ist $B$ eine Orthonormalbasis, so gilt:
\begin{equation}
\langle v,w\rangle = \sum_k \overline{v_k}\,w_k.
\end{equation}
Ist $B$ nur eine Orthogonalbasis, so gilt:
\begin{equation}
\langle v,w\rangle = \sum_k \langle b_k,b_k\rangle \overline{v_k}\,w_k
\end{equation}
Allgemein gilt:
\begin{equation}
\langle v,w\rangle = \sum_{i,j} g_{ij} \overline{v_i}\,w_j
\end{equation}
mit $g_{ij}=\langle b_i,b_j\rangle$. In reellen Vektorräumen
ist die komplexe Konjugation wirkungslos und kann somit entfallen.

Ist $B$ eine Orthogonalbasis und $v=\sum_k v_k b_k$, so gilt:
\begin{equation}
v_k = \frac{\langle b_k,v\rangle}{\langle b_k,b_k\rangle}.
\end{equation}
Ist $B$ eine Orthonormalbasis, so gilt speziell:
\begin{equation}
v_k = \langle b_k,v\rangle.
\end{equation}


\subsubsection{Orthogonale Projektion}
Orthogonale Projektion von $v$ auf $w$:
\begin{equation}
P[w](v) := \frac{\langle v,w\rangle}{\langle w,w\rangle}\,w.
\end{equation}
\subsubsection{Gram-Schmidt-Verfahren}
Für linear unabhängige Vektoren $v_1,\ldots,v_n$
wird durch%
\begin{equation}
w_k := v_k - \sum_{i=1}^{k-1} P[w_i](v_k)
\end{equation}
ein Orthogonalsystem $w_1,\ldots,w_n$ berechnet.

Speziell für zwei nicht kollineare Vektoren $v_1,v_2$ gilt
\begin{gather}
w_1=v_1,\\
w_2=v_2-P[w_1](v_2).
\end{gather}
\section{Koordinatenvektoren}
\subsection{Koordinatenraum}
Addition von $a,b\in K^n$:
\begin{equation}\label{eq:Koordinatenraum-Addition}
\begin{bmatrix}
a_1\\
\vdots\\
a_n
\end{bmatrix}
+\begin{bmatrix}
b_1\\
\vdots\\
b_n
\end{bmatrix}
:= \begin{bmatrix}
a_1+b_1\\
\vdots\\
a_n+b_n
\end{bmatrix}.
\end{equation}
Subtraktion:
\begin{equation}
\begin{bmatrix}
a_1\\
\vdots\\
a_n
\end{bmatrix}
-\begin{bmatrix}
b_1\\
\vdots\\
b_n
\end{bmatrix}
:= \begin{bmatrix}
a_1-b_1\\
\vdots\\
a_n-b_n
\end{bmatrix}.
\end{equation}
Skalarmultiplikation von $\lambda\in K$ mit $a\in K^n$:
\begin{align}\label{eq:Koordinatenraum-Skalarmultiplikation}
\lambda\begin{bmatrix}
a_1\\
\vdots\\
a_n
\end{bmatrix}
:= \begin{bmatrix}
\lambda a_1\\
\vdots\\
\lambda a_n
\end{bmatrix}.
\end{align}
Ist $K$ ein Körper, so bildet die Menge
\begin{equation}
K^n = \{(a_1,\ldots,a_n)\mid \forall k\colon a_k\in K\}
\end{equation}
bezüglich der Addition \eqref{eq:Koordinatenraum-Addition}
und der Multiplikation \eqref{eq:Koordinatenraum-Skalarmultiplikation}
einen Vektorraum, der \emdef{Koordinatenraum} genannt wird.
Das Tupel $E_n=(e_1,\ldots,e_n)$ mit
\begin{equation}\label{eq:kanonische-Basis}
\begin{split}
e_1 &:= (1,0,0,0,\ldots, 0),\\
e_2 &:= (0,1,0,0,\ldots, 0),\\
e_3 &:= (0,0,1,0,\ldots, 0),\\
\ldots\\
e_n &:= (0,0,0,0,\ldots, 1)
\end{split}
\end{equation}
bildet eine geordnete Basis von $K^n$, die \emdef{kanonische Basis}
genannt wird. Es gilt
\begin{equation}
a = (a_1,\ldots,a_n) = a_1 e_1+\ldots+a_n e_n.
\end{equation}

\subsection{Kanonisches Skalarprodukt}
\begin{Definition}
Für $a,b\in\R^n$:
\begin{equation}
\langle a,b\rangle := \sum_{k=1}^n a_k b_k.
\end{equation}
Für $a,b\in\C^n$:
\begin{equation}
\langle a,b\rangle := \sum_{k=1}^n \overline{a_k}\,b_k
\end{equation}
\end{Definition}
\noindent
Die kanonische Basis \eqref{eq:kanonische-Basis} ist eine
Orthonormalbasis bezüglich diesem Skalarprodukt, s. \ref{sec:ONB}.
Das Skalarprodukt induziert die Norm
\begin{equation}
|a| := \sqrt{\langle a,a\rangle} = \sqrt{\textstyle \sum_{k=1}^n |a_k|^2},
\end{equation}
die \emdef{Vektorbetrag} genannt wird.

Jedem Koordinatenvektor $a\ne 0$ lässt sich ein Einheitsvektor
$\hat a:=\frac{a}{|a|}$ zuordnen, der in Richtung von $a$ zeigt
und die Eigenschaft $|\hat a|=1$ besitzt.


\section{Matrizen}\index{Matrix}
\subsection{Quadratische Matrizen}%
\index{Matrix!quadratische}\index{quadratische Matrix}
\subsubsection{Matrizenring}%
\index{Ring!Matrizenring}\index{Matrizenring}
Mit $K^{n\times n}$ wird die Menge quadratischen Matrizen
\begin{equation}
(a_{ij}) = \begin{bmatrix}
a_{11} & \ldots & a_{1n}\\
\ldots & \ddots & \ldots\\
a_{n1} & \ldots & a_{nn}
\end{bmatrix}
\end{equation}
mit Einträgen $a_{ij}$ aus dem Körper $K$ bezeichnet.

Die Menge $K^{n\times n}$ bildet bezüglich Addition
und Multiplikation von Matrizen einen Ring (s. \ref{sec:Strukturen}).

Das neutrale Element der Multiplikation
ist die Einheitsmatrix
\begin{equation}
E_n = (\delta_{ij}),\quad
\delta_{ij}:=\begin{cases}
1 & \text{wenn}\;i=j,\\
0 & \text{sonst}.
\end{cases}
\end{equation}
Das sind
\begin{equation}
E_2 = \begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix},\quad
E_3 = \begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{bmatrix},
\quad\text{usw.}
\end{equation}

\subsubsection{Symmetrische Matrizen}
Eine quadratiche Matrix $A=(a_{ij})$ heißt
symmetrisch\index{symmetrische Matrix},
falls gilt $a_{ij}=a_{ji}$ bzw. $A^T=A$.

Jede reelle symmetrische Matrix besitzt ausschließlich reelle
Eigenwerte und die algebraischen Vielfachheiten stimmen mit den
geometrischen Vielfachheiten überein.

Jede reelle symmetrische Matrix $A$ ist diagonalisierbar, d.\,h. es gibt
eine invertierbare Matrix $T$ und eine Diagonalmatrix $D$, so dass
$A=TDT^{-1}$ gilt.

Sei $V$ ein $K$-Vektorraum und $(b_k)_{k=1}^n$ eine Basis von $V$.
Für jede symmetrische Bilinearform\index{symmetrische Bilinearform}
$f\colon V^2\to K$ ist die
Darstellungsmatrix
\begin{equation}
A = (f(b_i,b_j))
\end{equation}
symmetrisch. Ist $A\in K^{n\times n}$ eine symmetrische Matrix, so
ist
\begin{equation}\label{eq:symmBf}
f(x,y) = x^T A y.
\end{equation}
eine symmetrische Bilinearform für  $x,y\in K^n$.
Ist $K=\R$ und $A$ positiv definit, so ist
\eqref{eq:symmBf} ein Skalarprodukt auf $\R^n$.

\subsubsection{Reguläre Matrizen}\index{inverse Matrix}
Eine quadratische Matrix $A\in K^{n\times n}$ heißt \emdef{regulär}
oder \emdef{invertierbar}, wenn es eine inverse Matrix $A^{-1}$ gibt,
so dass
\begin{equation}
A^{-1}A = E_n \quad (\iff AA^{-1} = E_n)
\end{equation}
gilt, wobei mit $E_n$ die Einheitsmatrix gemeint ist. Jede
reguläre Matrix besitzt genau eine inverse Matrix. Eine Matrix $A$
ist genau dann regulär, wenn $\det(A)\ne 0$ gilt. Die Menge
der regulären Matrizen bildet bezüglich Matrizenmultiplikation
eine Gruppe, die
\emdef{allgemeine lineare Gruppe}\index{allgemeine lineare Gruppe}
\begin{equation}
\operatorname{GL}(n,K) := \{A\in K^{n\times n}\mid\det(A)\ne 0\}.
\end{equation}
Ist $V$ ein Vektorraum über dem Körper $K$, so bilden die
Automorphismen bezüglich Verkettung eine Gruppe, die
\emph{Automorphismengruppe}
\begin{equation}
\operatorname{GL}(V) = \operatorname{Aut}(V).
\end{equation}
Ein \emdef{Endomorphismus}\index{Endomorphismus!auf einem Vektorraum}
ist eine lineare Abbildung, welche eine Selbstabbildung ist.
Ein \emdef{Automorphismus}\index{Automorphismus!auf einem Vektorraum}
ist eine bijektiver Endomorphismus.

Wählt man auf $V$ eine Basis
$B$, so ist die Zuordnung der Darstellungsmatrix
\begin{equation}
M_B^B\colon \operatorname{Aut}(V)\to\operatorname{GL}(\dim V,K)
\end{equation}
eine Gruppenisomorphismus.

Eine quadratische Matrix, die nicht regulär ist, heißt
\emdef{singulär}. Endomorphismen, die nicht bijektiv sind, lassen
die Dimension ihrer Definitionsmenge schrumpfen:
\begin{equation}
f{\in}\operatorname{End}(V){\setminus}\operatorname{Aut}(V)
\Longleftrightarrow \dim f(V)<\dim V.
\end{equation}
Für Matrizen $A\in K^{n\times n}$ bedeutet das, dass sie nicht
den vollen Rang besitzen:
\begin{equation}
\det A=0\iff \operatorname{rk}(A) < n = \dim K^n.
\end{equation}
Inversionsformel:
\begin{equation}
\begin{bmatrix}
a & b\\
c & d
\end{bmatrix}^{-1}
= \frac{1}{ad-bc}\begin{bmatrix}
d & -b\\
-c & a
\end{bmatrix}.
\end{equation}
\begin{Definition}
Wird in der Matrix $A$ die Zeile $i$ und die Spalte $j$ entfernt,
so entsteht eine neue Matrix $[A]_{ij}$, die
\emdef{Streichungsmatrix}\index{Streichungsmatrix}
von $A$ genannt wird.
\end{Definition}
Laplacescher Entwicklungssatz:
\begin{align}
\det A = \sum_{i=1}^n (-1)^{i+j}a_{ij}\det([A]_{ij}),\\
\det A = \sum_{j=1}^n (-1)^{i+j}a_{ij}\det([A]_{ij}).
\end{align}

\subsection{Determinanten}\index{Determinante}
Für Matrizen $A,B\in K^{n\times n}$ und $r\in K$ gilt:
\begin{gather}
\det(AB) = \det(A)\det(B),\\
\det(A^T) = \det(A),\\
\det(rA) = r^n\det(A),\\
\det(A^{-1}) = \det(A)^{-1}.
\end{gather}
Für eine Diagonalmatrix $D=\diag(d_1,\ldots,d_n)$ gilt:
\begin{gather}
\det(D) = \prod_{k=1}^n d_k.
\end{gather}
Eine linke Dreiecksmatrix ist eine Matrix der Form
$(a_{ij})$ mit $a_{ij}=0$ für $i<j$. Eine rechte
Dreiecksmatrix ist die Transponierte einer linken
Dreiecksmatrix.

Für eine linke oder rechte Dreiecksmatrix $A=(a_{ij})$ gilt:
\begin{gather}
\det(A) = \prod_{k=1}^n a_{kk}.
\end{gather}

\subsection{Eigenwerte}
\strong{Eigenwertproblem:}\index{Eigenwert}
Für eine gegebene quadratische Matrix $A$ bestimme
\begin{equation}
\{(\lambda,v)\mid Av = \lambda v,\,v\ne 0\}.
\end{equation}
Das homogene lineare Gleichungssystem
\begin{equation}
Av=\lambda v \iff (A-\lambda E_n)v=0
\end{equation}
besitzt Lösungen $v\ne 0$ gdw.
\begin{equation}
p(\lambda)=\det(A-\lambda E_n)=0.
\end{equation}
Bei $p(\lambda)$ handelt es sich um ein normiertes Polynom
vom Grad $n$, das \emdef{charakeristisches Polynom}
\index{charakteristisches Polynom}
genannt wird.

\strong{Eigenraum:}\index{Eigenraum}
\begin{equation}
\Eig(A,\lambda):=\{v\mid Av=\lambda v\}.
\end{equation}
Die Dimension $\dim\Eig(A,\lambda)$ wird
\emdef{geometrische Vielfachheit}\index{geometrische Vielfachheit}
von $\lambda$ genannt.

\strong{Spektrum:}\index{Spektrum}
\begin{equation}
\sigma(A) := \{\lambda\mid \exists v\ne 0\colon Av=\lambda v\}.
\end{equation}

\section{Lineare Gleichungssysteme}
\index{lineares Gleichungssytem}
Ein lineares Gleichungssystem mit $m$ Gleichungen und $n$ Unbekannten
hat die Form:
\begin{equation}\label{eq:LGS}
\begin{split}
a_{11} x_1 + a_{12} x_2 + \ldots + a_{1n} x_n &= b_1,\\
a_{21} x_1 + a_{22} x_2 + \ldots + a_{2n} x_n &= b_2,\\
&\;\;\vdots\\
a_{m1} x_1 + a_{m2} x_2 + \ldots + a_{mn} x_n &= b_n.
\end{split}
\end{equation}
Das System lässt sich durch
\begin{equation}
A:=\begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1n}\\
a_{21} & a_{22} & \ldots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{m1} & a_{m1} & \ldots & a_{mn}
\end{bmatrix}
\end{equation}
und
\begin{equation}
x:=\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix},\quad
b:=\begin{bmatrix}
b_1 \\ b_2 \\ \vdots \\ b_n
\end{bmatrix}
\end{equation}
zusammenfassen.

Äquivalente Matrixform von \eqref{eq:LGS}:
\begin{equation}
Ax=b.
\end{equation}
Erweiterte Koeffizientenmatrix:\index{erweiterte Koeffizientenmatrix}
\begin{equation}
(A\,|\,b) := \left[\begin{array}{ccc|c}
a_{11} & \ldots & a_{1n} & b_1\\
\vdots & \ddots & \vdots & \vdots\\
a_{m1} & \ldots & a_{mn} & b_n
\end{array}\right].
\end{equation}
Lösungskriterium:
\begin{equation}
\exists x[Ax=b] \iff \rg(A)=\rg(A\,|\,b).
\end{equation}
Eindeutige Lösung (bei $n$ Unbekannten):
\begin{equation}
\exists! x[Ax=b] \iff\rg(A)=\rg(A\,|\,b)=n.
\end{equation}
Im Fall $m=n$ gilt:
\begin{equation}
\begin{split}
&\exists! x[Ax=b] \iff A\in\operatorname{GL}(n,K)\\
&\iff \rg(A)=n \iff \det(A)\ne 0.
\end{split}
\end{equation}

\newpage
\section{Multilineare Algebra}
\subsection{Äußeres Produkt}
Sei $V$ ein Vektorraum und sei $v_k\in V$ für alle $k$.

Sind $a=\sum_{k=1}^n a_k v_k$
und $b=\sum_{k=1}^n b_k v_k$ beliebige
Linearkombinationen, so gilt
\begin{equation}
\begin{split}
a\wedge b &= \sum_{i,j} a_i b_j\,v_i\wedge v_j\\
&= \sum_{1\le i<j\le n} (a_i b_j-a_j b_i)\,v_i\wedge v_j
\end{split}
\end{equation}
und
\begin{equation}
\begin{split}
a\wedge b &= a\otimes b-b\otimes a\\
&= \sum_{i,j} (a_i b_j-a_j b_i)\,v_i\otimes v_j\\
&= \sum_{i,j} a_i b_j (v_i\otimes v_j-v_j\otimes v_i).
\end{split}
\end{equation}
\subsubsection{Alternator}\index{Alternator}
Für $a_k\in V$ ist
$\operatorname{Alt}_p\colon T^p(V)\to A^p(V)\subseteq T^p(V)$
mit
\begin{equation}
\begin{split}
& \operatorname{Alt}_p (a_1\otimes\ldots\otimes a_p)\\
&:= \frac{1}{p!}\sum_{\sigma\in S_{\scriptstyle p}}
\sgn(\sigma)\,(a_{\sigma(1)}\otimes\ldots\otimes a_{\sigma(p)}).
\end{split}
\end{equation}
Es ist $\Lambda^p(V)$ isomorph zu $A^p(V)$ und man setzt:
\begin{equation}
a_1\wedge\ldots\wedge a_p
= p!\operatorname{Alt}_p(a_1\otimes\ldots\otimes a_p).
\end{equation}
Speziell gilt
\begin{equation}
\operatorname{Alt}_2 (a\otimes b) := \frac{1}{2}(a\otimes b-b\otimes a).
\end{equation}
und
\begin{equation}
a\wedge b = 2\operatorname{Alt}_2(a\otimes b).
\end{equation}

\subsubsection{Äußere Algebra}\index{aussere Algebra@äußere Algebra}
Darstellung als Quotientenraum:
\begin{equation}
\Lambda^2(V) = T^2(V)/\{v\otimes v\mid v\in V\}.
\end{equation}
Dimension: Ist $\dim(V)=n$, so gilt
\begin{equation}
\dim(\Lambda^k(V)) = \binom{n}{k}.
\end{equation}

\clearpage
\section{Analytische Geometrie}
\subsection{Geraden}\index{Gerade}
\subsubsection{Parameterdarstellung}
\index{Parameterdarstellung!einer Geraden}

\strong{Punktrichtungsform:}\index{Punktrichtungsform}
\begin{equation}
p(t) = p_0+t\underline v,
\end{equation}
$p_0$: Stützpunkt, $\underline v$: Richtungsvektor.
Die Gerade ist dann die Menge $g=\{p(t)\mid t\in\R\}$.

Der Vektor $\underline v$ repräsentiert außerdem die Geschwindigkeit,
mit der diese Parameterdarstellung durchlaufen wird:
$p'(t)=\underline v$.

\strong{Gerade durch zwei Punkte:}
Sind zwei Punkte $p_1,p_2$ mit $p_1\ne p_2$ gegeben, so ist
durch die beiden Punkte eine Gerade gegeben. Für diese Gerade ist
\begin{equation}
p(t) = p_1+t(p_2-p_1)
\end{equation}
eine Punktrichtungsform\index{Punktrichtungsform}.
Durch Umformung ergibt sich die \strong{Zweipunkteform:}
\begin{equation}\label{eq:Zweipunkteform}
p(t) = (1-t)p_1+tp_2.
\end{equation}
Bei \eqref{eq:Zweipunkteform} handelt es sich um eine
Affinkombination. Gilt $t\in[0,1]$, so ist \eqref{eq:Zweipunkteform}
eine Konvexkombination: eine Parameterdarstellung für die Strecke
von $p_1$ nach $p_2$.

\subsubsection{Parameterfreie Darstellung}
\strong{Hesse-Form:}
\begin{equation}\label{eq:Hesse-Form}
g = \{p\mid\langle \uv n,p-p_0\rangle = 0\},
\end{equation}
$p_0$: Stützpunkt, $\uv n$: Normalenvektor.

Die Hesse-Form ist nur in der Ebene möglich.
Form \eqref{eq:Hesse-Form} hat in Koordinaten
die Form
\begin{equation}
\begin{split}
g &= \{(x,y)\mid n_x(x-x_0)+n_y(y-y_0)=0\}\\
&= \{(x,y)\mid n_x x+n_y y = n_x x_0+n_y y_0\}.
\end{split}
\end{equation}

\strong{Hesse-Normalform:} \eqref{eq:Hesse-Form} mit $|\uv n|=1$.


Sei $\uv v\wedge\uv w$ das äußere Produkt.

\strong{Plückerform:}
\begin{equation}
g = \{p\mid (p-p_0)\wedge \underline v=0\}.
\end{equation}
Die Größe $\underline m = p_0\wedge\underline v$ heißt Moment.
Beim Tupel $(\underline v:\underline m)$ handelt es sich um
Plückerkoordinaten für die Gerade.

In der Ebene gilt speziell:
\begin{equation}\label{eq:Gerade-Ebene}
g = \{(x,y)\mid (x-x_0)\Delta y = (y-y_0)\Delta x\}
\end{equation}
mit $\underline v=(\Delta x,\Delta y)$.

Sei $a:=\Delta y$ und $b:=-\Delta x$ und $c:=ax_0+by_0$.
Aus \eqref{eq:Gerade-Ebene} ergibt sich:
\begin{equation}
g = \{(x,y)\mid ax+by=c\}.
\end{equation}
Im Raum ergibt sich ein Gleichungssystem:
\begin{equation}
g = \{\begin{pmatrix}x\\ y\\ z\end{pmatrix}
\mid
\begin{vmatrix}
(x-x_0)\Delta y = (y-y_0)\Delta x\\
(y-y_0)\Delta z = (z-z_0)\Delta y\\
(x-x_0)\Delta z = (z-z_0)\Delta x
\end{vmatrix}\}
\end{equation}
mit $\underline v=(\Delta x,\Delta y,\Delta z)$.

\subsubsection{Abstand Punkt zu Gerade}
Sei $p(t):=p_0+t\underline v$ die Punktrichtungsform einer Geraden und
sei $q$ ein weiterer Punkt. Bei $\underline d(t):=p(t)-q$ handelt
es sich um den Abstandsvektor in Abhängigkeit von $t$.

Ansatz: Es gibt genau ein $t$, so dass gilt:
\begin{equation}
\langle\underline d,\underline v\rangle=0.
\end{equation}
Lösung:
\begin{equation}
t = \frac
  {\langle\underline v,q{-}p_0\rangle}
  {\langle\underline v,\underline v\rangle}.
\end{equation}

\subsection{Ebenen}\index{Ebene}
\subsubsection{Parameterdarstellung}
\index{Parameterdarstellung!einer Ebene}
Seien $\uv u, \uv v$ zwei nicht kollineare Vektoren.

Punktrichtungsform:
\begin{equation}\label{eq:Ebene-Punktrichtungsform}
p(s,t) = p_0+s\uv u+t\uv v.
\end{equation}

\subsubsection{Parameterfreie Darstellung}
Seien $\uv v, \uv w$ zwei nicht kollineare Vektoren.
Durch
\begin{equation}
E = \{p\mid (p-p_0)\wedge\uv v\wedge\uv w=0\}.
\end{equation}
wird eine Ebene beschrieben.

\strong{Hesse-Form:}
\begin{equation}
E = \{p\mid \langle\uv n,p-p_0\rangle=0\},
\end{equation}
$p_0$: Stützpunkt, $\uv n$: Normalenvektor. Die Hesse-Form einer
Ebene ist nur im dreidimensionalen Raum möglich.
Den Normalenvektor bekommt man aus \eqref{eq:Ebene-Punktrichtungsform}
mit $\uv n = \uv u\times\uv v$.

\subsubsection{Abstand Punkt zu Ebene}
Sei $p(s,t):=p_0+s\uv u+t\uv v$ die Punktrichtungsform einer Ebene
und sei $q$ ein weiterer Punkt. Bei $\uv d(s,t):=p-q$ handelt es sich um
den Abstandsvektor in Abhängigkeit von $(s,t)$.

Ansatz: Es gibt genau ein Tupel $(s,t)$, so dass gilt:
\begin{equation}
\langle\uv d,\uv u\rangle=0\enspace\text{und}\enspace
\langle\uv d,\uv v\rangle=0.
\end{equation}
Lösung: Es ergibt sich ein LGS:
\begin{equation}
\begin{bmatrix}
\langle\uv u,\uv v\rangle & \langle\uv v,\uv v\rangle\\
\langle\uv v,\uv v\rangle & \langle\uv u,\uv v\rangle
\end{bmatrix}
\begin{bmatrix}
s\\ t
\end{bmatrix}
= \begin{bmatrix}
\langle\uv v,q{-}p_0\rangle\\
\langle\uv u,q{-}p_0\rangle
\end{bmatrix}.
\end{equation}
Bemerkung: Die Systemmatrix $g_{ij}$ ist der metrische Tensor für die
Basis $B=(\uv u,\uv v)$. Die Lösung des LGS ist:
\begin{gather}
s = \frac
  {\langle g_{12}\uv v-g_{12}\uv u, q{-}p_0\rangle}
  {g_{11}^2-g_{12}^2},\\
t = \frac
  {\langle g_{12}\uv u-g_{12}\uv v, q{-}p_0\rangle}
  {g_{11}^2-g_{12}^2}.
\end{gather}

