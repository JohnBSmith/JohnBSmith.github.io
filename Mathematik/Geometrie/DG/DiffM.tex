
\chapter{Differenzierbare Mannigfaltigkeiten}

\section{Differentialgleichungen}

Sei $M$ eine differenzierbare Mannigfaltigkeit und sei $c\colon\R\to M$
eine differenzierbare Parameterkurve in $M$. Daher gibt es auch
$c'\colon\R\to TM$. Das bringt uns auf die folgende Idee. Hat man
ein durch $t$ parametrisiertes Vektorfeld
\begin{equation}
f\colon\R\times M\to TM,\quad f(t,p)\in T_p M,
\end{equation}
gegeben, dann lässt sich das Anfangswertproblem
\begin{equation}
c'(t) = f(t,c(t)),\quad c_0=c(t_0),\quad 
\end{equation}
formulieren. Für eine abstrakte Mannigfaltigkeit muss das
Anfangswertproblem aber nun in lokale Koordinaten übersetzt
werden. Sei dazu $\varphi$ eine lokale Karte und $c=\varphi\circ x$.
Anwendung der Kettenregel ergibt
\begin{equation}
c'(t) = (\varphi\circ x)'(t) = \mathrm d\varphi_{x(t)}(x'(t)).
\end{equation}
Aus $\mathrm d\varphi_{x(t)}(x'(t))=f(t,\varphi(x(t)))$ erhält man
\begin{equation}
x'(t) = f_\varphi(t,x(t)),\quad
f_\varphi(t,x):=\mathrm d\varphi_{x}^{-1}(f(t,\varphi(x)).
\end{equation}
In lokalen Koordinaten lässt sich das Anfangswertproblem also als
ein gewöhnliches Anfangswertproblem im Koordinatenraum darstellen.
Für eine abstrakte Mannigfaltigkeit ist auch nur $f_\varphi$ bekannt.
Wir wollen nun aber in Erfahrung bringen, welche Gestalt das
Anfangswertproblem nach einem Kartenwechsel bekommt. Sei $\psi$
dazu eine zweite Karte. Daraus ergibt sich der Vergleich
\begin{equation}
f(t,p)
= \mathrm d\varphi_x(f_\varphi(t,x))
= \mathrm d\psi_{\tilde x}(f_\psi(t,\tilde x)),\quad
p = \varphi(x) = \psi(\tilde x).
\end{equation}
Umformen bringt $x = (\varphi^{-1}\circ\psi)(\tilde x)$ und
\begin{equation}
f_\varphi(t,x) = (\mathrm d\varphi_x^{-1}\circ \mathrm d\psi_{\tilde x})(f_\psi(t,\tilde x))
= \mathrm d(\varphi^{-1}\circ \psi)_{\tilde x}(f_\psi(t,\tilde x)).
\end{equation}
Es gilt aber auch
\begin{equation}
x'(t) = (\varphi^{-1}\circ\psi\circ\tilde x)'(t)
= \mathrm d(\varphi^{-1}\circ\psi)_{\tilde x(t)}(\tilde x'(t)).
\end{equation}
Das Differential des Kartenwechsels hebt sich weg und man
erhält%
\begin{equation}
\tilde x'(t) = f_\psi(t,\tilde x(t))
= \mathrm d(\psi^{-1}\circ\varphi)_{x(t)}(f_\varphi(t,x(t))).
\end{equation}
Bei der Transformation des Anfangswertproblems werden die Werte des
parametrisierten Vektorfelds also mit dem Differential der
Kartenwechselabbildung transformiert.

\newpage
\section{Dynamische Systeme}

\begin{definition}[Dynamisches System]\mbox{}\\*
Ein \emdef{dynamisches System} ist eine Abbildung
$\Phi\colon T\times M\to M$, welche
die beiden Eigenschaften
\begin{gather}
\label{eq:dyn-Sys-1}
\Phi(0,x) = x,\\
\label{eq:dyn-Sys-2}
\Phi(t_1+t_2,x) = \Phi(t_2,\Phi(t_1,x))
\end{gather}
erfüllt. Man bezeichnet $M$ als \emdef{Zustandsraum} des Systems und
$T$ als \emdef{Menge der Zeitpunkte}.
Für $T=\R$ oder $T=\R^+$ spricht man von einem
\emdef{Zeit-kontinuierlichen} System, für $T=\Z$ oder $T=\N$ von einem
\emdef{Zeit-diskreten}.
\end{definition}

\noindent
Wir wollen uns nun auf Zeit-kontinuierliche Systeme beschränken,
bei denen der Zustandsraum $M$ eine differenzierbare Mannigfaltigkeit
ist. Man betrachte das autonome Anfangswertproblem
\begin{equation}\label{eq:AWP-M}
c'(t) = f(c(t)),\quad c(0)=c_0,
\end{equation}
wobei das Vektorfeld $f\colon M\to TM$ hinreichend gutartig sein soll,
so dass zu jedem $c_0$ eine eindeutige Lösung $c\colon\R\to M$
existiert. 

\begin{theorem}\label{autonom-fasert}
Besitzt das Anfangswertproblem \eqref{eq:AWP-M} zu jedem $c_0$ eine
eindeutige Lösung, dann verläuft durch jeden Punkt von $M$ genau
eine Lösung, d.\,h. keine zwei Lösungen können sich schneiden
oder berühren.
\end{theorem}
\noindent\strong{Beweis.}
Seien $c_1(t)$ und $c_2(t)$ zwei Lösungen der Dgl. und sei
$c_1(t_1)=c_2(t_2)$. Setze $a=t_1-t_2$, dann gilt $c_2(t_2)=c_1(t_2+a)$.
Es muss gezeigt werden, dass $c_2$ nur Parameter"=verschoben
zu $c_1$ um $a$ ist, d.\,h. $c_2(t)=c_1(t+a)$ für alle $t$. Dazu
definieren wir $c(t):=c_1(t+a)$. Für $c$ ergibt sich das
Anfangswertproblem
\begin{equation}
c'(t) = c_1'(t+a) = f(c_1(t+a)) = f(c(t)),\quad c(t_2)=c_1(t_2+a).
\end{equation}
Außerdem gilt
\begin{equation}
c_2'(t) = f(c_2(t)), \quad c_2(t_2)=c_1(t_2+a).
\end{equation}
Das ist beides das selbe Anfangswertproblem. Da die Lösung nach
Voraussetzung eindeutig ist, muss $c(t)=c_2(t)$
für alle $t$ sein.\;\qedsymbol

\begin{theorem}
Unter der Voraussetzung, dass es zu jedem $c_0$ eine eindeutige
Lösung des Anfangswertproblems \eqref{eq:AWP-M} gibt, ist durch
$\Phi(t,c_0)=c(t)$ ein dynamisches System gegeben.
\end{theorem}
\noindent\strong{Beweis.}
Die Eigenschaft \eqref{eq:dyn-Sys-1} ist gemäß $\Phi(0,c_0)=c_0$
trivial erfüllt. Die Eigenschaft \eqref{eq:dyn-Sys-2} bekommt
die Form $c(t_1+t_2)=\Phi(t_2,c(t_1))$. Nach Satz
\ref{autonom-fasert} verläuft durch den Zustand $c(t_1)$ genau
eine Bahn $p(t)=\Phi(t,c(t_1))$ und nach \eqref{eq:dyn-Sys-1} gilt
$p(0)=\Phi(0,c(t_1))=c(t_1)$. Dann muss $p$ zu $c$ Parameter"=verschoben
sein und es gilt $p(t)=c(t+t_1)$. Setze speziell $t=t_2$.\;\qedsymbol

\newpage
\section{Optimierung unter Nebenbedingungen}

Manchmal möchte man für eine Funktion $f(x,y)$ ein Optimum finden,
wobei der Definitionsbereich durch eine Nebenbedingung $g(x,y)=0$
eingeschränkt wird. Laut dem Verfahren der lagrangeschen
Multiplikatoren ist die Gleichung $\nabla f = \lambda\nabla g$
eine notwendige Bedingung dafür. Eigentlich benötigt man anstelle
der Gradienten bloß die totalen Differentiale, es ist nur so, dass
im Koordinatenraum beides koinzidiert. Infolgedessen ist das
Verfahren allgemein für Funktionen auf Mannigfaltigkeiten
formulierbar.

\begin{theorem}[Notwendiges Kriterium]\mbox{}\\*
Sei $M$ eine zweidimensionale differenzierbare Mannigfaltigkeit.
Seien $f\colon M\to\R$ und $g\colon M\to\R$
differenzierbar. Sei $N:=g^{-1}(0)$, wobei 0 ein regulärer Wert 
von $g$ ist. Hat die Einschränkung $f|_N$ an der Stelle $p$ einen
lokalen Extremwert, dann muss $\mathrm df_p$ kollinear zu
$\mathrm dg_p$ sein, d.\,h. es gibt ein $\lambda\in\R$ mit
$\mathrm df_p = \lambda\mathrm dg_p$, bzw. es ist
$\mathrm df_p\land\mathrm dg_p=0$.
\end{theorem}
\noindent\strong{Beweis.}
Bei einer lokalen Extremstelle $p$ liegt eine waagerechte Tangente vor.
Das bedeutet, dass die Richtungsableitung $\mathrm df_p(v)$ für alle
Vektoren $v$ verschwindet, die tangential an $N$ liegen. Zur
Bestätigung dieser Beobachtung betrachten wir eine Parametrisierung
$\varphi\colon (-\varepsilon,\varepsilon) \to N$ mit
$\varphi(0)=p$ und $\varphi'(0)\ne 0$. Die Prämisse lautet nun
$(f\circ\varphi)'(0)=0$. Gemäß Kettenregel ist
\begin{equation}
0 = (f\circ\varphi)'(0) =  \mathrm df_p(\varphi'(0)).
\end{equation}
Da $\mathrm df_p$ eine lineare Abbildung ist, ist
auch $\mathrm df_p(v)=0$ für alle $v\in\R\varphi'(0)$.
Sei nun $e$ orthogonal zu $\varphi'(0)$, dann ist
$(e,\varphi'(0))$ eine Orthogonalbasis von $T_p M$. Gemäß der
Vorbetrachtung ist $\mathrm df_p$ alleinig von $v\in\R e$ abhängig.

Die Funktion $g$ verschwindet definitionsgemäß auf ihrer
Nullstellenmenge $N$, ist dort also konstant. Demnach
ist auch $(g\circ\varphi)'(0)=0$. Nach der gleichen Argumentation
kann $\mathrm dg_p$ also auch nur von $v\in\R e$ abhängig sein.

Es muss also ein $\lambda\in\R$ geben, so dass
$\mathrm df_p(v) = \mathrm dg_p(\lambda v)$ für alle $v\in T_p M$.
Aufgrund der Homogenität linearer Abbildungen ergibt sich daraus
\begin{equation}
\mathrm df_p(v) = \lambda\mathrm dg_p(v).\;\qedsymbol
\end{equation}
In Retrospektive kann man das ganze auch etwas abstrakter
betrachten: Die Nullstellenmenge $N$ ist eine Untermannigfaltigkeit von
$M$ und es gilt $T_p N=\operatorname{ker}(\mathrm dg_p)=\R\varphi'(0)$.
Die Beobachtung war gewesen, dass
$T_p N\subseteq \operatorname{ker}(\mathrm df_p)$
sein muss.

Hat man für ein lokales Koordinatensystem $\varphi\colon U\to M$ die
lokalen Darstellungen $\tilde f = f\circ\varphi$ und
$\tilde g = g\circ\varphi$, dann bekommt die Bedingung laut
\eqref{eq:Richtungsableitung-lokal} die klassische Form
\begin{equation}
\nabla\tilde f = \lambda\nabla\tilde g.
\end{equation}
Bei der praktischen Betrachtung des Verfahrens tut sich nun die
Frage auf, inwiefern das Verfahren numerischen Methoden zugänglich
ist. Dem Ansatz nach entsteht ein im Allgemeinen nichtlineares
Gleichungssystem, das man auch numerisch lösen könnte. Eigentlich
würde man auch gerne das Gradientenabstiegsverfahren auf das
ursprüngliche Optimierungsproblem anwenden, jedoch ist dieses
zunächst nur auf Optimierungsprobleme ohne Nebenbedingungen anwendbar.
Allerdings gibt es einen Trick, mit dem sich das Optimierungsproblem
mit Nebenbedingungen als eines ohne Nebenbedingungen formulieren lässt,
womit auf Anhieb alle Sätze über solche anwendbar werden.

Für diesen Trick sei
\begin{equation}
L\colon M\times\R\to\R,\quad L(p,\lambda) := f(p)-\lambda g(p).
\end{equation}
Man bezeichne $L$ als Lagrange"=Funktion. Der Optimierung unter
Nebenbedingungen entspricht nun die gewöhnliche Optimierung von $L$.
Um dies einzusehen, gehen wir von der gewöhnlichen notwendigen
Bedingung $\mathrm dL=0$ aus. Einsetzen von $L$ ergibt
\begin{equation}
0 = \mathrm dL = \mathrm df-\lambda\mathrm dg-g\mathrm d\lambda.
\end{equation}
Da $f,g$ nicht von $\lambda$ abhängig sind, ist $\mathrm d\lambda$
linear unabhängig von $\{\mathrm df,\mathrm dg\}$. Dies gestattet
die Aufteilung der Gleichung in $g\mathrm d\lambda=0$
und $\mathrm df-\lambda\mathrm dg=0$. Daraus ergibt sich wie
gewünscht die Nebenbedingung $g=0$ und das notwendige Kriterium
$\mathrm df=\lambda\mathrm dg$.


