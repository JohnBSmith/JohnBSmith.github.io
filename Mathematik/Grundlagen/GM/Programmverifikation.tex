
\chapter{Programmverifikation}

\section{Programme}

Zur Lösung mancher Probleme, Beantwortung mancher Fragen mag man eine
mehr oder weniger festgelegte Methode, ein Verfahren anwenden.
Zur Präzisierung legt man das Verfahren durch einen Algorithmus fest.
Mit konkreten Mitteln, das Verfahren durchführen zu können, entsteht
aus dem Algorithmus ein \emph{Programm}. Sei dem zweiten Weltkrieg kamen
immer leistungsfähigere Rechenmaschinen auf, die immer anspruchsvollere
Aufgaben bewältigten. Die Programme, die auf den Maschinen abliefen,
wurden mit der Zeit immer komplexer.

Reflektiert man eine Weile, kann man zur Sichtweise gelangen, dass
Problemlösung mehr oder weniger allgemein algorithmisch stattfinden
kann. Es scheint, als ob wir uns fast notgedrungen mit Programmen
auseinandersetzen müssen.

Die frühen Computer führten meist numerische Rechnungen aus, die den
Bedürfnissen von Ingenieuren und Naturwissenschaftlern entsprangen.
Leider nicht immer für friedliche Zwecke. Mit der Entwicklung ging
die Auffindung vieler numerischer Verfahren und Ausreizung bereits
bekannter Verfahren einher. Dazu ein kleines Beispiel. Das Programm
\ref{lst:Heron} zeigt die iterative Berechnung der Quadratwurzel $x=\sqrt{a}$
mit dem Heron"=Verfahren
\[x_0 := \frac{a + 1}{2},\quad x_{n+1} := \frac{1}{2}\Big(x_n + \frac{a}{x_n}\Big).\]
Die Folge $(x_n)$ konvergiert rasch gegen $x$.

\begin{figure}
\begin{lstlisting}[language=Python,%
label=lst:Heron,
caption={Programm zur iterativen Berechnung von Quadratwurzeln}]
def sqrt(a, epsilon = 1E-12):
    x = 0.5*(a + 1)
    while True:
        x = 0.5*(x + a/x)
        if abs((x*x - a)/a) < epsilon:
            return x
\end{lstlisting}
\begin{minipage}[t]{.48\textwidth}
\begin{lstlisting}[language=Python,%
label=lst:Potenz,%
caption={\raggedright Programm zur iterativen Berechnung der Potenz $x^n$}]
def power(x, n):
    y = 1
    while n != 0:
        y = y*x
        n = n - 1
    return y
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}[t]{.48\textwidth}
\begin{lstlisting}[language=Python,%
caption={\raggedright Programm zur rekursiven Berechnung der Potenz $x^n$}]
def power(x, n):
    if n == 0:
        return 1
    else:
        return x*power(x, n - 1)
\end{lstlisting}
\end{minipage}
\end{figure}

\section{Operationelle Semantik}

Hat man bereits viele Programme geschrieben, gelesen und sich die Vorgänge
auf der Assembler"=Ebene angeschaut, mag man ein intuitives Verständnis
dafür bekommen haben, wie Programme ablaufen. Es hat sich allerdings
als sehr förderlich herausgestellt, diese Abläufe formal zu präzisieren.
Das gelingt zum Beispiel, indem eine tatsächliche oder emulierte
Rechenmaschine festgelegt wird. Um aber nicht den Fokus auf das
Wesentliche zu verlieren, abstrahiert man von den Details der Maschine,
indem den syntaktischen Konstrukten der Programmiersprache direkt eine
Semantik zugeordnet wird. Ich will näher auf die \emph{operationelle
Semantik}\index{operationelle Semantik} eingehen.

Es sei $\mathrm T$ das Symbol für einen Term und $\mathrm{Zahl}$
das Symbol für eine Zahl. Wir legen mit der Produktionsregel
\[\mathrm T \to \mathrm{Zahl} \mid (\mathrm T + \mathrm T)\;\;\text{bzw.}\;\;
\mathrm T\to\mathrm{Zahl}\;\;\text{und}\;\;\mathrm T\to (\mathrm T+\mathrm T)\]
eine sehr einfache Grammatik fest. Damit ist gemeint, dass ein
Term entweder eine Zahl oder ein geklammerter Summenterm sein
soll, dessen Summanden wiederum Terme sind. Ein aus dem Startsymbol
$\mathrm T$ erzeugter Term sei grammatisch, sobald nur noch
Terminalsymbole, das sind in diesem Fall konkrete Zahlen, vorkommen.
In EBNF notiert, nimmt die Produktionsregel die Form
\[\verb/T ::= Zahl | '(' T '+' T ')';/\]
an. Bezeichnen wir mit $Z$ die Menge der Zahlen und
mit $T$ die Menge der Terme, können wir die Grammatik
alternativ auch durch die Inferenzregeln
\[\dfrac{t\in Z}{t\in T},\quad\;
\dfrac{t_1\in T\quad\; t_2\in T}{(t_1+t_2)\in T}\]
festlegen.

Die Auswertung von Termen findet statt gemäß einer Relation
$(\to)\subseteq T\times Z$, wobei wir $t\to v$ für $(t,v)\in(\to)$
schreiben. Damit ist gemeint, dass $t$ zu $v$ auswertet oder auswerten
kann. Die Relation wird festgelegt durch die Regeln
\[\dfrac{v\in Z}{v\to v},\quad\;\dfrac{t_1\to v_1\qquad t_2\to v_2}
{(t_1+t_2)\to v'},\]
wobei $v'$ der der Wert von $v_1+v_2$ sein soll. Zu bemerken ist, dass
durch diese Regeln keine Auswertungsreihenfolge vorgegeben wird.

Erweitern wir die Sprache nun dergestalt, dass in Ausdrücken auch
Variablen $x,y,z\in\mathrm{Var}$ enthalten sein dürfen, stellt sich
sogleich die Frage, welcher Wert einer Variablen bei ihrer Auswertung
zukommen soll. Dies hängt anscheinend davon ab, in welchem \emph{Zustand}
sich das Programm gerade befindet. Nennen wir $S$ die Menge der Zustände,
ist mit einem Zustand $s\in S$ eine Abbildung $s\colon\mathrm{Var}\to Z$
verbunden. Meist brauchen wir lediglich den Teilzustand betrachten, der
gerade von Bedeutung ist. Für den Zustand $s$ mit $s(x)=7$ und $s(y)=2$
schreibt man dann auch kurzum $s=\{x=7,y=2\}$. Die Auswertungsrelation stellt
nun eine Teilmenge von $T\times S\times Z$ dar, wobei $\langle t,s\rangle\to v$
bedeuten soll, dass der Term $t$ im Zustand $s$ den Wert $v$ annehmen
kann. Die Auswertung ist entsprechend festgelegt durch die Regeln
\[\dfrac{v\in Z}{\langle v,s\rangle\to v},\quad\;
\dfrac{x\in\mathrm{Var}}{\langle x,s\rangle\to s(x)},\quad\;
\dfrac{\langle t_1,s\rangle\to v_1\qquad \langle t_2,s\rangle\to v_2}
{\langle (t_1+t_2),s\rangle\to v_1+v_2}.\]
Die Sprache wird nun erweitert um Ausdrücke gemäß
\[\mathrm E \to\kw{false}\mid\kw{true}\mid\mathrm T = \mathrm T\mid
\mathrm T \le \mathrm T\mid \lnot\mathrm E\mid
(\mathrm E\land\mathrm E)\mid (\mathrm E\lor\mathrm E).\]
Es sei $E$ die Menge der auf diese Weise formierbaren Ausdrücke.
Zu diesen wird ebenfalls eine Auswertungsrelation definiert,
\[(\to)\subseteq E\times S\times\mathrm{Bool},\quad
\mathrm{Bool} := \{\kw{false},\kw{true}\}.\]
Wir legen die Auswertung intuitiv fest als
\[\dfrac{}{\langle\kw{false},s\rangle\to\kw{false}},\quad
\dfrac{}{\langle\kw{true},s\rangle\to\kw{true}},\quad
\dfrac{\langle t_1,s\rangle\to v_1\quad\langle t_2,s\rangle\to v_2}
{\langle t_1=t_2, s\rangle\to v'}\]
und so weiter, wobei $v'$ der Wert $\kw{true}$ sein soll, wenn $v_1,v_2$
übereinstimmen, sonst $\kw{false}$. Die Auswertung der Junktoren geschieht gemäß
\[\dfrac{\langle e,s\rangle\to v}{\langle\lnot e,s\rangle\to v'},\quad
\dfrac{\langle e_1,s\rangle\to v_1\quad\langle e_2,s\rangle\to v_2}
{\langle (e_1\land e_2),s\rangle\to v'}\]
und so weiter, wobei $v'$ jeweils der Wahrheitswert sein soll, der
sich aus der Wahrheitstafel des jeweiligen Junktors ergibt.

Die Produktionsregel
\[\mathrm P \to \kw{skip}\mid \mathrm{Var} := \mathrm T \mid\mathrm P;\mathrm P\mid
\kw{if}\;\mathrm E\;\kw{then}\;\mathrm P\;\kw{else}\;\mathrm P\;\kw{end}
\mid\kw{while}\;\mathrm E\;\kw{do}\;\mathrm P\;\kw{end}\]
beschreibt die Syntax einer kleinen imperativen Programmiersprache,
die im Weiteren als Studienobjekt dienen wird. Wir definieren zunächst
ein Semantik, die die intuitive Vorstellung vom Ablauf der Anweisungen
und Kontrollstrukturen widerspiegelt. Dies geschieht durch Festlegung
der Übergangsrelation $(\to)\subseteq P\times S\times S$. Mit
$\langle p,s\rangle\to s'$ soll gemeint sein, dass der Zustand $s$ mit dem
Durchlauf des Programms $p$ in den Zustand $s'$ übergeht oder zumindest
übergehen kann.

Zur Anweisung $\kw{skip}$ muss nicht viel gesagt werden, sie wird einfach
übersprungen, der Zustand bleibt unberührt. Ihre Regel lautet daher
\[\dfrac{}{\langle\kw{skip},s\rangle\to s}.\]
Der Übergang zur Zuweisung verläuft gemäß
\[\dfrac{\langle t,s\rangle\to v}{\langle x:=t, s\rangle \to s[x:=v]}.\]
Zu einer Sequenz von Programmen verläuft der Übergang gemäß
\[\dfrac{\langle p_1,s\rangle\to s'\quad\;\langle p_2,s'\rangle\to s''}
{\langle p_1; p_2, s\rangle\to s''}.\]
Bei if"=Anweisungen findet der Übergang nach den Regeln
\[\dfrac{\langle e,s\rangle\to\kw{true}\quad\;\langle p_1,s\rangle\to s'}
{\langle\kw{if}\;e\;\kw{then}\;p_1\;\kw{else}\;p_2\;\kw{end}\rangle\to s'},\quad\;
\dfrac{\langle e,s\rangle\to\kw{false}\quad\;\langle p_2,s\rangle\to s'}
{\langle\kw{if}\;e\;\kw{then}\;p_1\;\kw{else}\;p_2\;\kw{end}\rangle\to s'}\]
statt. Schließlich muss noch die Semantik für Schleifen\index{Schleife}
festgelegt werden. Wertet die Schleifenbedingung zu $\kw{false}$ aus,
wird der Schleifenkörper schlicht übersprungen, ohne eine Zustandsänderung
herbeizuführen,
\[\dfrac{\langle e,s\rangle\to\kw{false}}
{\langle\kw{while}\;e\;\kw{do}\;p\;\kw{end},s\rangle\to s}.\]
Wertet die Schleifenbedingung zu $\kw{true}$ aus, liegt die Regel
nicht mehr gänzlich auf der Hand. Die Überlegung ist folgende. Wurde
bereits geklärt, dass der erste Durchlauf den Zustand $s$ in $s'$
überführt und die restliche Schleife von $s'$ in $s''$ überführt,
dann muss die Schleife den Zustand $s$ in $s''$ überführen. Man gelangt
zur Regel
\[\dfrac{\langle e,s\rangle\to\kw{true}\quad\;\langle p,s\rangle\to s'\quad\;
\langle\kw{while}\;e\;\kw{do}\;p\;\kw{end},s'\rangle\to s''}
{\langle\kw{while}\;e\;\kw{do}\;p\;\kw{end},s\rangle\to s''}.\]

\section{Der Hoare-Kalkül}

Ein Hoare"=Tripel\index{Hoare-Tripel} $\{A\}\,p\,\{B\}$ soll die logische
Aussage ausdrücken, dass nach dem Durchlaufen des Programms $p$ die Aussage
$B$ gilt, sofern zuvor die Aussage $A$ galt. Hierbei bezeichnet man $A$ als
\emph{Vorbedingung}\index{Vorbedingung} und $B$ als
\emph{Nachbedingung}\index{Nachbedingung}. Das Tripel macht
allerdings keine Aussage darüber, ob das Programm $p$ tatsächlich
terminiert. Ist das Tripel erfüllt, nennt man das Programm in Bezug auf
die Vor- und Nachbedingung \emph{partiell korrekt}.\index{partiell korrekt}
Terminiert das Programm zudem, spricht man von \emph{totaler Korrektheit}.%
\index{total Korrekt}

Bei $A,B$ soll es sich um prädikatenlogische Formeln handeln, mit der
Besonderheit, dass in ihr die Terme und Ausdrücke der Programmiersprache
auftreten dürfen. Dazu kann formal die \emph{Zusicherungssprache}
festgelegt werden, aus der die Zusicherungen $A,B$ entstammen sollen.

Die Gültigkeit eines Hoare"=Tripels wird definiert gemäß
\[(\models\{A\}\,p\,\{B\})\,:\Leftrightarrow\,
\forall s\in S\colon (s\models A)\cond\forall s'\in S\colon
(\langle p,s\rangle\to s')\cond (s'\models B).\]
Interessanterweise kommt darin recht direkt die Kripke"=Semantik%
\index{Kripke-Semantik} einer Notwendigkeit zum Vorschein. Die Zustände
nehmen hierbei die Rolle der Welten ein, die Übergangsrelation je
Programm $p$ die Rolle der Zugänglichkeitsrelation. Demnach liegt eine
multimodale Logik vor, da je Programm $p$ ein Operator $\lnec_p$ existiert,
der üblicherweise $[p]$ geschrieben wird. Setzen wir diesbezüglich
\[(s\models [p]B)\;:\Leftrightarrow\;\forall s'\in S\colon
(\langle p,s\rangle\to s')\cond (s'\models B),\]
findet sich die Äquivalenz
\[(\models\{A\}\,p\,\{B\})\Leftrightarrow (\models A\cond [p]B),\]
die eine jähe Beziehung des Hoare"=Kalküls zur ihr, der
\emph{dynamischen Logik}\index{dynamische Logik}
herstellt, auf die ich an dieser Stelle aber nicht näher eingehen will.
Trotzdem mag der Leser sie später in Erinnerung rufen, um ein
übersichtlicheres Bild von den Sachverhalten zu bekommen.

\strong{Die Regeln zur Herleitung der Tripel.} Für Zuweisungen gilt
\[\dfrac{}{\vdash\{A[x:=t]\}\; x := t\;\{A\}}.\]
Zur Gültigkeit dieser Regel. Es gelte $s\models A[x:=t]$. Weiterhin
wird $\langle x:=t,s\rangle\to s'$ angenommen. Zu zeigen ist $s'\models A$.
Insofern die Auswertung von Termen deterministisch stattfindet, existiert
nun genau ein $v$ mit $\langle t,s\rangle\to v$. Mithin liegt der
Sachverhalt $s'=s[x:=v]$ vor. Letztlich gilt die Äquivalenz
\[(s\models A[x:=t])\Leftrightarrow (s[x:=v]\models A),\]
da man zum gleichen Resultat gelangt, wenn $x$ in $A$ direkt mit $v$
belegt wird, oder zunächst gegen $t$ ersetzt und daraufhin $t$ den
Wert $v$ erhält. Formal bestätigt man dies per struktureller
Induktion über den Aufbau von $A$.

Für eine Sequenz von Programmen gilt
\[\dfrac{\vdash\{A\}p_1\{B\}\quad\;\;\vdash\{B\}p_2\{C\}}{\vdash\{A\}p_1; p_2\{C\}}.\]
Zur Gültigkeit dieser Regel. Es gelte $s\models A$. Angenommen,
es kann $s''$ erreicht werden, also $\langle s,p_1; p_2\rangle\to s''$.
Dann muss ein Zustand $s'$ mit $\langle p_1,s\rangle\to s'$ und
$\langle p_2,s'\rangle\to s''$ existieren. Sollte $s'$ nicht eindeutig
bestimmt sein, betrachten wir $s'$ einfach fest, aber beliebig. Zu zeigen
ist $s''\models C$. Vermittels der ersten Prämisse erhält man
$s'\models B$, vermittels der zweiten daraufhin $s''\models C$.

Die Regel für Verzweigungen lautet
\[\dfrac{\vdash\{A\land B\}\,p_1\,\{C\}\quad\;\;\vdash\{A\land\lnot B\}\,p_2\,\{C\}}
{\vdash\{A\}\;\kw{if}\;B\;\kw{then}\;p_1\;\kw{else}\;p_2\;\kw{end}\;\{C\}}.\]
Zur Gültigkeit dieser Regel. Sei $s$ der Zustand vor, und $s'$ der
Zustand nach Durchlaufen der Verzweigung. Es gelte $s\models A$, zu
zeigen ist $s'\models C$. Fallunterscheidung. Im Fall
$\langle B,s\rangle\to\kw{true}$
gilt $s\models B$, also $s\models A\land B$. Es wird $p_1$ ausgeführt,
also muss $\langle p_1,s\rangle\to s'$ gelten. Laut der ersten Prämisse
gilt dann $s'\models C$. Im Fall $\langle B,s\rangle\to\kw{false}$
gilt $s\models\lnot B$, also $s\models A\land\lnot B$. Es wird $p_2$
ausgeführt, also muss $\langle p_2,s\rangle\to s'$ gelten. Laut der
zweiten Prämisse gilt dann $s'\models C$.

Die Regel für Schleifen lautet
\[\dfrac{\vdash\{I\land B\}\,p\,\{A\}}
{\vdash\{I\}\;\kw{while}\;B\;\kw{do}\;p\;\kw{end}\;\{I\land\lnot B\}}.\]
Zur Gültigkeit dieser Regel. Sei $s$ der Zustand vor, und $s''$
der Zustand nach dem Durchlaufen der Schleife. Es gelte $s\models I$.
Induktion über die Anzahl der Durchläufe. Im Induktionsanfang wird findet
kein Durchlauf von  $p$ statt, womit $\langle B,s\rangle\to\kw{false}$
gilt, also $s\models\lnot B$. Da dem Zustand keine Änderung widerfährt,
gilt $s''=s$, also $s''\models I\land\lnot B$.
Nun zum Induktionsschritt. Da $p$ durchlaufen wird, gilt
$\langle B,s\rangle\to\kw{true}$, also $s\models B$,
ergo $s\models I\land B$. Mit dem Durchlauf von $p$ existiert
$s'$ mit $\langle p,s\rangle\to s'$. Laut der Prämisse gilt
$s'\models I$. Mit dem Durchlaufen der restlichen Schleife geht $s'$
in $s''$ über. Vermittels der Induktionsvoraussetzung erhält man
$s''\models I\land\lnot B$.

Eine Aussage $I$, welche sowohl vor als auch nach dem Durchlauf des
Schleifenrumpfs und somit der Schleife als Ganzes gültig ist, bezeichnet
man als \emph{Schleifeninvariante}. Die Auffindung einer zielführenden
Schleifeninvariante ist nicht immer einfach.

Gilt ein Tripel als bestätigt, kann man intuitiv die Vorbedingung beliebig
verstärken und die Nachbedingung beliebig abschwächen. Man erhält ein
Tripel mit weniger Aussagekraft, das aber für die Beschreibung des
Sachverhaltes genügen mag. Diese Überlegung wird formalisiert durch
die Regel
\[\dfrac{\vdash A'\cond A\quad\;\;\vdash\{A\}\,p\,\{B\}\quad\;\;\vdash B\cond B'}
{\vdash\{A'\}\,p\,\{B'\}}.\]
Zur Gültigkeit dieser Regel. Aus $s\models A'$ und
$\langle p,s\rangle\to s'$ ist $s'\models B$ abzuleiten.
Vermittels $\models A'\cond A$ erhält man zunächst $s\models A$,
vermittels $\models\{A\}\,p\,\{B\}$ daraufhin $s'\models B$,
und vermittels $\models B\cond B'$ daraufhin schließlich $s'\models B'$.

Die Regel setzt sich zusammen aus den beiden Spezialfällen
\[\dfrac{\vdash A'\cond A\quad\;\;\vdash\{A\}\,p\,\{B\}}
{\vdash\{A'\}\,p\,\{B\}},\quad\;\;
\dfrac{\vdash\{A\}\,p\,\{B\}\quad\;\;\vdash B\cond B'}
{\vdash\{A\}\,p\,\{B'\}}.\]

\begin{figure}
\begin{center}
\begin{minipage}[t]{.48\textwidth}
\begin{lstlisting}[language=Python,caption={Schnelles Potenzieren},%
label=lst:Potenz-schnell]
def power(x, n):
    y = 1
    while n > 1:
        if n%2 == 1: y = y*x
        n //= 2; x = x*x
    if n == 1: y = y*x
    return y
\end{lstlisting}
\end{minipage}
\begin{minipage}[t]{.48\textwidth}
\begin{lstlisting}[language=Python,caption={Mit Hilfsvariablen},%
label=lst:Potenz-schnell-Hilfsvariablen]
def power(x, n):
    y = 1; a = x; i = n
    while i > 1:
        if i%2 == 1: y = y*a
        i //= 2; a = a*a
    if i == 1: y = y*a
    return y
\end{lstlisting}
\end{minipage}
\end{center}
\end{figure}

\noindent\strong{Beispiel.}
Das Listing \ref{lst:Potenz-schnell} zeigt den Algorithmus zum schnellen
Potenzieren. Wie \ref{lst:Potenz} berechnet dieser die Potenz $x^n$,
benötigt dafür aber weniger Multiplikationen. Soll beispielsweise $x^{100}$
berechnet werden, kann dies auf $(x^{50})^2$ zurückgeführt werden. Dafür
dass am Ende einmal quadriert wird, wurden $50$ Multiplikationen
eingespart. Der Algorithmus stellt ein allgemeines Verfahren hierfür
dar.

Allerdings ist nicht mehr auf den ersten Blick ersichtlich, ob der
Algorithmus korrekt arbeitet. Zur Analyse werden zunächst die
Hilfsvariablen $a,i$ eingeführt, damit $x,n$ konstant bleiben, siehe
Listing \ref{lst:Potenz-schnell-Hilfsvariablen}. Nach dem Durchlaufen
des Programms soll die Nachbedingung $y=x^n$ bestehen. Zunächst findet
sich das Tripel
\[\begin{array}{l}
\{y = x^n\land i = 0 \lor y\cdot a = x^n\land i = 1\}\\
\text{\texttt{\textbf{if} i == 1: y = y*a}}\\
\{y = x^n\}.
\end{array}\]
Im Fall $i=1$ reduziert sich die Vorbedingung nämlich zu $y\cdot a=x^n$,
die bedingte Anweisung wird ausgeführt und das Resultat ist $y=x^n$ laut
Zuweisungsregel. Im Fall $i\ne 1$ muss $i=0$ sein, denn nach der
Schleife wird $0\le i\le 1$ gelten. In diesem Fall reduziert sich die
Vorbedingung zu $y=x^n$, die bedingte Anweisung wird nicht ausgeführt,
das Resultat ist somit ebenfalls $y=x^n$.

Nun kommt ein kritischer Schritt, dessen Auffindung ohne Übung schwierig
sein mag. Vor der Schleife gilt $a^i=x^n$, was etwas mit der
Schleifeninvariante zu tun haben könnte. Betrachten wir nun die Vorbedingung
\[y = x^n\land i = 0\lor y\cdot a = x^n\land i = 1.\]
Ist $a^i=x^n$ so modifizierbar, dass diese gilt? Ja, nämlich zu
$y\cdot a^i=x^n$, denn
\begin{gather*}
\text{im Fall $i = 0$ gilt $y\cdot a^i = y\cdot a^0 = y$},\\
\text{im Fall $i = 1$ gilt $y\cdot a^i = y\cdot a^1 = y\cdot a$}.
\end{gather*}
Die Gleichung $y\cdot a^i=x^n$ gilt ebenfalls vor der Schleife, denn
dort ist $y=1$. Zudem kommen in der Gleichung alle drei Variablen vor,
die sich in der Schleife verändern. Tatsächlich stellt diese Gleichung eine
zielführende Schleifeninvariante dar.

Nun zum Schleifenrumpf. Das Tripel zur ersten Anweisung ist
\[\begin{array}{l}
\{y\cdot a^i = x^n\}\\
\text{\texttt{\textbf{if} i\%2 == 1: y = y*a}}\\
\{y\cdot a^{i-1} = x^n\land i\operatorname{mod} 2 = 1
\lor y\cdot a^i = x^n\land i\operatorname{mod}2 = 0\}.
\end{array}\]
Die lange Nachbedingung kann man kompakter schreiben als
\[y\cdot a^{i-i\operatorname{mod}2} = x^n.\]
Außerdem gilt $i-i\operatorname{mod}2 = 2\lfloor\frac{i}{2}\rfloor$,
wobei $\lfloor\frac{i}{2}\rfloor$ mit dem Programmterm \texttt{i//2}
gleichbedeutend ist. Mit dieser Vorbereitung stellt die restliche
Argumentation nur mehr eine Leichtigkeit dar. Gemäß der Zuweisungsregel
sind die Tripel
\[\begin{array}{l}
\{y\cdot a^{2\lfloor i/2\rfloor} = x^n\}\\
\text{\texttt{i //= 2;}}\\
\{y\cdot a^{2i} = x^n\}
\end{array}
\quad\;\;\text{und}\quad\;\;
\begin{array}{l}
\{y\cdot a^{2i} = x^n\}\\
\text{\texttt{a = a*a;}}\\
\{y\cdot a^i = x^n\}
\end{array}\]
erfüllt. Damit gilt die Invariante als bewiesen. Weil nichts weiter zu
beweisen verbleibt, gilt auch der gesamte Algorithmus als bewiesen.

\section{Zum Kalkül der schwächsten Vorbedingung}

Zu einem Programm $p$ mag man zu einer Nachbedingung $B$ mehr als eine
Vorbedingung $A$ finden, so dass $\{A\}\,p\,\{B\}$ gilt. Dahingehend
gelangt man zur Überlegung, ob man $A$ nicht zu einer Vorbedingung $A'$
mit $A\cond A'$ abschwächen kann, so dass $\{A'\}\,p\,\{B\}$ erfüllt
bleibt. Wir nennen $A_0$ \emph{schwächste liberale Vorbedingung},
wenn für jede andere Vorbedingung $A$ die Äquivalenz
\[\{A\}\,p\,\{B\}\bicond (A\cond A_0)\]
besteht. Man schreibt $\mathrm{wlp}(p,B):=A_0$, wobei $\mathrm{wlp}$ für
\emph{weakest liberal precondition} steht. Wie bei Erörterung der
Semantik des Hoare"=Tripels bereits festgestellt wurde, handelt es sich
bei $\mathrm{wlp}(p,B)$ um nichts anderes als die modale Aussage $[p]B$.

\begin{Satz}
Es gilt $[p_1;p_2]B\equiv [p_1][p_2]B$.
\end{Satz}
\begin{Beweis}
Zur Abkürzung sei $\varphi_1:\equiv(\langle p_1,s\rangle\to s')$ sowie
$\varphi_2:\equiv(\langle p_2,s'\rangle\to s'')$ und $\psi:\equiv (s''\models B)$.
Es findet sich die äquivalente Umformung
\begin{align*}
&(s\models [p_1][p_2]B) \iff (\forall s'\colon\varphi_1\cond \forall s''\colon\varphi_2\cond\psi)\\
\iff &(\forall s'\colon\forall s''\colon\varphi_1\land\varphi_2\cond\psi)
\iff (\forall s''\colon\forall s'\colon\varphi_1\land\varphi_2\cond\psi)\\
\iff &(\forall s''\colon (\exists s'\colon\varphi_1\land\varphi_2)\cond\psi)
\iff (s\models [p_1;p_2]B).
\end{align*}
Der letzte Schritt gelingt hierbei aufgrund der Äquivalenz
\[(\exists s'\colon\varphi_1\land\varphi_2)\Leftrightarrow
(\langle p_1;p_2,s\rangle\to s'').\,\qedsymbol\]
\end{Beweis}

\begin{Satz}
Es gilt $[x:=t]B\equiv B[x:=t]$.
\end{Satz}
\begin{Beweis}
Es gelte $s\models [x:=t]B$, womit $s'\models B$ für jedes $s'$ mit
$\langle x:=t,s\rangle\to s'$ gelten muss. Es existiert aber genau ein
solches $s'$, und für dieses gilt $s'=s[x:=v]$ mit
$\langle t,s\rangle\to v$. Diesbezüglich ist $s'\models B$ gleichbedeutend
mit $s\models B[x:=t]$. Diese Argumentation kann auch
unschwer umgekehrt werden.\,\qedsymbol
\end{Beweis}

Der Umstand, dass man einfach die Symbolik verdrehen kann, ist natürlich
eine glückliche Fügung in Form einer syntaktischen Koinzidenz. Wählt man
eine andere Notation, etwa $\lnec_{x:=t} B\equiv B[t/x]$, geht sie verloren.

\begin{Satz}\label{dl-nec}
Aus $\models A$ folgt $\models [p]A$.
\end{Satz}
\begin{Beweis}
Unter der Annahme $\langle p,s\rangle\to s'$ muss $s'\models A$ bestätigt
werden. Laut der Prämisse ist $s'\models A$ aber per se erfüllt.\,\qedsymbol
\end{Beweis}

\begin{Satz}\label{dl-K}
Es gilt $[p](A\cond B)\cond ([p]A\cond [p]B)$.
\end{Satz}
\begin{Beweis}
Unter der Annahme $\langle p,s\rangle\to s'$ muss $s'\models B$ bestätigt
werden. Vermittels der ersten Prämisse erhält man $s'\models A$ aus
der Annahme. Vermittels der zweiten Prämisse erhält man
$s'\models A\cond B$ aus der Annahme, was definitionsgemäß zu
$(s'\models A)\cond (s'\models B)$ äquivalent ist. Per Modus ponens
folgt somit $s'\models B$.\,\qedsymbol
\end{Beweis}

\begin{Satz}
Es gilt $[p](A\land B)\equiv [p]A\land [p]B$.
\end{Satz}
\begin{Beweis}
Dies ist ein allgemeiner Umstand der Modallogik K, die
laut Satz \ref{dl-nec} und Satz \ref{dl-K} je fixem Programm $p$
vorliegt.\,\qedsymbol
\end{Beweis}

\begin{Satz}
Es gilt $[\kw{if}\,B\,\kw{then}\,p_1\,\kw{else}\,p_2\,\kw{end}]C\equiv
(B\cond [p_1]C)\land (\lnot B\cond [p_2]C)$.
\end{Satz}
\begin{Beweis}
Es gelte die linke Seite der Äquivalenz. Zum ersten Teil der Konjunktion.
Aus der Annahme von $s\models B$ und $\langle p_1,s\rangle\to s'$ ist
$s'\models C$ abzuleiten. Wegen $\langle B,s\rangle\to\kw{true}$ erhält man
\[\langle\kw{if}\,B\,\kw{then}\,p_1\,\kw{else}\,p_2\,\kw{end},s\rangle\to s',\]
woraus $s'\models C$ vermittels der linken Seite folgt. Die Argumentation
zum zweiten Teil der Konjunktion verläuft analog.

Es gelte die rechte Seite der Äquivalenz. Es ist $s'\models C$
abzuleiten. Fallunterscheidung. Im Fall $\langle B,s\rangle\to\kw{true}$
gilt $s\models B$ und außerdem $\langle p_1,s\rangle\to s'$. Mit
diesen erhält man $s'\models C$ vermittels des ersten Teils der
Konjunktion. Im Fall $\langle B,s\rangle\to\kw{false}$ verläuft
die Argumentation analog.\,\qedsymbol
\end{Beweis}

\noindent
Ich möchte bemerken, dass in klassischer Logik die allgemeine Äquivalenz
\[(B\cond A_1)\land (\lnot B\cond A_2)\Leftrightarrow
(B\land A_1)\lor (\lnot B\land A_2)\]
besteht.
