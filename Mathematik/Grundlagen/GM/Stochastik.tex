
\chapter{Elemente der Stochastik}

\section{Grundbegriffe}

\subsection{Ereignisse}

Die Wahrscheinlichkeitstheorie beschäftigt sich mit
\emph{Zufallsexperimenten}. Darunter versteht man ein Experiment
mit zufälligem Ausgang, das, um der Wissenschaftlichkeit genüge zu tun,
unter genau definierten Versuchsbedingungen durchgeführt wird.
Der Ausgang führt immer zu einem \emph{Ergebnis}. Alle erreichbaren
Ergebnisse fasst man zur \emph{Ergebnismenge} zusammen. Allgemeiner
genügt es, wenn jedes Ergebnis in der Ergebnismenge liegt, wobei diese
aber auch Elemente enthalten darf, die das Experiment niemals abwirft.
Jede Teilmenge der Ergebnismenge nennt man ein \emph{Ereignis}\index{Ereignis}.
Die Potenzmenge der Ergebnismenge heißt \emph{Ereignisraum}, sie
besteht aus allen denkbaren Ereignissen. Man sagt, ein Ereignis sei
eingetreten, wenn das Ergebnis des Versuchs im diesem Ereignis liegt.

Zu beachten ist, dass wir dabei eine endliche oder höchstens
abzählbar unendliche Ergebnismenge voraussetzen. Bei überabzählbaren
Ergebnismengen kommt es zu Unwägbarkeiten, deren Klärung Gegenstand der
Maßtheorie ist.

Ein schlichtes Experiment bietet der Wurf des Spielwüfels, ein mit
Augenzahlen beschriftetes regelmäßiges Hexaeder. Die Ergebnismenge
wird als
\[\Omega := \{1, 2, 3, 4, 5, 6\}\]
festgelegt. Betrachten wir die drei Ereignisse
\[A := \{2\},\quad B := \{1,2\},\quad C:=\{1,3\}.\]
Ist $\omega=2$ das Ergebnis des Versuchs, sind die Ereignisse $A,B$
eingetreten. Zwei Ereignisse, die niemals gleichzeitig eintreten,
heißen \emph{disjunkt}. So sind die $A,C$ disjunkt, weil ihre
Schnittmenge leer ist.

\subsection{Wahrscheinlichkeiten}

Man kann nicht voraussagen, wie ein Experiment ausgehen wird. Das
Wahrscheinlichkeitsmaß liefert dennoch ein Maß dafür, wie sicher der
Eintritt eines Ereignisses ist. Wahrscheinlichkeit wird tiefergründig
verständlich, wenn dasselbe Zufallsexperiment abermals wiederholt wird.
Wir zählen, wie häufig ein Elementarereignis eingetreten ist.

Es sei ein Versuch $n$ mal durchgeführt worden, was zu den Ergebnissen
$a_i$ für $i=1$ bis $i=n$ geführt hat. Wir definieren die \emph{relative
Häufigkeit} des Ereignisses $A$ als die Zahl
\[r_{n,a}(A) := \frac{1}{n}|\{i\in\{1,\ldots,n\}\mid a_i\in A\}|.\]
Relative Häufigkeiten bieten bei hinreichend großem $n$ eine Näherung
für die Wahrscheinlichkeit. Zur Vermessung eines Würfels wird man
diesen also möglichst oft werfen wollen. Man erhält so die relativen
Häufigkeiten der Elementarereignisse, und damit näherungsweise auch
ihre Wahrscheinlichkeiten. So lässt sich feststellen, ob ein
Würfel gezinkt wurde.

Fassen wir $a$ als Funktion $i\mapsto a_i$ auf, können wir schreiben
\[\{i\mid a_i\in A\} = \{i\mid i\in a^{-1}(A)\} = a^{-1}(A).\]
Für disjunkte Ereignisse $A,B$ erhält man nun
\begin{align*}
r_{n,a}(A\cup B) &= \tfrac{1}{n}|a^{-1}(A\cup B)|
= \tfrac{1}{n}|a^{-1}(A)\cup a^{-1}(B)|\\
&= \tfrac{1}{n}|a^{-1}(A)| + \tfrac{1}{n}|a^{-1}(B)|
= r_{n,a}(A) + r_{n,a}(B).
\end{align*}

\subsection{Zufallsgrößen}

Eine \emph{Zufallsgröße}\index{Zufallsgroesse@Zufallsgröße} darf man
sich als eine Funktion $X\colon\Omega\to\Omega'$
vorstellen, die eine kausale Verbindung zwischen den Ergebnismengen
$\Omega,\Omega'$ schafft. Ein Ergebnis $\omega\in\Omega$ führt
zu $X(\omega)$. Ursächlich für ein $x\in\Omega'$ sind daher all die
$\omega$ mit $x=X(\omega)$. Das heißt, ursächlich für das
Elementarereignis $\{x\}$ ist dessen Urbild $X^{-1}(\{x\})$.
Infolge muss die Wahrscheinlichkeit von $\{x\}$ die es Urbildes sein.
Insofern definiert man auf $\Omega'$ das Wahrscheinlichkeitsmaß%
\[P_X\colon\mathcal P(\Omega')\to [0,1],\quad P_X(A) := P(X^{-1}(A)).\]
Man nennt $P_X$ die \emph{Verteilung}\index{Verteilung} von $X$. Mit der
identischen Zufallsgröße
\[\id\colon\Omega\to\Omega,\quad \id(\omega) := \omega\]
versteht sich auch das ursprüngliche Maß $P$ als die Verteilung $P=P_{\id}$.

Geläufig sind die Schreibweisen
\begin{align*}
P(X=x) &:= P(X^{-1}(\{x\})), & \{X=x\} &:= X^{-1}(\{x\}),\\
P(X\in A) &:= P(X^{-1}(A)), & \{X\in A\} &:= X^{-1}(A).
\end{align*}
Es ist $\{X=x\}$ dasselbe wie $\{X\in\{x\}\}$. 
Ist $P$ die Gleichverteilung auf $\Omega$, ergibt sich
\[P(X\in A) = \frac{|\{X\in A\}|}{|\Omega|}.\]
Standardbeispiel. Wir werfen zwei Spielwürfel.
Die Ergebnismenge sei%
\[\Omega := \{1,\ldots,6\}\times\{1,\ldots,6\},\]
und jedes der 36 elementaren Ereignisse sei gleich wahrscheinlich, habe
also die Wahrscheinlichkeit $\tfrac{1}{36}$.
Es bezeichne $\omega_1$ das Ergebnis des ersten, und $\omega_2$
das des zweiten Wurfs. Wir betrachten die Zufallsgröße%
\[X\colon\Omega\to\{2,\ldots,12\},\quad
X(\omega_1,\omega_2) := \omega_1+\omega_2.\]
Gesucht sei $P(X=4)$. Man ermittelt
\[\{X=4\} = \{(1,3), (2,2), (3,1)\},\quad\text{ergo}\;P(X=4) = \tfrac{3}{36}.\]
Allgemein zerfällt ein Ereignis $A$ ja in seine disjunkten Elementarereignisse
$\{x\}$, so dass $A = \bigcup_{x\in A} \{x\}$ gilt. Weil nun die
Fasern $X^{-1}(\{x\})$ ebenfalls disjunkt sind, muss $P(X\in A)$ die
Summe der $P(X=x)$ mit $x\in A$ sein. Das heißt, man rechnet%
\[P(X\in A) = P(X^{-1}(\bigcup_{x\in A} \{x\})) = P(\bigcup_{x\in A} X^{-1}(\{x\}))
= \sum_{x\in A} P(X=x).\]
Die Verteilung $P_X$ ist demzufolge bereits eindeutig bestimmt,
sobald $P(X=x)$ für jedes $x\in\Omega'$ vorliegt. Dies motiviert
uns, die Funktion
\[p_X\colon\Omega\to[0,1],\quad p_X(x) := P(X=x)\]
zu definieren, genannt die \emph{Wahrscheinlichkeitsfunktion} der
Zufallsgröße $X$.

\newpage
\section{Mehrstufige Experimente}

\subsection{Bedingte Wahrscheinlichkeiten}

Es findet ein zweistufiges Experiment statt, welches sich sich
aus einem ersten und einem zweiten Wurf eines Spielwürfels
zusammensetzt. Bei jedem der Würfe bestehe eine Gleichverteilung.
Zur Frage steht, wie wahrscheinlich das Ereignis $\{(6,6)\}$ ist.
Ein Paar $(\omega_1,\omega_2)$ fasse hierbei das Ergebnis $\omega_1$
des ersten und $\omega_2$ des zweiten Wurfs zusammen.

Die Wahrscheinlichkeit der ersten Sechs beträgt $\tfrac{1}{6}$,
die der zweiten ebenfalls $\tfrac{1}{6}$. Sie multiplizieren sich
zu zu $\tfrac{1}{36}$, richtig?

Es wäre doch möglich, dass zwischen den beiden Würfen eine,
sagen wir, geisterhafte Beziehung besteht, dergestalt dass
der zweite Wurf niemals in einer Sechs resultiert, sofern das
Ergebnis des ersten eine war. Trotzdem sind die Wahrscheinlichkeiten bei
jedem der Würfe für sich allein gesehen gleichverteilt. Dafür muss man
nicht unbedingt die Wirklichkeit manipulieren. Das Phänomen ist bereits
bei der Erzeugung von Zufallszahlen im Computer beobachtbar. War die
erste Zufallszahl eine Sechs, braucht der Generator die zweite lediglich
solange zu verwerfen, wie sie eine Sechs sein sollte. Umstände dieser
Art stellen nicht nur ein Gedankenspiel dar, so dass wir uns notgedrungen
mit ihnen auseinandersetzen müssen. Sie führen zum Begriff der
\emph{bedingten Wahrscheinlichkeit}.

Bisher wurde immer nur die Verteilung der Wahrscheinlichkeiten eines
Würfels für sich allein betrachtet. Das war modelliert durch die Größe
\[X_0\colon\Omega\to\Omega,\quad X_0(\omega):=\omega,\quad \Omega := \{1,\ldots,6\},\]
mit der Gleichverteilung $P_0$, so dass $P_0(X=6)=\tfrac{1}{6}$.

Wir modellieren das zweistufige Experiment durch die Zufallsgröße
\[X\colon\Omega^2\to\Omega^2,\quad X(\omega) := (X_1,X_2)(\omega)
= (X_1(\omega), X_2(\omega)),\]
die sich mit $\omega = (\omega_1,\omega_2)$ aus den zwei Zufallsgrößen
\begin{gather*}
X_1\colon\Omega^2\to\Omega,\quad X_1(\omega_1,\omega_2) := \omega_1,\\
X_2\colon\Omega^2\to\Omega,\quad X_2(\omega_1,\omega_2) := \omega_2
\end{gather*}
zusammensetzt. Es stellt $X_1(\omega)$ das Ergebnis des ersten und
$X_2(\omega)$ das des zweiten Wurfs dar. Wie gewünscht gilt
\[(X_1(\omega),X_2(\omega)) = (X_0(\omega_1),X_0(\omega_2)) = (\omega_1,\omega_2).\]
Es bezeichne $P$ die Verteilung auf $\Omega^2$. Wir wissen hier allerdings
lediglich
\begin{gather*}
P(X_1=\omega_1) = P_0(X_0=\omega_1) = \tfrac{1}{6},\\
P(X_2=\omega_2) = P_0(X_0=\omega_2) = \tfrac{1}{6}.
\end{gather*}
Die Fehlannahme besteht nun darin, dass per se
\[P(\{X_1=\omega_1\}\cap\{X_2=\omega_2\}) = P(X_1=\omega_1)P(X_2=\omega_2)\]
gelten müsse. Ist diese Gleichung erfüllt, nennt man die
Zufallsgrößen $X_1,X_2$ \emph{unabhängig}. In der bisherigen Sichtweise,
wo wir nur $X_0$ mit $P_0$ gesehen haben, war es uns nicht möglich,
stochastische Abhängigkeit zu beschreiben. Man notiert allgemein%
\[P(X=x,Y=y) := P(\{X=x\}\cap\{X=y\}) = P(X=x)P(Y=y\mid X=x).\]
Der letzte Faktor bezeichne hierbei die bedingte Wahrscheinlichkeit
für das Ereignis $\{Y=y\}$, unter der Bedingung, dass $\{X=x\}$
bereits eingetreten ist.
\begin{Definition}[Bedingte Wahrscheinlichkeit]\newlinefirst
Die bedingte Wahrscheinlichkeit für den Eintritt von $A$ unter der
Bedingung $B$ ist für $P(B)\ne 0$ definiert gemäß
\[P(A\mid B) := \frac{P(A\cap B)}{P(B)}.\]
\end{Definition}
Wir setzen speziell $B:=\{X=x\}$ und $A:=\{Y=y\}$ ein,
das macht
\[P(Y=y\mid X=x) = \frac{P(X=x,Y=y)}{P(X=x)}.\]
Sind $X,Y$ unabhängig, gilt also
\[P(Y=y\mid X=x) = P(Y=y).\]
Mit der geisterhaften Beziehung zwischen den Würfeln wäre allerdings
\[0 = P(X_2=6\mid X_1=6) \ne P(X_1=6) = \tfrac{1}{6}.\]
