
\chapter{Logisches Schließen}

\section{Grundbegriffe}

\subsection{Schlussregeln}

% Alles Schlussregeln und Axiome müssen unmittelbar einsichtig sein,
% müssen unbedenklich sein. Normalerweise will man in der Wissenschaft
% nicht so informell sein, aber hier sind wir am basalen Grund.
% Selbst in der Semantik wird der hier aufgestellte Formalismus
% benötigt. Der Korrektheitsbeweis würde somit zirkulär.

% Am Anfang ist noch nicht allzu viel zu rechnen. Wir müssen uns
% erst um die Schlussregeln und Axiome bemühen, bevor das erste Theorem
% bewiesen werden kann.

% [todo]
% Subjunktion oder Konditional, Bijunktion oder Bikonditional?

Logisches Schließen findet in einzelnen Schritten statt. Ein Schritt
stellt hierbei immer die Ableitung einer Schlussfolgerung aus einer
oder mehreren Voraussetzungen dar. Die Voraussetzungen heißen
\emph{Prämissen}\index{Praemisse@Prämisse}, die Schlussfolgerung
\emph{Konklusion}\index{Konklusion}. Darstellen wollen wir den Schritt
durch eine waagerechte Linie, wobei die Prämissen oberhalb befindlich
sein sollen, und die Konklusion unterhalb. Der Schritt
\[\dfrac{\text{Wenn es regnet, wird die Straße nass}\qquad\text{Es regnet}}
{\text{Die Straße wird nass}}\]
beschreibt beispielsweise, dass aus den Prämissen »Wenn es regnet, wird
die Straße nass« und »Es regnet« die Konklusion »Die Straße wird nass«
gefolgert wird.

Schlüsse wie der Obige treten in der Mathematik ständig auf. Ihnen allen
liegt ein bestimmtes Muster zugrunde, welches sich durch eine als
\emph{Modus ponens}\index{Modus ponens} oder
\emph{Abtrennungsregel}\index{Abtrennungsregel}
bezeichnete schematische \emph{Schlussregel}\index{Schlussregel}
beschreiben lässt. Es bezeichne hierzu $A\cond B$ die Implikation
»wenn $A$, dann $B$«. Es dürfen nun in
\[\dfrac{A\cond B\qquad A}{B}\]
für $A,B$ beliebige Aussagen eingesetzt werden. So darf »Es regnet«
für $A$ und »Die Straße wird nass« für $B$ eingesetzt werden.

\subsection{Sequenzen}

Das Schließen von Aussagen allein genügt nicht. Um freier argumentieren
zu können, würden wir gerne den Umstand beschreiben können, dass eine
Aussage unter bestimmten Annahmen abgeleitet werden konnte. Diese
Annahmen $A_k$ sind selbst Aussagen. Wir fassen sie zu einer endlichen
Ansammlung
\[\Gamma := [A_1,A_2,\ldots,A_n]\]
zusammen, worunter wir eine endliche Liste, oder auch eine endliche
Menge verstehen wollen, denn man soll mit dieser Liste umgehen können
wie mit einer Menge. Das heißt, es ist nicht von Bedeutung, wie
oft eine Aussage vorkommt oder in welcher Reihenfolge die Aussagen
stehen. Man bezeichnet $\Gamma$ als die \emph{Antezedenz}%
\index{Antezedenz} oder die Liste der \emph{Antezedenzen}. Es wird
$\Gamma$ auch der \emph{Kontext}\index{Kontext}
oder die \emph{Umgebung}\index{Umgebung} genannt, das sind auf die
Typentheorie zurückzuführende Sprechweisen, die einen ganz ähnlichen
Formalismus besitzt. Wir bezeichnen die Symbolik
\[\Gamma\vdash A\]
als \emph{Sequenz}\index{Sequenz}. Sie drückt das \emph{Urteil}%
\index{Urteil} aus, dass die Aussage $A$ aus den Annahmen
vermittels Schlussregeln ableitbar ist. Darin nennt man $A$ die
\emph{Hinterformel} oder \emph{Sukzedenz}.
Der Modus ponens\index{Modus ponens} wird nun in der allgemeinen Form
\[\dfrac{\Gamma\vdash A\cond B\qquad\Gamma\vdash A}{\Gamma\vdash B}\]
beschrieben. Wir argumentieren beim Schließen also ab jetzt nicht mehr
mit den Aussagen selbst, sondern mit den Sequenzen. Dies hat einen wichtigen
Grund, nämlich dass die Berücksichtigung der Abhängigkeit von Annahmen
expliziter Teil des Schließens wird.

Ein Kontext kann auch eine leere Liste sein. Besitzt eine vermittels
Schlussregeln ableitbare Sequenz einen leeren Kontext, so bezeichnet
man die Sukzedenz als ein \emph{Theorem}\index{Theorem} im engeren
Sinne. Ein Theorem ist also eine Aussage, die für sich allein gilt,
ohne dass dafür irgendwelche Annahmen getroffen werden müssen.

Für Sequenzen gilt die \emph{Abschwächungsregel}%
\index{Abschwaechungsregel@Abschwächungsregel}. Sie besagt, dass
falls die Aussage $A$ bereits aus $\Gamma$ ableitbar ist, diese
Aussage erst recht ableitbar ist, wenn zu $\Gamma$ weitere Annahmen
$\Gamma'$ hinzugefügt werden. Kurzum gilt die Regel
\[\dfrac{\Gamma\vdash A}{\Gamma,\Gamma'\vdash A}.\]
Hierbei bedeutet $\Gamma,\Gamma'$ die Konkatenation der Listen
$\Gamma$ und $\Gamma'$, also im Wesentlichen dasselbe wie die
Vereinigung $\Gamma\cup\Gamma'$, insofern man die Kontexte als
Mengen betrachtet.

\subsection{Zulässige Schlussregeln}

Wiewohl die Regeln des Schließens den Mechanismus zum Beweis
von Aussagen bilden, ist ihre Rolle sogar noch ein wenig tiefgreifender.
Wir können sie nämlich ebenfalls zur Ableitung \emph{weiterer Regeln}
nutzen. Das heißt, wir können sie dazu nutzen, den logischen Kalkül
selbst zu erweitern. Erweiterungen dieser Art nennen wir
\emph{zulässige Schlussregeln}%
\index{zulaessige Schlussregel@zulässige Schlussregel}.

Mit den bisherigen Regeln ist bereits die zulässige Regel
\[\dfrac{\Gamma\vdash A\cond B\qquad\Gamma'\vdash A}
{\Gamma,\Gamma'\vdash B}\]
ableitbar, die eine allgemeinere Form des Modus ponens darstellt. Man
erhält sie kurzerhand, indem den Prämissen des Modus
ponens jeweils die Abschwächungsregel vorgesetzt wird:
\[\infer{\Gamma,\Gamma'\vdash B}{
  \infer{\Gamma,\Gamma'\vdash A\cond B}{\Gamma\vdash A\cond B}
& \infer{\Gamma,\Gamma'\vdash A}{\Gamma'\vdash A}}
\]
Die einfache Form des Modus ponens erhält man mit $\Gamma':=\Gamma$ als
Spezialfall unter Anwendung der Kontraktionsregel.

\subsection{Implikationseinführung}

Ich möchte mich nun der Frage zuwenden, wie eine Implikation $A\cond B$
bewiesen wird. Intuitiv ist hierzu aus der Annahme $A$ die
Aussage $B$ zu folgern. Das heißt, es genügt die Ableitung
der Sequenz $A\vdash B$. Ein weiteres Mal gilt es noch zu
berücksichtigen, dass ein Beweis auch auf einen vorausgesetzten
Kontext $\Gamma$ beschränkt sein dürfen soll. Reflektiert man darüber
eine Weile, dürfte es der Überlegung nach wohl genügen, dass $A$
einfach dem Kontext $\Gamma$ hinzugefügt wird, woraus $B$ zu folgern
ist. Man gelangt zur Regel
\[\dfrac{\Gamma,A\vdash B}{\Gamma\vdash A\cond B}.\]
Wer diese Regel nicht so leicht fassbar findet, insbesondere nicht
direkt plausibel, ob sie bedenkenlos eingesetzt werden darf, der
ist nicht allein. Es gibt auch logische Kalküle, die diese Regel nicht
explizit enthalten. Sie tritt dennoch als \emph{Deduktionstheorem} in
Erscheinung, ein metalogisches Theorem, dessen Beweis erst erbracht
werden muss. Ich möchte diesen Weg allerdings aus einem bestimmten Grund
nicht gehen. Nämlich ist beim Beweis eigentlich natürliches Schließen
auf der metalogischen Ebene zu verwenden, wenn dies auch in informaler
Weise stattfinden mag. Aber nicht jeder Leser weiß zu diesem Zeitpunkt,
wie akkurates logisches Schließen geht. Der Leser benötigt am Anfang
etwas, um sich an den eigenen Haaren aus dem Sumpf zu ziehen.

\subsection{Axiome}

Zur Komplettierung des Kalküls gesellen sich schließlich auch noch
\emph{Axiome}\index{Axiom} hinzu, das sind gemachte Grundannahmen, die
nicht weiter bewiesen werden müssen. Sie sollten daher möglichst
plausibel, oder besser noch zweifelsfrei einsichtig sein. Für die Logik
selbst genügt das Axiom
\[A\vdash A.\]
Der Kalkül funktioniert dergestalt, dass für $A$ eine beliebige Aussage
eingesetzt werden darf, worunter auch zusammengesetzte Aussagen
fallen. Eine gern gewählter Weg der Definition des logischen
Kalküls sieht $A$ als eine metasprachliche Variable, für die eine
beliebige Formel eingesetzt werden darf. Unter dieser Sichtweise
spricht man von einem \emph{Axiomenschema}\index{Axiomenschema}. Wie
eine Schablone produziert es für jede Einsetzung einer konkreten
logischen Formel ein eigenes Axiom.

Anstelle $A,B,C$ werden für metasprachliche Variablen zuweilen auch
die griechischen Buchstaben $\varphi,\psi,\chi$ benutzt. Man muss sie
von atomaren logischen Variablen unterscheiden, für die wir in diesem
Buch, um Missverständnissen aus dem Weg zu gehen, kleine Buchstaben
$a,b,c$ oder $p,q,r$ verwenden werden. Sprachlich suggestiv steht
$\varphi$ für \emph{Formel}\index{Formel} oder \emph{formula}, $a$ für
\emph{Aussage} und $p$ für \emph{proposition}.

In diesem Sinne sind auch die Schlussregeln Schemata. Sofern man
Schlussregeln mit null Prämissen gestattet, lässt sich das
Axiomenschema auch als Regel
\[\dfrac{}{A\vdash A}\]
auffassen. In dieser Weise wollen wir die Anwendung von Axiomen in den
Beweisbäumen darstellen.

Axiome in der Form von Sequenzen heißen auch
\emph{Grundsequenzen}\index{Grundsequenz}.

Wir haben nun die Mittel in der Hand, um erste Theoreme beweisen
zu können. Es ist $A\cond A$ ein Theorem. Der Beweisbaum ist:
\[
\infer[\infernote{Implikationseinführung}]{\vdash A\cond A}{
  \infer[\infernote{Axiom}]{A\vdash A}{}}
\]
Unter der Lesung, dass $A$ eine Metavariable ist, handelt es
eigentlich nicht nur um ein Theorem, sondern um ein Schema von
Theoremen\index{Theoremschema}. Setzt man für $A$ bspw. die konkrete
Formel $p\cond q$ ein, bekommt man das konkrete Theorem
\[\vdash (p\cond q)\cond (p\cond q).\]
% [todo]
% Es ist zu bemerken, dass die Unterscheidung zwischen Metavariablen
% und atomaren logischen Variablen später durch die Einsetzungsregel
% mehr oder weniger hinfällig wird. Hierbei handelt es sich aber um eine
% höhere Überlegung, deren Beweis der Logiker erbringen will. In der
% Wissenschaft, insbesondere in der Logik, will man den Dingen auf den
% Grund gehen, will alles genau auseinandernehmen. Da möchte man bestimmte
% Regeln nicht einfach so als gegeben voraussetzen.

\subsection{Junktoren}

Bisher traten zusammengesetzte Aussagen alleinig in Form einer
Implikation auf. Will man logische Zusammenhänge beschreiben können,
muss die logische Sprache um weitere Junktoren bereichert werden.
Unter einem \emph{Junktor}\index{Junktor} versteht man einen logischen
Operator, der Aussagen zu einer zusammengesetzten Aussage verknüpft.
Von Belang sind zunächst fünf Stück.

Wir werden einen Junktor durch \emph{Einführungsregeln}%
\index{Einfuehrungsregel@Einführungsregel} und
\emph{Beseitigungsregeln}\index{Beseitigungsregel}
charakterisieren. Die Regeln der Implikation
wurden bereits beschrieben; die Einführung geschieht per
Implikationseinführung, die Beseitigung per Modus ponens.
Für die restlichen Junktoren der Aussagenlogik lassen sich die Regeln
wahlweise in Form von Axiomenschemata oder in Form von Schlussregeln
darstellen. Ich möchte das per Schemata machen, weil diese ein wenig
kompakter sind, was sie hoffentlich ein wenig leichter einsichtig macht.
Die entsprechenden Schlussregeln leiten wir anschließend als zulässige
Regeln ab.

Die Konjunktion\index{Konjunktion} $A\land B$, auch Und"=Verknüpfung
genannt, sprich »$A$ und $B$«, ist charakterisiert durch die Sequenzen
\[A,B\vdash A\land B;\qquad A\land B\vdash A;\qquad A\land B\vdash B.\]
Aus dem Fall von sowohl Regen als auch Schnee ist der Fall von
Schneeregen ableitbar. Aus dem Fall von Schneeregen ist der Fall
von Regen ableitbar. Aus dem Fall von Schneeregen ist der Fall
von Schnee ableitbar. So sind diese Sequenzen zu verstehen.

Die Einführung der Konjunktion geschieht mit der Regel
\[\dfrac{\Gamma\vdash A\qquad\Gamma'\vdash B}{\Gamma,\Gamma'\vdash A\land B}.\]
Denn es findet sich der Beweisbaum:
\[
\infer[\infernote{MP}]{\Gamma,\Gamma'\vdash A\land B}{
  \infer[\infernote{MP}]{\Gamma\vdash B\cond A\land B}{
    \infer[\infernote{Impl-Einf.}]{\vdash A\cond (B\cond A\land B)}{
      \infer[\infernote{Impl-Einf.}]{A\vdash B\cond A\land B}{
        \infer[\infernote{Axiom}]{A,B\vdash A\land B}{}}}
  & \Gamma\vdash A}
& \Gamma'\vdash B}
\]
Es steht MP als Abkürzung für Modus ponens, und Impl-Einf. für
Implikationseinführung. Man schreibt alternativ auch das Kürzel
$\cond$E anstelle Impl-Einf. und das Kürzel $\cond$B anstelle
von MP. Hierbei steht E offenkundig für \emph{Einführung} und
B für \emph{Beseitigung}. Aber Vorsicht, in der englischsprachigen
Literatur sind das I für \emph{introduction} und E für
\emph{elimination}.

Die beiden Regeln zur Beseitigung der Konjunktion sind
\[\dfrac{\Gamma\vdash A\land B}{\Gamma\vdash A},
\qquad\dfrac{\Gamma\vdash A\land B}{\Gamma\vdash B}.\]
Denn es findet sich:
\[
\infer[\infernote{MP}]{\Gamma\vdash A}{
  \infer[\infernote{Impl-Einf.}]{\vdash A\land B\cond A}{
    \infer[\infernote{Axiom}]{A\land B\vdash A}{}}
& \Gamma\vdash A\land B}
\]
Die Disjunktion\index{Disjunktion} $A\lor B$, auch Oder"=Verknüpfung
genannt, sprich »$A$ oder $B$«, ist charakterisiert durch die Sequenzen
\[A\vdash A\lor B;\qquad B\vdash A\lor B;\qquad
A\lor B, (A\cond C), (B\cond C)\vdash C.\]
So ist »Die Erde des Beetes ist nass« ableitbar aus »Es hat geregnet
oder das Beet wurde gegossen«. Denn sowohl »Es hat geregnet« als auch
»Das Beet wurde gegossen« impliziert »Die Erde des Beetes ist nass«.

Die beiden Regeln zur Einführung der Disjunktion sind
\[\dfrac{\Gamma\vdash A}{\Gamma\vdash A\lor B},\qquad
\dfrac{\Gamma\vdash B}{\Gamma\vdash A\lor B}.\]
Die Regel zur Beseitigung der Disjunktion ist
\[\dfrac{\Gamma\vdash A\lor B\qquad\Gamma',A\vdash C\qquad\Gamma'',B\vdash C}
{\Gamma,\Gamma',\Gamma''\vdash C}.\]
Die Beweise dieser Regeln seien dem Leser überlassen.

Eine Aussage wie »Bertram wird seine Hausaufgaben nicht machen«
formuliert man gern in der Form »Wenn Bertram seine Hausaufgaben macht,
färbt sich der Mond grün«. In gleichartiger Weise lässt sich die
Verneinung auch in der formalen Logik definieren. Hierzu legt man als
Hilfsbegriff zunächst $\bot$ als die \emph{Kontradiktion}%
\index{Kontradiktion}\index{Widerspruch} fest, sie steht für eine
widersprüchliche Formel.

Die Negation\index{Negation} $\lnot A$, auch Verneinung genannt, sprich
»nicht $A$«, definiert man als identisch mit $A\cond\bot$. Hierdurch
sind die Regeln zu ihrer Einführung und Beseitigung auf die der
Implikation zurückführbar. Es ergibt sich
\[\dfrac{\Gamma,A\vdash\bot}{\Gamma\vdash\lnot A},
\qquad\dfrac{\Gamma\vdash\lnot A\qquad\Gamma'\vdash A}
{\Gamma,\Gamma'\vdash\bot}.\]
Alternativ ließe sich die Negation durch die Sequenzen
\[(A\cond\bot)\vdash\lnot A;\qquad A,\lnot A\vdash\bot\]
charakterisieren. Man überzeuge sich, dass dies aufs selbe hinausläuft.

Die Äquivalenz\index{Aequivalenz@Äquivalenz} $A\bicond B$, sprich
»$A$ genau dann, wenn $B$«, definiert man als identisch mit
$(A\cond B)\land (B\cond A)$. Insofern sind die Regeln zu ihrer
Einführung und Beseitigung auf die der Konjunktion zurückführbar.
Es ergibt sich
\[\dfrac{\Gamma\vdash A\cond B\qquad\Gamma'\vdash B\cond A}
{\Gamma,\Gamma'\vdash A\bicond B},\qquad
\dfrac{\Gamma\vdash A\bicond B}{\Gamma\vdash A\cond B},\qquad
\dfrac{\Gamma\vdash A\bicond B}{\Gamma\vdash B\cond A}.\]
Die entsprechenden charakterisierenden Sequenzen sind
\[(A\cond B),(B\cond A)\vdash A\bicond B;\qquad (A\bicond B),A\vdash B;
\qquad (A\bicond B),B\vdash A.\]

\subsection{Quantoren}

Eine logische Sprache, die der freien Formulierung mathematischer
Zusammenhänge dienlich sein soll, muss hinreichend reichhaltig sein.
Ebenfalls schrieb der Philosoph Ludwig Wittgenstein in seinem
\emph{Tractatus} den ähnlichen Gedanke »Die Grenzen meiner Sprache
bedeuten die Grenzen meiner Welt.« Bislang fehlt das wichtige Konzept
der Quantifizierung, das die Aussagenlogik zur Prädikatenlogik
erweitert.

In der Prädikatenlogik treten \emph{Aussageformen}\index{Aussageform}
auf. Das sind Formeln, die \emph{freie Variablen}\index{freie Variable}
enthalten. Erst wenn jede der freien Variablen mit einem Wert belegt
wird, entsteht eine Aussage. Außerdem treten Quantoren auf. Ein
\emph{Quantor}\index{Quantor} bindet eine freie Variable, und macht
eine Aussageform dabei ebenfalls zu einer bestimmten Aussage.

% [todo]
% Enthalten dürfen. Wenn jede (möglicherweise von null) belegt wurde,
% entsteht eine Aussage.

Es sei $A(x)$ eine Aussageform mit der freien Variable $x$. Anstelle
von $A(x)$ schreibt man auch kurzum $A$. Anstelle von $A(t)$ schreibt
man auch $A[x:=t]$ oder $A[t/x]$, womit die Ersetzung jedes Vorkommens
von $x$ durch den Term $t$ gemeint ist. Genauer gesagt die Ersetzung
jedes \emph{freien} Vorkommens, wobei man außerdem einer möglichen
Überschattung einer der in $t$ enthaltenen Variablen aus dem Weg gehen
muss. Diese Spitzfindigkeiten tauchen allerdings erst auf, wenn man mit
Verschachtelungen von Quantoren hantiert. Ich will später
näher darauf eingehen.

Die wesentlichen beiden Quantoren sind der \emph{Allquantor}%
\index{Allquantor}\index{Universalquantor} $\forall$ und der
\emph{Existenzquantor}\index{Existenzquantor} $\exists$. Man ließt
$\forall x\colon A(x)$ als »für alle $x$ gilt $A(x)$« oder »jedes $x$
hat die Eigenschaft $A(x)$«. Man ließt $\exists x\colon A(x)$ als »es
gibt mindestens ein $x$, für das $A(x)$ gilt« oder »mindestens ein $x$
hat die Eigenschaft $A(x)$«.

Quantifiziert wird immer über ein bestimmtes \emph{Diskursuniversum}%
\index{Diskursuniversum}. Darunter versteht man die Gesamtheit der
Objekte, auf die sich »für alle« und »es gibt« bezieht. Um bestimmten
Komplikationen aus dem Weg zu gehen, muss es nichtleer sein. Zur
Veranschaulichung des Übergangs von der Aussagenlogik zur Prädikatenlogik
wählen wir ein endliches, das lediglich die Zahlen von eins bis vier
enthält. Die Aussage $\forall x\colon A(x)$ bedeutet nun insofern
dasselbe wie
\[A(1)\land A(2)\land A(3)\land A(4).\]
Diese schlichte Konjunktion gibt Anlass zu der Schlussregel
\[\dfrac{\Gamma\vdash\forall x\colon A(x)}{\Gamma\vdash A(t)}.
\qquad(\text{$t$ muss eine der Zahlen von eins bis vier sein})\]
Die Beseitigung der Allquantifizierung darf insofern als Analogon zur
Beseitigung der Konjunktion verstanden werden.

Im Fortgang soll $\Gamma\vdash A(a)$ bedeuten, dass die Aussageform
$A(a)$ aus dem Kontext $\Gamma$ ableitbar ist, wobei $a$ beliebig
gelassen wird. Man leitet die vier Sequenzen
\[\Gamma\vdash A(1);\quad\Gamma\vdash A(2);\quad\Gamma\vdash A(3);
\quad\Gamma\vdash A(4)\]
sozusagen in einen Zug ab. Es stellt sich nun die Frage
\[\dfrac{\Gamma\vdash A(a)}{\Gamma\vdash\forall x\colon A(x)}?\]
Betrachten wir dazu $a=1\vdash A(a)$. Mit der bedenklichen Regel erhielte
man aus ihr $a=1\vdash\forall x\colon A(x)$. Diese trifft insbesondere
im Fall $a:=1$ zu. Nun braucht man $1=1$ nicht vorauszusetzen, womit
man $\vdash\forall x\colon A(x)$ erhält. Den Quantor beseitigen
wir nun noch mit $x:=a$, und erhalten $\vdash A(a)$. Die Annahme
wurde also aus der Sequenz herausgemogelt. Um dies zu unterbinden,
legen wir fest, dass $a$ keine freie Variable einer der Antezedenzen
sein darf.

Die Regel zur Einführung ist demnach zu formulieren als
\[\dfrac{\Gamma\vdash A(a)}{\Gamma\vdash\forall x\colon A(x)}
(a\notin\mathrm{FV}(\Gamma,\forall x\colon A(x))).\]
Hierbei steht die Symbolik $\mathrm{FV}(\Gamma)$ für die Menge der
freien Variablen von $\Gamma$. Mit elementarer Mengenlehre definiert
man sie präzise als Rekursion über den Formelaufbau. Man legt fest
\begin{gather*}
\mathrm{FV}(A\land B) = \mathrm{FV}(A\lor B)
= \mathrm{FV}(A\cond B) = \mathrm{FV}(A\bicond B)
= \mathrm{FV}(A)\cup\mathrm{FV}(B),\\
\mathrm{FV}(\lnot A) = \mathrm{FV}(A),\quad
\hspace{13pt}\mathrm{FV}(\forall x\colon A) = \mathrm{FV}(\exists x\colon A)
= \mathrm{FV}(A)\setminus\{x\},\\
\mathrm{FV}(\bot) = \mathrm{FV}(\top) = \emptyset,\quad
\mathrm{FV}(P(t_1,\ldots,t_n)) = \mathrm{FV}(t_1)\cup\ldots\cup\mathrm{FV}(t_n).
\end{gather*}
Hierbei steht $P$ für ein $n$-stelliges Prädikat. Und es ist $\mathrm{FV}(t)$
die Menge der Variablen des Terms $t$. Man legt sie abermals
rekursiv fest als
\begin{gather*}
\mathrm{FV}(t_1+t_2) = \mathrm{FV}(t_1-t_2) = \mathrm{FV}(t_1\cdot t_2)
= \mathrm{FV}(t_1)\cup\mathrm{FV}(t_2),\\
\mathrm{FV}(-t)=\mathrm{FV}(t),\quad
\mathrm{FV}(v)=\{v\},\quad\mathrm{FV}(c) = \emptyset,
\end{gather*}
wobei $v$ für eine Variable und $c$ für eine Konstante steht.

Die Aussage $\exists x\colon A(x)$ ist gleichwertig mit
\[A(1)\lor A(2)\lor A(3)\lor A(4).\]
Diese Perspektive gibt Anlass zur Einführungsregel
\[\dfrac{\Gamma\vdash A(t)}{\Gamma\vdash\exists x\colon A(x)}.\quad
(\text{$t$ muss eine der Zahlen von eins bis vier sein})\]
Bei der Beseitigung müssen wir nun gewissermaßen eine Fallunterscheidung
in die vier Fälle vornehmen und bestätigen, dass jeder Fall dieselbe
Aussage $B$ impliziert. Dies soll allerdings parametrisch in einer
einzigen Ableitung stattfinden. Man gelangt zu
\[\dfrac{\Gamma\vdash\exists x\colon A(x)\qquad\Gamma',A(a)\vdash B}
{\Gamma,\Gamma'\vdash B}(a\notin\mathrm{FV}(\Gamma,\Gamma',B,\exists x\colon A(x))).\]
Diese Regel wird wie folgt interpretiert. Mit der Existenzaussage
$\exists x\colon A(x)$ liegt ein Zeuge $a$ mit $A(a)$ vor. Unter
Verwendung von $A(a)$ wird nun eine Aussage $B$ abgeleitet, in der $a$
nicht frei vorkommt. Somit gilt $B$ unabhängig vom gewählten Zeugen,
was notwendig ist, da unbekannt bleibt, welcher der Zahlen von eins
bis vier als Zeuge vorliegt.

Ohne die Bedingung an $a$ ließe sich leicht Schabernack vollführen.
Man könnte beispielsweise kurzerhand eine Existenzaussage zu einer
Allaussage machen:
\[\infer{\vdash\forall x\colon A(x)}{
  \infer{\vdash A(a)}{
    \vdash \exists x\colon A(x)
  & \infer{A(a)\vdash A(a)}{}}}\]
Abschließend verbleibt noch näher zu erläutern, wie Substitution
vonstatten geht. Ersetzt wird nur jedes freie Vorkommen einer
Variablen. Ein durch einen Quantor gebundenes Vorkommen der Variable
bleibt dagegen erhalten. So resultiert die Substitution
\[(A(x)\land\forall x: B(x))[x:=y]\quad\text{in}\quad
A(y)\land\forall x\colon B(x).\]
Außerdem darf es bei einer Substitution nicht zu einer Überschattung
durch eine gebundene Variable kommen. Die Substitution
\[(\forall y\colon A(x)\land B(y))[x:=y]\]
darf beispielsweise nicht direkt ausgeführt werden. Man geht der
Überschattung aus dem Weg, indem die gebundene Variable $y$ zuerst
in eine frische, nehmen wir $z$, umbenannt wird. Das Resultat der
Substitution ist also
\[\forall z\colon A(y)\land B(z),\quad\text{nicht}\quad
\forall y\colon A(y)\land B(y).\]

\subsection{Substitution}

Es ist noch zu präzisieren, wie Substitution genau vonstatten geht.
Substituiert werden können sowohl atomare logische Variablen gegen
Formeln als auch Individuenvariablen gegen Terme. Betrachten wir
zunächst die logischen.

Allgemein definiert man ihre Substitution
\[A[v_1:=C_1,\ldots v_n:=C_n],\,\text{kurz $A[S]$ oder $S(A)$}\]
rekursiv über den Formelaufbau als
\begin{align*}
(\lnot A)[S] &:= \lnot (A[S]), & (\forall x\colon A)[S] &:= (\forall x\colon (A[S])),\\
(A\circ B)[S] &:= ((A[S])\circ (B[S])), & (\exists x\colon A)[S] &:= (\exists x\colon (A[S])).
\end{align*}
wobei $\circ$ jeder der zweistelligen Junktoren
$\land,\lor,\cond,\bicond$ ist. Die Basisfälle sind für die atomaren
Variablen $v_1,\ldots, v_n$ und $w$ definiert gemäß
\[w[v_1:=C_1, \ldots, v_n:=C_n] := \begin{cases}
C_k, & \text{wenn sich $k$ mit $v_k=w$ findet},\\
w, & \text{sonst}.
\end{cases}\]
Für $n\ge 2$ spricht man von \emph{simultaner Substitution}.

Für $n=1$ vereinfacht sich die Substitution zu
\begin{align*}
w[v:=C] := \begin{cases}
C, & \text{wenn}\;v=w,\\
w, & \text{wenn}\;v\ne w.
\end{cases}
\end{align*}
Beispielsweise ist
\[(a\land b\cond a)[a:=a\lor b] = ((a\lor b)\land b\cond (a\lor b)).\]
Simultane Substitution darf im Allgemeinen nicht schrittweise
durchgeführt werden, weil dadurch ein anderes Resultat entstehen kann.
Zum Beispiel ist
\begin{alignat*}{2}
& (a\land b)[a:=b,b:=c] &&= (b\land c),\\
& (a\land b)[a:=b][b:=c] &&= (c\land c).
\end{alignat*}
Implementiert man die Substitution als Computerprogramm, bildet sie
üblicherweise abstrakte Syntaxbäume auf abstrake Syntaxbäume ab.

Die Substitution von Individuenvariablen gegen Terme definiert man
ganz analog. Hier ist allerdings hinsichtlich der Quantoren zu beachten,
dass nur freie Variablen substituiert werden, und man eine unter
Umständen entstehende Überschattung durch Umbenennung der gebundenen
Variablen umgehen muss.

\subsection{Zur Syntax}

So wie »Punktrechnung vor Strichrechnung« gilt, legt man für jeden
Junktor zur Einsparung von Klammern eine Stufe der Priorität fest. In
absteigender Rangfolge sind dies ${\lnot}, {\land}, {\lor}, {\cond},
{\bicond}$. So wird die Formel
\[\lnot A\land B\lor C\cond D\quad\text{gelesen als}\quad
(((\lnot A)\land B)\lor C)\cond D.\]
Weiterhin legt man die Implikation als rechtsassoziativ%
\index{rechtsassoziativ} fest. So wird
\[A\cond B\cond C\quad\text{gelesen als}\quad A\cond (B\cond C).\]
Wie den Junktoren kommt auch den Quantoren eine Rangstufe zu. Weil
diese aber präfix sind, ist bei ihnen lediglich die rechte Seite zu
berücksichtigen. Hier sind zwei Varianten verbreitet. In der
Schreibweise $(\forall x)A(x)$ oder kurz $\forall xA(x)$ haben
sie wie die Verneinung die höchste Rangstufe. In der Schreibweise
$\forall x\colon A(x)$ oder $\forall x.\, A(x)$ dagegen die niedrigste,
also eine Stufe niedriger als das Bikonditional, so dass alles
hinter dem Doppelpunkt in den Wirkungsbereich des Quantors fällt.
So wird
\[\forall x\colon A(x)\land B\cond C\quad\text{gelesen als}\quad
\forall x\colon ((A(x)\land B)\cond C).\]
Manche Schüler haben Schwierigkeiten, die Struktur von Termen zu
durchschauen. Infolge kann es bei ihnen zu Flüchtigkeitsfehlern
bei der Ersetzung von Variablen durch Terme kommen. Sie vergessen,
dass ein Term vor der Einfügung zunächst geklammert werden muss.
Erst die Operatorrangfolge gewährt es, die Klammern unter Umständen
nachträglich entfallen zu lassen. Für diese Schüler mag es förderlich
sein, einen Term als \emph{abstrakten Syntaxbaum} darzustellen.
Gleichermaßen verhält es sich mit der Programmiersprache Lisp, die
Terme als Schachtelung von Listen darstellt, deren Klammern
obligatorisch sind.  Die Aussage $A\land B\cond C$ ist beispielsweise
beschreibbar als
\[\text{\texttt{(implies (and A B) C)}}.\]
Im Wesentlichen veranschaulicht diese Schachtelung
nichts anderes als den abstrakten Syntaxbaum. Man kann gewissermaßen
sagen, dass Lisp eine Programmiersprache ohne Syntax ist. Fast ohne,
im höheren Sinne ohne.

Um sich unmissverständlich auszudrücken, formalisieren Logiker die
logische Sprache gern. Es wird hierzu eine \emph{formale Sprache} spezifiziert,
was vermittels sogenannter \emph{Produktionsregeln} gemacht werden kann.
Insofern Produktionsregeln etwas kryptisch anmuten mögen, beschreiben
Logiker die syntaktische Struktur auch in Worten.
Für die Aussagenlogik üblicherweise folgendermaßen.

\begin{enumerate}\setlength\itemsep{0em}
\item Die atomaren Variablen $a,b,c$ usw. sind Formeln.
\item Die Symbole $\bot,\top$ sind Formeln.
\item Ist $\lnot A$ eine Formel, so ist auch $(\lnot A)$ eine.
\item Sind $A,B$ Formeln, so ist auch $(A\land B)$ eine.
\item Sind $A,B$ Formeln, so ist auch $(A\lor B)$ eine.
\item Sind $A,B$ Formeln, so ist auch $(A\cond B)$ eine.
\item Sind $A,B$ Formeln, so ist auch $(A\bicond B)$ eine.
\item Nichts anderes ist eine Formel.
\end{enumerate}

\noindent
Schreibt man viele logische Formeln auf, drängt es, zumindest bei privaten
Notizen und Rechnungen, nach Kurzschreibweisen. In der Logik ist für das
Konditional $A\cond B$ auch die Schreibweise $A\rightarrow B$ gebräuchlich,
für das Bikonditional $A\bicond B$ entsprechend $A\leftrightarrow B$.
Insbesondere in der Schaltalgebra schreibt man auch $\overline A$
anstelle von $\lnot A$, und $AB$ anstelle von $A\land B$ sowie $A+B$
anstelle von $A\lor B$. Hierbei darf die Disjunktion $A+B$ allerdings
nicht mit der Kontravalenz $A\oplus B$ verwechselt werden. Für die
Quantifizierung $\forall x\colon A(x)$ bietet sich $\forall_x A_x$ oder
$\forall x.\, A_x$ als kurzschriftliche Form an.

\newpage
\section{Natürliches Schließen}

\subsection{Darstellungsformen}

Abgeleitet werden soll das Theorem
\[\vdash (A\cond B)\cond (\lnot B\cond\lnot A).\]
Meine favorisierte und in diesem Buch genutzte Form der Darstellung
des natürlichen Schließens fügt die aus den Schlussregeln erhaltenen
Schlüsse wie Legosteine zu einem Baum zusammen, dem
\emph{Beweisbaum}\index{Beweisbaum} oder \emph{Herleitungsbaum}.
Im Eigentlichen stehen in den Blättern die Grundsequenzen, und in der
Wurzel das Theorem. Wie wir es bereits getan haben, arbeitet man
allerdings auch mit Exemplaren, die irgendwelche Sequenzen in
irgendwelche Sequenzen überführen, womit man zulässige Schlussregeln
erhält. Allgemeiner ginge ferner die Formulierung als gerichteter
azyklischer Graph, die bei einigen Beweisen ein wenig den
Schreibaufwand reduzieren würde.

Der Beweisbaum des genannten Theorems ist:
\[
\infer[\infernote{$\cond$E}]{\vdash (A\cond B)\cond (\lnot B\cond\lnot A)}{
  \infer[\infernote{$\cond$E}]{A\cond B\vdash \lnot B\cond\lnot A}{
    \infer[\infernote{$\lnot$E}]{A\cond B, \lnot B\vdash\lnot A}{
      \infer[\infernote{$\lnot$B}]{A\cond B, \lnot B, A\vdash\bot}{
        \infer[\infernote{Axiom}]{\lnot B\vdash\lnot B}{}
      & \infer[\infernote{$\cond$B}]{A\cond B, A\vdash B}{
          \infer[\infernote{Axiom}]{A\cond B\vdash A\cond B}{}
        & \infer[\infernote{Axiom}]{A\vdash A}{}}}}}}
\]
Die Ausformulierung der Sequenzen verlangt langwieriges erneutes
Aufschreiben der Antezedenzen. Sobald man das Prozedere einmal
verstanden hat, erscheint es überausführlich. Man kann sich daher
verkürzte Darstellungen der Beweisbäume überlegen:
\[
\begin{tabular}{@{}l@{\qquad\quad}l}
\infer{\vdash (A\cond B)\cond (\lnot B\cond\lnot A)}{
  \infer{2\vdash \lnot B\cond\lnot A}{
    \infer{1, 2\vdash\lnot A}{
      \infer{1, 2, 3\vdash\bot}{
        \infer{1\equiv\lnot B}{}
      & \infer{2, 3\vdash B}{
          \infer{2\equiv A\cond B}{}
        & \infer{3\equiv A}{}}}}}}
&
\infer[\infernote{$\sim$2}]{(A\cond B)\cond (\lnot B\cond\lnot A)}{
  \infer[\infernote{$\sim$1}]{\lnot B\cond\lnot A}{
    \infer[\infernote{$\sim$3}]{\lnot A}{
      \infer{\bot}{
        \infer[\infernote{1}]{\lnot B}{}
      & \infer{B}{
          \infer[\infernote{2}]{A\cond B}{}
        & \infer[\infernote{3}]{A}{}}}}}}
\end{tabular}
\]
Die linke Form kürzt die Antezedenzen durch Nummern ab. In der rechten
Form entfallen die Antezedenzen vollständig. Stattdessen tauchen sie
als in den Blättern gemachte nummerierte \emph{Annahmen} auf, die im
Fortgang zur Wurzel irgendwann zu tilgen sind. Ihre Tilgung erscheint
nun als Randnotiz.

Eine weitere, sehr systematische Darstellung setzt den Beweis aus
einer Liste von Tabellenzeilen zusammen. Allerdings ist sie ein wenig
mühevoll zu lesen. Jede Zeile enthält eine Aussage und dahinter
zusätzlich die Information, wie und woraus die Aussage abgeleitet wurde.
Jede der Aussagen bekommt eine Nummer, siehe Tabelle \ref{tab:Kontraposition}.
Die Nummerierung der Abhängigkeiten ist in derselben Reihenfolge wie
zuvor bei den Bäumen angegeben. Wer die Liste genauer betrachtet, erkennt,
dass die jeweilige Zeile nichts anderes als die Sequenz
$\text{Abh.}\vdash\text{Nr.}$ darstellt.

\begin{table}
\begin{center}
\caption{Beweis in Form einer Liste von Tabellenzeilen}
\label{tab:Kontraposition}
\begin{tabular}{cclll}
\toprule
\strong{Abh.} & \strong{Nr.} & \strong{Aussage} & \strong{Regel} & \strong{auf}\\
\midrule[\heavyrulewidth]
1 & 1 & $\lnot B$ & Axiom &\\
2 & 2 & $A\cond B$ & Axiom &\\
3 & 3 & $A$ & Axiom &\\
2, 3 & 4 & $B$ & $\cond$B & 2, 3\\
1, 2, 3 & 5 & $\bot$ & $\lnot$B & 1, 4\\
1, 2 & 6 & $\lnot A$ & $\lnot$E & 5\\
2 & 7 & $\lnot B\cond\lnot A$ & $\cond$E & 6\\
$\emptyset$ & 8 & $(A\cond B)\cond(\lnot B\cond\lnot A)$ & $\cond$E & 7\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Unabhängig von Gentzen entwickelte Stanisław Jaśkowski das natürliche
Schließen einige Jahre zuvor. Während Gentzen Beweise als Bäume darstellte,
nutzte Jaśkowski zunächst eine grafische Darstellung, die später von Frederic
Brenton Fitch adaptiert wurde und in dieser Form nun als \emph{Fitch"=Style}%
\index{Fitch-Style} bekannt ist. Die Abhängigkeit von einer Annahme
wird hier kenntlich gemacht, indem die aus der Annahme abgeleiteten
Aussagen hinter einer senkrechten Linie eingerückt stehen. Die Annahme
selbst steht am Anfang der Einrückung, und zwar bereits innerhalb,
weil sie ja von sich selbst abhängig ist.
Siehe Tabelle \ref{tab:Kontraposition-Fitch}.
% Annahmen können im Fitch-Style nur in der umgekehrten Reihenfolge
% getilgt werden, in der sie eingeführt wurden.
% In \emph{forall x: Calgary} wird das Schließen im Fitch-Style
% sehr ausführlich beschrieben. Im Buch ist das Schließen sehr
% ausführlich beschrieben, die Formalisierung gänzlich im Fitch-Style
% gehalten.

\begin{table}
\begin{center}
\caption{Beweis im Fitch-Style}
\label{tab:Kontraposition-Fitch}
$\begin{nd}
\open
  \hypo {1} {A\cond B}
  \open
    \hypo {2} {\neg B}
    \open
    \hypo {3} {A}
    \have {4} {B} \ie{1,3}
    \have {5} {\bot} \ne{2,4}
    \close
  \have {6} {\neg A} \ni{5}
  \close
\have {7} {\neg B\cond\neg A} \ii{6}
\close
\have {8} {(A\cond B)\cond (\neg B\cond\neg A)} \ii{7}
\end{nd}$
\end{center}
\end{table}

Zu guter Letzt muss die klassische Darstellung der Beweisführung
aufgeführt werden. Die in Worten. Sie zeichnet sich durch die Auslassung
mühseliger technischer Details und blumige Formulierungen aus,
soll aber genug Information enthalten, dass der Leser im Zweifel
eine Formalisierung des Beweises erstellen und verifizieren kann.

\begin{Satz}
Es gilt $(A\cond B)\cond (\lnot B\cond\lnot A)$.
\end{Satz}
\strong{Beweis.} Aus der Annahme von sowohl $A\cond B$ als
auch $\lnot B$ als auch $A$ ist ein Widerspruch abzuleiten.
Man erhält $B$ zunächst per Modus ponens aus $A\cond B$ und $A$.
Nun steht $\lnot B$ bereits im Widerspruch zu $B$.\,\qedsymbol

Als komfortablen Bonus erhält man mit dem Theorem nun im Anschluss
kurzerhand eine weitere zulässige Regel, die
\emph{Kontrapositionsregel}\index{Kontraposition}
\[\dfrac{\Gamma\vdash A\cond B}{\Gamma\vdash\lnot B\cond\lnot A},
\quad\text{denn}\quad
\dfrac{\vdash (A\cond B)\cond (\lnot B\cond\lnot A)\quad\Gamma\vdash A\cond B}
{\Gamma\vdash \lnot B\cond\lnot A}.\]
Fügt man ihr den Modus ponens an, findet sich der
\emph{Modus tollens}\index{Modus tollens}
\[\dfrac{\Gamma\vdash A\cond B\qquad\Gamma'\vdash\lnot B}
{\Gamma,\Gamma'\vdash\lnot A}.\]

% [todo]
% De morgansche Regeln für die Verneinung von Quantifizierungen.
% Zum Beweis der Verneinung einer Allaussage genügt es, ein
% Gegenbeispiel zu finden.

\subsection{Theoreme der Prädikatenlogik}

In einer Formelsammlung zur Prädikatenlogik findet man eine Reihe von
Äquivalenzen und Implikationen vor, von denen wir einige beweisen
wollen.

\begin{Satz}
Es ist $A\land\forall x\colon B(x)$ äquivalent zu $\forall x\colon A\land B(x)$,
sofern die Variable $x$ nicht frei in $A$ vorkommt.
\end{Satz}
\begin{Beweis}
Zur Implikation von links nach rechts findet sich der Baum:
\begin{small}
\[
\begin{array}{@{}l@{\qquad}l}
\infer[\infernote{$\sim$1}]{(A\land\forall x\colon B(x))\cond (\forall x\colon A\land B(x))}{
  \infer{\forall x\colon A\land B(x)}{
    \infer{A\land B(x)}{
      \infer{A}{
        \infer[\infernote{1}]{A\land\forall x\colon B(x)}{}}
    & \infer{B(x)}{
        \infer{\forall x\colon B(x)}{
          \infer[\infernote{1}]{A\land\forall x\colon B(x)}{}}}}}}
&
\infer[\infernote{$\sim$1}]{(A\land\forall x\colon B(x))\cond (\forall x\colon A\land B(x))}{
  \infer{\forall x\colon A\land B(x)}{
    \infer{A\land B(u)}{
      \infer{A}{
        \infer[\infernote{1}]{A\land\forall x\colon B(x)}{}}
    & \infer{B(u)}{
        \infer{\forall x\colon B(x)}{
          \infer[\infernote{1}]{A\land\forall x\colon B(x)}{}}}}}}
\end{array}
\]
\end{small}%
Beide Formen sind zulässig. Die rechte Form gibt dem Parameter den
eigenen Bezeichner $u$, die linke nennt diesen ebenfalls so wie die
gebundene Variable. In Worten würde man diesen Beweis wie folgt
führen. Es sei $u$ fest, aber beliebig, woraus $A\land B(u)$ abzuleiten
ist. Laut Voraussetzung gilt sowohl $A$ als auch $\forall x\colon B(x)$.
Spezialisierung $x:=u$ liefert $B(u)$. Ergo gilt sowohl $A$ als
auch $B(u)$, und somit $A\land B(u)$.

Zur Implikation von rechts nach links findet sich:
\begin{small}
\[
\begin{array}{@{}l@{\qquad}l}
\infer[\infernote{$\sim$1}]{(\forall x\colon A\land B(x))\cond (A\land\forall x\colon B(x))}{
  \infer{A\land\forall x\colon B(x)}{
    \infer{A}{
      \infer{A\land B(x)}{
        \infer[\infernote{1}]{\forall x\colon A\land B(x)}{}}}
  & \infer{\forall x\colon B(x)}{
      \infer{B(x)}{
        \infer{A\land B(x)}{
          \infer[\infernote{1}]{\forall x\colon A\land B(x)}{}}}}}}
&
\infer[\infernote{$\sim$1}]{(\forall x\colon A\land B(x))\cond (A\land\forall x\colon B(x))}{
  \infer{A\land\forall x\colon B(x)}{
    \infer{A}{
      \infer{A\land B(u)}{
        \infer[\infernote{1}]{\forall x\colon A\land B(x)}{}}}
  & \infer{\forall x\colon B(x)}{
      \infer{B(u)}{
        \infer{A\land B(u)}{
          \infer[\infernote{1}]{\forall x\colon A\land B(x)}{}}}}}}
\end{array}
\]
\end{small}%
In Worten: Es ist $A\land\forall x\colon B(x)$ abzuleiten, also
sowohl $A$ als auch $\forall x\colon B(x)$, was sich mit $B(u)$ für ein
festes, aber beliebiges $u$ bestätigt. Laut Voraussetzung erhält man
$A\land B(u)$ per Spezialisierung $x:=u$. Ergo gilt sowohl $A$ als
auch $B(u)$.\,\qedsymbol
\end{Beweis}

\noindent
Die ausführliche Besprechung verdeutlicht nochmals, wie die Floskel
\emph{fest, aber beliebig} einen Parameter, über den die Argumentation verläuft,
einführt. Der Parameter steht für ein \emph{festes} Objekt, insofern er
während der Argumentation für nur ein Objekt steht. Weil das Objekt
\emph{beliebig} ist, also keine Einschränkungen an dessen Beschaffenheit
gemacht wurden, darf anschließend ohne Weiteres die Einführung einer
Allquantifizierung vorgenommen werden. Es besteht im logischen System,
wie es in diesem Buch dargelegt wurde, kein formaler Unterschied
zwischen einem Parameter und einer Variable. Ein Parameter verhält sich
als freie Variable.

\subsection{Bezug zum Sequenzenkalkül}

Einige Regeln des natürlichen Schließens bieten bei der Lesung eines
Beweisbaums von der Wurzel aus zu den Blättern hin routinemäßige
Zurückführung eines Ziels auf Unterziele. Jedoch nicht jede der
Regeln, womit das Schließen dennoch zum Denksport wird. Aufgrund dessen
gestaltet sich auch die Auffindung eines Algorithmus' zur automatischen
Erzeugung eines Beweisbaums als schwierig.

Der Sequenzenkalkül macht das Schließen nun gänzlich zur Routine. Mithin
ist zu diesem Kalkül ein Algorithmus zur Erzeugung von Beweisbäumen
vergleichsweise leicht zu finden.

\begin{table}
\begin{center}
\caption{Die Regeln des Sequenzenkalküls}
\label{tab:Sequenzenkalkuel}
\begin{tabular}{@{}c@{\quad\;\;}c@{\quad\;\;}c@{\quad\;\;}c@{}}
\toprule
& \strong{Linke Regel} & \strong{Rechte Regel} &\\
\midrule[\heavyrulewidth]
$\dfrac{\Gamma\vdash\Delta}{\Gamma,\Gamma'\vdash\Delta}$
& $\dfrac{\Gamma, A, B\vdash\Delta}{\Gamma, A\land B\vdash\Delta}$
& $\dfrac{\Gamma\vdash A,\Delta\qquad\Gamma'\vdash B,\Delta'}
{\Gamma,\Gamma'\vdash A\land B,\Delta,\Delta'}$
& $\dfrac{\Gamma\vdash\Delta}{\Gamma\vdash\Delta,\Delta'}$\\[14pt]
$\dfrac{\Gamma,A,A\vdash\Delta}{\Gamma,A\vdash\Delta}$
& $\dfrac{\Gamma,A\vdash\Delta\qquad\Gamma',B\vdash\Delta'}
{\Gamma,\Gamma',A\lor B\vdash\Delta,\Delta'}$
& $\dfrac{\Gamma\vdash A,B,\Delta}{\Gamma\vdash A\lor B,\Delta}$
& $\dfrac{\Gamma\vdash B,B,\Delta}{\Gamma\vdash B,\Delta}$\\[14pt]
$\dfrac{\Gamma,\top\vdash\Delta}{\Gamma\vdash\Delta}$
& $\dfrac{\Gamma\vdash A,\Delta\qquad\Gamma',B\vdash\Delta'}
{\Gamma,\Gamma',A\cond B\vdash\Delta,\Delta'}$
& $\dfrac{\Gamma,A\vdash B,\Delta}{\Gamma\vdash A\cond B,\Delta}$
& $\dfrac{\Gamma\vdash\Delta,\bot}{\Gamma\vdash\Delta}$\\[14pt]
$\dfrac{}{\Gamma,\bot\vdash\Delta}$
& $\dfrac{\Gamma\vdash A,\Delta}{\Gamma,\lnot A\vdash\Delta}$
& $\dfrac{\Gamma,A\vdash\Delta}{\Gamma\vdash\lnot A,\Delta}$
& $\dfrac{}{\Gamma\vdash\top,\Delta}$\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Man darf als dienlich erachten, dass die Darstellung des natürlichen
Schließens, wie sie in diesem Buch dargelegt wurde, mit dem
Sequenzenkalkül kompatibel ist. In ihm dürfen Sequenzen von der
allgemeineren Form
\[A_1\ldots,A_m\vdash B_1,\ldots,B_n\]
sein, die für die Aussage
\[A_1\land\ldots\land A_m\cond B_1\lor\ldots\lor B_n\]
steht. Um die Regeln des Sequenzenkalküls vermittels natürlichem Schließen
als zulässige Regeln herzuleiten, wird man daher zunächst die
Übersetzungsregeln
\[\dfrac{\Gamma\vdash B_1\lor\ldots\lor B_n}{\Gamma\vdash B_1,\ldots,B_n},\quad\;
\dfrac{\Gamma\vdash\bot}{\Gamma\vdash},\quad\;
\dfrac{\Gamma\vdash B_1,\ldots,B_n}{\Gamma\vdash B_1\lor\ldots\lor B_n},\quad\;
\dfrac{\Gamma\vdash}{\Gamma\vdash\bot}.\]
fordern. Eine Auflistung der wesentlichen Regeln zeigt die Tabelle
\ref{tab:Sequenzenkalkuel}. Sie untergliedern sich in linke und rechte
Regeln. Die linken gestatten es dabei, Ziele ebenfalls bezüglich
Antezedenzen auf Unterziele zurückzuführen. Jede der Ansammlungen
$\Gamma,\Gamma',\Delta,\Delta'$ darf leer sein. Eine algorithmische
Umsetzung der Beweissuche mag $\Gamma=\Gamma'$ und $\Delta=\Delta'$
setzen, für den Menschen entstünde dadurch aber umständlicher
Schreibaufwand. Exemplarisch soll die linke Regel zur Disjunktion als
zulässig bestätigt werden. Für sie findet sich der Baum:
\[
\infer{\Gamma,\Gamma',A\lor B\vdash \Delta,\Delta'}{
  \infer{\Gamma,\Gamma',A\lor B\vdash C}{
    \infer{A\lor B\vdash A\lor B}{}
  & \infer{\Gamma,A\vdash C}{
      \infer{\Gamma,A\vdash\Delta,\Delta'}{
        \Gamma,A\vdash\Delta}}
  & \infer{\Gamma',B\vdash C}{
      \infer{\Gamma',B\vdash\Delta,\Delta'}{
        \Gamma',B\vdash\Delta'}}}}
\]
Hierbei soll $C$ die Disjunktion der Aussagen von $\Delta,\Delta'$ sein.

Es folgt am Beispiel des Theoremschemas zur Kontraposition, wie das
Schließen vonstatten geht. Die Beweisbäume wären von unten nach oben
zu lesen, weil das, was weiter oben steht, eigentlich noch unbekannt ist:
\[
\begin{array}{@{}l@{\qquad\quad}l}
\infer{\vdash (A\cond B)\cond (\lnot B\cond\lnot A)}{
  \infer{A\cond B\vdash\lnot B\cond\lnot A}{
    \infer{A\cond B, \lnot B\vdash\lnot A}{
      \infer{\vdash A, \lnot A}{
        \infer{A\vdash A}{}}
    & \infer{B, \lnot B\vdash}{
        \infer{B\vdash B}{}}}}}
&
\infer{\vdash (\lnot B\cond\lnot A)\cond (A\cond B)}{
  \infer{\lnot B\cond\lnot A\vdash A\cond B}{
    \infer{\lnot B\cond\lnot A, A\vdash B}{
      \infer{\vdash \lnot B, B}{
        \infer{B\vdash B}{}}
    & \infer{\lnot A, A\vdash}{
        \infer{A\vdash A}{}}}}}
\end{array}
\]
Der Kalkül muss ein klassischer sein, sonst wäre die Kontraposition
nicht rückgängig zu machen. Noch drastischere Einsicht diesbezüglich
schaffen die Bäume:
\[
\begin{array}{l@{\qquad}l}
\infer[\infernote{$\lor$L}]{\vdash A\lor\lnot A}{
  \infer[\infernote{$\lnot$R}]{\vdash A,\lnot A}{
    \infer{A\vdash A}{}}}
&
\infer[\infernote{$\lnot$R}]{\lnot\lnot A\vdash A}{
  \infer[\infernote{$\lnot$L}]{\vdash\lnot A, A}{
    \infer{A\vdash A}{}}}
\end{array}
\]
Es gibt auch Varianten des Sequenzenkalküls, die ausschließlich die
Ableitung von Theoremen der intuitionistischen Logik gestatten. Sie sind
allerdings umständlicher zu verwenden, da es sich um Einschränkungen des
klassischen Kalküls handelt. Bereits Gentzen beschrieb so einen
als LJ in \cite{Gentzen1935}. Eine sorgfältige Diskussion findet man
in \cite{Mimram}.

Alternative Formen der Regeln zur Implikation sind
\[\dfrac{\Gamma,\lnot A\vdash\Delta\qquad\Gamma, B\vdash\Delta}
{\Gamma, A\cond B\vdash\Delta},\qquad
\dfrac{\Gamma\vdash \lnot A, B,\Delta}{\Gamma\vdash A\cond B,\Delta}.\]
Die Regeln der Allquantifizierung sind
\[\dfrac{\Gamma, A(t)\vdash\Delta}{\Gamma,\forall x\colon A(x)\vdash\Delta},\qquad
\dfrac{\Gamma\vdash A(u),\Delta}{\Gamma\vdash (\forall x\colon A(x)),\Delta}
(u\notin\mathrm{FV}(\Gamma,\Delta,A(x))),\]
die der Existenzquantifizierung sind
\[\dfrac{\Gamma, A(u)\vdash\Delta}{\Gamma,\exists x\colon A(x)\vdash\Delta}
(u\notin\mathrm{FV}(\Gamma,\Delta,A(x))),\qquad
\dfrac{\Gamma\vdash A(t),\Delta}{\Gamma\vdash (\exists x\colon A(x)),\Delta}.\]
Eine besondere Bedeutung besitzt die Schnittregel
\[\dfrac{\Gamma\vdash A,\Delta\qquad\Gamma',A\vdash\Delta'}
{\Gamma,\Gamma'\vdash\Delta,\Delta'}.\]
Laut Gentzens Hauptsatz findet sich zu jedem Beweis einer Sequenz,
in dem die Schnittregel zur Anwendung kommt, ein alternativer Beweis,
der auf sie verzichtet. Kurzum ist sie zulässig, aber redundant. Dieses
Resultat ist von großer Bedeutung für die Beweistheorie.

\subsection{Bezug zum Tableaukalkül}

Der \emph{Tableaukalkül}\index{Tableaukalkuel@Tableaukalkül} ist ein
systematisches Beweisverfahren, bei dem durch abermalige Zurückführung
einer Formel auf kleinere Formeln ein Baum entsteht. In einer
geläufigen Form des Verfahrens kommt der Beweis einer Aussage zustande,
indem ihre Verneinung widerlegt wird. Die Widerlegung stellt sich
dadurch her, dass jeder Pfad eine Formel enthält, deren
Verneinung bereits auf dem direkten Pfad zur Wurzel vorkam, was auch
als \emph{Schließung} des Pfades bezeichnet wird. Der Baum verzweigt
sich nicht bei jeder Formel. Man unterscheidet zwischen
Formeln vom Typ einer Konjunktion und Formeln vom Typ einer
Disjunktion. Lediglich bei den Formeln vom Typ einer Disjunktion
findet eine Verzweigung statt.

Es stellt sich im Fortgang heraus, dass der Tableaukalkül in der Logik
nicht abgeschieden steht. Ganz im Gegenteil lässt sich ein enger Bezug
zum Schließen von Sequenzen herstellen. Genauer gesagt gehört zu jeder
Regel des Tableaukalküls eine zulässige Regel des Schließens von
Sequenzen, womit sich dieser als ein Teilsystem des allgemeinen
Sequenzenkalküls erweist.

Laut der Reductio ad absurdum ist $\Gamma\vdash A$ auf
$\Gamma,\lnot\vdash\bot$ zurückführbar. Im Weiteren wird $\Gamma\vdash$
als Abkürzung für $\Gamma\vdash\bot$ geschrieben. Man ruft sich nun
die Äquivalenz der Aussagen $A\cond B$ und $\lnot A\lor B$ in Erinnerung.
Weiterhin befindet man mit den de morganschen Gesetzen die Aussage
$\lnot (A\land B)$ zu $\lnot A\lor\lnot B$ äquivalent, sowie $\lnot (A\lor B)$
zu $\lnot A\land\lnot B$. Aus diesen Überlegungen heraus
gelangt man zu den unverzweigenden zulässigen Regeln%
\[
\dfrac{\Gamma,A,B\vdash}{\Gamma,A\land B\vdash},\qquad
\dfrac{\Gamma,\lnot A,\lnot B\vdash}{\Gamma,\lnot (A\lor B)\vdash},\qquad
\dfrac{\Gamma,A,\lnot B\vdash}{\Gamma,\lnot(A\cond B)\vdash},
\]
und den verzweigenden zulässigen Regeln
\[
\dfrac{\Gamma,A\vdash\quad\Gamma,B\vdash}{\Gamma,A\lor B\vdash},\qquad
\dfrac{\Gamma,\lnot A\vdash\quad\Gamma,\lnot B\vdash}{\Gamma,\lnot (A\land B)\vdash},\qquad
\dfrac{\Gamma,\lnot A\vdash\quad\Gamma,B\vdash}{\Gamma,(A\cond B)\vdash}.
\]
Schließlich wäre noch festzustellen, dass $\Gamma,A,\lnot A\vdash$
mit $\Gamma,A\vdash A$ gleichbedeutend ist, und somit die Rolle einer
Grundsequenz einnehmen darf.

Ein Beispiel. Mit den aufgestellten Regeln ergibt sich für die
Kontraposition der folgende Beweisbaum:
\[
\infer{\vdash (A\cond B)\cond (\lnot B\cond\lnot A)}{
  \infer{\lnot ((A\cond B)\cond (\lnot B\cond\lnot A))\vdash}{
    \infer{A\cond B, \lnot(\lnot B\cond\lnot A)\vdash}{
      \infer{A\cond B, \lnot B, \lnot\lnot A\vdash }{
        \infer{\lnot A, \lnot B, \lnot\lnot A\vdash}{}
      & \infer{B, \lnot B, \lnot\lnot A\vdash}{}}}}}
\]
Wird dieser Baum nun auf den Kopf gestellt und die Notation dergestalt
verkürzt, dass jede der Formeln nur einmal aufgeschrieben werden
muss, ergibt sich der Tableaukalkül. Man mag das Schließen von
Sequenzen insofern als vielseitig bewerten. Die dazugewonnene Sichtweise
schafft überdies ein tiefergründiges Verständnis des Tableaukalküls.

\section{Bemerkungen zur Beweisführung}

\subsection{Widerspruchsbeweise}

Beim \emph{Beweis durch Widerspruch} widerlegt man eine Aussage, indem
gezeigt wird, dass die Annahme der Aussage zu einem logischen
Widerspruch\index{Widerspruch} führt. In manchen Situationen bietet
diese Art der Argumentation eine große Hilfe. So schreibt der britische
Mathematiker Godfrey Harold Hardy in seinem Essay
\emph{A Mathematician's Apology} die Worte
\begin{quote}
»The proof is by \emph{reductio ad absurdum}, and \emph{reductio ad
absurdum}, which Euclid loved so much, is one of a mathematician's
finest weapons. It is a far finer gambit than any chess gambit: a chess
player may offer the sacrifice of a pawn or even a piece, but a
mathematician offers \emph{the game}.«
\end{quote}
Zur Schaffung von Klarheit muss man zunächst zwei inhaltlich
verschiedene Arten des Widerspruchsbeweises unterscheiden.
Präzisieren lässt sich diese Unterscheidung anhand
der Regeln
\[\dfrac{\Gamma,A\vdash\bot}{\Gamma\vdash\lnot A},\qquad
\dfrac{\Gamma,\lnot A\vdash\bot}{\Gamma\vdash A}.\]
Die linke Regel stellt die stets verfügbare Negationseinführung dar,
die man auch als \emph{Widerlegung durch Widerspruch} bezeichnen kann.
In der rechten Regel, der klassischen \emph{Reductio ad absurdum},
gelangt man zunächst per Negationseinführung von
$\Gamma,\lnot A\vdash\bot$ zu $\Gamma\vdash\lnot\lnot A$, und daraufhin
zu $\Gamma\vdash A$. Die Beseitigung der Doppelnegation%
\index{Doppelnegation} ist allerdings lediglich in der klassischen
Logik verfügbar, in der intuitionistischen gilt sie dagegen als
unzulässig.

Manche stellen die Regeln in einer Form dar, in der die Kontradiktion
nicht explizit auftaucht. Wir erhalten sie als zulässige Regeln, indem
der Einführung der Kontradiktion direkt ihre Beseitigung
angeschlossen wird. Es findet sich
\[\dfrac{\Gamma,A\vdash\lnot B\qquad\Gamma'\vdash B}
{\Gamma,\Gamma'\vdash\lnot A},\qquad
\dfrac{\Gamma,\lnot A\vdash\lnot B\qquad\Gamma'\vdash B}
{\Gamma,\Gamma'\vdash A}.\]
Wie zuvor ist die linke Form allgemein verfügbar, die rechte dagegen
nur bei Vorhandensein der Beseitigung der Doppelnegation.

\subsection{Klassische Kontraposition}

Eine weitere Regel ist die umgekehrte Kontraposition
\[\dfrac{\Gamma\vdash\lnot A\cond\lnot B}{\Gamma\vdash B\cond A}.\]
Sie verlangt ebenfalls die klassische Logik. Mit ihr lässt sich
nämlich die klassische Reductio ad absurdum herleiten:
\[
\infer[\infernote{Kürzung der Tautologie}]{\Gamma\vdash A}{
  \infer[\infernote{Modus ponens}]{\Gamma,\top\vdash A}{
    \infer[\infernote{bedenklich}]{\Gamma\vdash\top\cond A}{
      \infer[\infernote{Impl-Einf.}]{\Gamma\vdash\lnot A\cond\lnot\top}{
        \infer[\infernote{Neg-Einf.}]{\Gamma,\lnot A\vdash\lnot\top}{
          \infer[\infernote{Abschwächung}]{\Gamma,\lnot A,\top\vdash\bot}{
            \Gamma,\lnot A\vdash\bot}}}}
  & \infer[\infernote{Axiom}]{\top\vdash\top}{}}}
\]
Weil alle anderen Schlüsse unbedenklich sind, kann die umgekehrte
Kontraposition als der bedenkliche Schritt identifiziert werden.
In der klassischen Logik ist sie allerdings zulässig. Die Herleitung
unter Verwendung der Beseitigung der Doppelnegation sei dem Leser
überlassen. Als kleiner Tipp sei aber gegeben, dass die Einführung
der Doppelnegation unter allen Umständen zulässig ist, wie man sich
unschwer überzeugt.

Wir schreiben die Abkürzungen DNE für die Beseitigung der
Doppelnegation, LEM für den Satz vom ausgeschlossenen Dritten
und EFQ für ex falso quodlibet. Eine Alternative zu DNE bietet
LEM zuzüglich EFQ. Manche Beweise verkürzen sich damit, andere
verlängern sich. Tatsächlich lässt sich DNE aus LEM zuzüglich EFQ
herleiten. Umgekehrt lässt sich sowohl LEM als auch EFQ aus DNE
herleiten. Es tut sich die Frage auf, ob sich EFQ aus LEM herleiten
lässt. Die Antwort darauf lautet nein. Eine ausführliche Untersuchung
findet man in \cite{Diener}.

\subsection{Notwendige und hinreichende Bedingungen}

Manchmal trifft man auf die Ausdrucksweise, eine Bedingung $B$ sei für
eine Aussage $A$ notwendig. Sie sagt aus, dass $\lnot B$ zu $\lnot A$
führt. Falls die Bedingung verletzt ist, kann die Aussage unmöglich
gelten. Es liegt demnach die Implikation $\lnot B\cond\lnot A$ vor.
Sie wird im Sinne der klassischen Logik verstanden. Man hat also
\[(A\cond B) \iff (\text{$B$ ist notwendig für $A$}).\]
Die Ausdrucksweise, eine Bedingung $B$ sei für $A$ hinreichend,
sagt aus, dass $B$ die Aussage $A$ bereits nach sich zieht. Es liegt
demnach die Implikation $B\cond A$ vor. Man hat also
\[(B\cond A) \iff (\text{$B$ ist hinreichend für $A$}).\]
Ist eine Bedingung $B$ sowohl notwendig als auch hinreichend für $A$,
liegt eine Äquivalenz vor. Man hat also
\[(A\bicond B) \iff (\text{$B$ ist notwendig und hinreichend für $A$}).\]

\newpage
\subsection{Die zulässige Ersetzungsregel}

Mit der Äquivalenz zweier Aussagen verhält es sich in gewisser Weise
wie mit einer Gleichheit. Und zwar vermittelt die \emph{zulässige
Ersetzungsregel}, eine Teilformel gegen eine zu ihr äquivalente
Formel ersetzen zu dürfen, analog wie bei der Termumformung eines
Teilterms. Sie ermöglicht die bequeme Äquivalenzumformung von
Aussagen, womit man sie als besonders nützlich erachten darf.

\begin{Satz}[Zulässige Ersetzungsregel]%
\index{Ersetzungsregel}\newlinefirst
Es gelten die beiden gleichwertigen Regeln
\[\dfrac{\Gamma\vdash A\Leftrightarrow B}
{\Gamma\vdash C(A)\Leftrightarrow C(B)},\qquad
\dfrac{\Gamma\vdash A\Leftrightarrow B\qquad\Gamma'\vdash C(A)}
{\Gamma,\Gamma'\vdash C(B)}.\]
\end{Satz}
\begin{Beweis}
Zunächst zur Gleichwertigkeit der beiden Regeln:
\begin{small}
\[
\begin{array}{@{}l@{\;\;}l}
\infer{\Gamma,\Gamma'\vdash C(B)}{
  \infer{\Gamma\vdash C(A)\cond C(B)}{
    \infer{\Gamma\vdash C(A)\bicond C(B)}{
      \Gamma\vdash A\bicond B}}
& \Gamma'\vdash C(A)}
&
\infer{\Gamma\vdash C(A)\bicond C(B)}{
  \infer{\Gamma\vdash C(A)\cond C(B)}{
    \infer{\Gamma, C(A)\vdash C(B)}{
      \Gamma\vdash A\bicond B
    & \infer{C(A)\vdash C(A)}{}}}
& \infer{\Gamma\vdash C(B)\cond C(A)}{
    \infer{\Gamma, C(B)\vdash C(A)}{
      \Gamma\vdash A\bicond B
    & \infer{C(B)\vdash C(B)}{}}}}
\end{array}
\]
\end{small}%
Wir führen nun eine strukturelle Induktion über den Formelaufbau durch.
Die Behauptung wird hierbei in der Form
\[\dfrac{\Gamma\vdash F\Leftrightarrow F'}{
\Gamma,C(F)\vdash C(F')},\quad C(F) := C[X:=F],\quad C(F') := C[X:=F']\]
geschrieben. Weil $F,F'$ vertauscht werden dürfen, erhält man somit auch
die umgekehrte Folgerung, so dass die Äquivalenz von $C(F)$ und $C(F')$
hergestellt wird. Wir definieren die Abkürzungen
\[A := C(F),\quad A' := C(F'),\quad B := D(F),\quad B' := D(F').\]
Zunächst die Basisfälle. Die Formeln $C:=\bot$,
$C:=\top$ und $C:=v$ mit atomarer Variable $v\ne X$
bleiben von der Substitution unbetroffen und sind offenkundig zu sich
selbst äquivalent. Für $C=X$ erhält man schlicht die Prämisse.

Zum Induktionsschritt. Man hat nun
\[(C\land D)[X{:=}F]
\iff C[X{:=}F]\land D[X{:=}F]\iff A\land B\]
usw. Induktionsvoraussetzung sei also $\Gamma\vdash A\Leftrightarrow A'$
und $\Gamma\vdash B\Leftrightarrow B'$. Zu zeigen ist
\begin{align*}
& \Gamma,A\land B\vdash A'\land B',
&& \Gamma,A\cond B\vdash A'\cond B',
&& \Gamma,\forall x\colon A\vdash\forall x\colon A',\\
& \Gamma,A\lor B\vdash A'\lor B',
&& \Gamma,A\bicond B\vdash A'\bicond B',
&& \Gamma,\exists x\colon A\vdash\exists x\colon A'
\end{align*}
und $\Gamma,\lnot A\vdash\lnot A'$. Es findet sich:
\begin{small}
\[
\begin{array}{@{}l@{\quad\;\;}l}
\infer{\Gamma,\lnot A\vdash\lnot A'}{
  \infer{\Gamma,\lnot A,A'\vdash\bot}{
    \infer{\Gamma,A'\vdash A}{\Gamma\vdash A\bicond A'}
  & \infer{\lnot A\vdash\lnot A}{}}}
&
\infer{\Gamma,A\land B\vdash A'\land B'}{
  \infer{\Gamma,A\land B\vdash A'}{
    \infer{\Gamma\vdash A\cond A'}{\Gamma\vdash A\bicond A'}
  & \infer{A\land B\vdash A}{
      \infer{A\land B\vdash A\land B}{}}}
& \infer{\Gamma,A\land B\vdash B'}{
    \infer{\Gamma\vdash B\cond B'}{\Gamma\vdash B\bicond B'}
  & \infer{A\land B\vdash B}{
      \infer{A\land B\vdash A\land B}{}}}}
\end{array}
\]
\end{small}%
Die Erstellung der restlichen Bäume sei dem Leser überlassen.

Für die Quantoren findet sich:
\[
\begin{array}{@{}l@{\quad\;\;}l}
\infer[\infernote{$x\notin\mathrm{FV}(\Gamma)$}]{\Gamma,\forall x\colon A\vdash\forall x\colon A'}{
  \infer{\Gamma,\forall x\colon A\vdash A'}{
    \infer{\Gamma\vdash A\cond A'}{\Gamma\vdash A\bicond A'}
  & \infer{\forall x\colon A\vdash A}{
      \infer{\forall x\colon A\vdash\forall x\colon A}{}}}}
&
\infer[\infernote{$x\notin\mathrm{FV}(\Gamma)$}]{\Gamma,\exists x\colon A\vdash\exists x\colon A'}{
  \infer{\exists x\colon A\vdash\exists x\colon A}{}\hspace{-12pt}
& \infer{\Gamma, A\vdash\exists x\colon A'}{
    \infer{\Gamma, A\vdash A'}{
      \infer{\Gamma\vdash A\cond A'}{
        \Gamma\vdash A\bicond A'}
    & \infer{A\vdash A}{}}}}
\end{array}
\]
Es darf hierbei $x\notin\mathrm{FV}(\Gamma)$ vorausgesetzt werden, weil
die gebundene Variable andernfalls ja vor der Betrachtung in eine
frische umbenannt werden kann.\,\qedsymbol
\end{Beweis}

\noindent
Es wäre noch zu erwähnen, dass die Regel bei den modalisierenden
Operatoren für gewöhnlich unzulässig ist. So ist in den modallogischen
Systemen K, B, D, S4, S5 keine der Formeln
\begin{gather*}
(a\bicond b)\cond (\lnec a\bicond\lnec b),\\
(a\bicond b)\cond (\lpos a\bicond\lpos b)
\end{gather*}
ein Theorem. Ein Gegenmodell ist jeweils schnell gefunden, dafür
bedarf es nicht mehr als zwei Welten.

\newpage
\section{Logik mit Gleichheit}

\subsection{Axiome der Gleichheit}

Im Folgenden Abschnitt geht es um allgemeingültige Erwägungen zur
Gleichheit. Das wären Gesetzmäßigkeiten, die die Gleichheit immer
erfüllen soll, ungeachtet, ob sie zwischen zwei Zahlen, zwei Mengen,
oder zwei wie auch immer gearteten Objekten besteht.

Moderne Formulierungen charakterisieren die Gleichheit durch die Axiome
\begin{align*}
&\vdash\forall x\colon x=x, &&\text{(Reflexivität)}\\
&\vdash\forall x\colon\forall y\colon x=y\cond A(x)\cond A(y).
&&\text{(Ersetzung)}
\end{align*}
Das zweite Axiom ist eigentlich ein Schema, weil dieses für jede
Formel $A$ gilt. Es meint $A(u)$ eine Aussageform, wobei $A(t)$ als
$A(u)[u:=t]$ zu verstehen sein soll. Man gewinnt aus dem Schema
kurzerhand die Regel
\[\dfrac{\Gamma\vdash t=t'\qquad\Gamma'\vdash A(t)}
{\Gamma,\Gamma'\vdash A(t')}.\qquad\text{($t,t'$ sind beliebige Terme)}\]
Das erste Axiom charakterisiert insofern die Regel zur Einführung
der Gleichheit, das zweite die Regel zur Beseitigung. Die Symmetrie
der Gleichheit lässt sich aus den beiden Regeln ableiten. Sei hierzu
$A(u):\bicond (u=x)$. Nun ist $A(x)\bicond (x=x)$ und
$A(y)\bicond (y=x)$. Man setze $t:=x$ und $t':=y$. Es findet sich:
\[
\infer{x=y\vdash y=x}{
  \infer{x=y\vdash x=y}{}
& \infer{\vdash x=x}{}}
\]
Aus dieser Sequenz erhält man anschließend
\[\vdash\forall x\colon\forall y\colon x=y\cond y=x.\]
Bezüglich $A(u):\bicond (f(x)=f(u))$ führt die Ausübung der soeben
gemachten Vorgehensweise auf
\[\vdash\forall x\colon\forall y\colon x=y\cond f(x)=f(y).\]
Es induziert die Ersetzungsregel für Funktionen,
\[\dfrac{\Gamma\vdash t=t'}{\Gamma\vdash f(t)=f(t')}.\]
Zu beachten wäre allerdings, dass $f$ hierfür auf dem gesamten
Diskursuniversum definiert sein muss. Würde das Symbol $f$ mit einer
Funktion belegt, die für eine bestimmte Belegung von $x$ nicht definiert
ist, was soll $f(x)$ dann sein?

Mehrmalige Anwendung des Ersetzungsaxioms ermöglicht darüber hinaus
mehrstellige Ersetzungen wie
\begin{gather*}
\vdash\forall x,x',y,y'\colon x=x'\land y=y'\cond A(x,y)\cond A(x',y'),\\
\vdash\forall x,x',y,y'\colon x=x'\land y=y'\cond f(x,y)=f(x',y').
\end{gather*}
Ferner implizieren die Axiome das Transitivgesetz
\[\vdash\forall x,y,z\colon x=y\land y=z\cond x=z.\]
Es steht $\forall x,y,z\colon A$ als Abkürzung für
$\forall x\colon\forall y\colon\forall z\colon A$.

Ganz allgemein gilt
\[\vdash\forall x\colon\forall y\colon x=y\cond s(x)=s(y)\]
für jeden Term $s$. Dies bestätigt sich unschwer folgendermaßen.
Sei $h$ eine frische Hilfsvariable, die nicht
frei in $s$ vorkommt und
\[A:\bicond (s[u:=x]=s[u:=h]),\]
wobei $s(x)=s[u:=x]$ und $s(y)=s[u:=y]$ ist. Der Schluss
\[\dfrac{\Gamma\vdash x=y\quad\vdash A[h:=x]}{\Gamma\vdash A[h:=y]}\]
vereinfacht sich nun zu
\[\dfrac{\Gamma\vdash x=y\quad\overline{\vdash s[u:=x]=s[u:=x]}}{\Gamma\vdash s[u:=x]=s[u:=y]},
\;\;\text{kurz}\;\;\dfrac{\Gamma\vdash x=y\quad\overline{\vdash s(x)=s(x)}}{\Gamma\vdash s(x)=s(y)}.\]
Es genügt übrigens, das Ersetzungsaxiom für atomare Aussageformen zu fordern.
Seien hierzu $P,Q$ Prädikate. Sei $A(x)$ zum Beispiel die Formel $P(x)\land Q(x)$.
Dann ist die Regel
\[\dfrac{\Gamma\vdash x=y\qquad \Gamma'\vdash A(x)}{\Gamma,\Gamma'\vdash A(y)}\]
zulässig, denn:
\[
\infer{\Gamma,\Gamma'\vdash P(y)\land Q(y)}{
  \infer{\Gamma,\Gamma'\vdash P(y)}{
    \Gamma\vdash x=y
  & \infer{\Gamma'\vdash P(x)}{
      \Gamma'\vdash P(x)\land Q(x)}}
& \infer{\Gamma,\Gamma'\vdash Q(y)}{
    \Gamma\vdash x=y
  & \infer{\Gamma'\vdash Q(x)}{
      \Gamma'\vdash P(x)\land Q(x)}}}
\]
Für die anderen Junktoren klappt es analog. Per struktureller Induktion
über den Formelaufbau bestätigt sich die Regel daraufhin in allgemeiner
Weise als zulässig.

\subsection{Von der Identität des Ununterscheidbaren}

Dem Gleichheitsbegriff wohnt das \emph{Principium
identitatis indiscernibilium} inne, das \emph{Prinzip der Identität des
Ununterscheidbaren}, englisch \emph{Identity of indiscernibles}.
Es besagt, dass man keine zwei ungleichen Objekte finden kann,
die in allen ihren Eigenschaften übereinstimmen. Man nennt es auch
\emph{Gleichheit nach Leibniz}, weil Wilhelm Gottfried
Leibniz dieses im 27. Kaptiel von
\emph{Nouveaux Essais sur L'entendement humain II}
im Bezug zum Kosmos beschrieb. Am Ende findet man das Wesentliche
nochmals in fasslicher, bildhafter Form,

\begin{quote}
»Ich erinnere mich, dass eine große Prinzessin, die von erhabenem Geist
ist, einmal sagte, als sie in ihrem Garten spazieren ging, dass sie
nicht glaube, dass es zwei vollkommen ähnliche Blätter gebe. Ein
geistreicher Herr, der mit auf dem Spaziergang war, glaubte, dass es
leicht sein würde, solche zu finden; aber obwohl er viel suchte, wurde
er durch seine Augen davon überzeugt, dass man immer einen Unterschied
bemerken könne. Aus diesen bisher vernachlässigten Erwägungen wird
ersichtlich, wie weit man sich in der Philosophie von den natürlichsten
Begriffen entfernt hat und wie weit man von den großen Prinzipien der
wahren Metaphysik entfernt war.«
\end{quote}

\noindent
Formalisierung erfährt das Prinzip durch die Aussage
\[(\forall P\colon P(x)\bicond P(y))\implies x=y.\]
Man sollte bedenken, dass diese Formulierung die Prädikatenlogik
zweiter Stufe erfordert, da hier über Prädikate quantifiziert wird.
Die Umkehrung
\[x=y\implies (\forall P\colon P(x)\bicond P(y)).\]
wird als unbedenklich angesehen, so dass man das Prinzip auch als
Äquivalenz formuliert. Die Umkehrung ist fast trivial aus den
Axiomen ableitbar, weil es sich bereits um eine gewisse Form des
Ersetzungsaxioms handelt. Umgekehrt können wir aus der Äquivalenz die
beiden Axiome zurückgewinnen. Reflexivität besteht offenkundig, weil
$P(x)$ immer äquivalent zu $P(x)$ ist. Und zum Ersetzungsaxiom wurde
bereits ausgeführt, dass es genügt, dieses für Prädikate zu fordern.

Es verbleibt zu untersuchen, ob das Prinzip aus den Axiomen
herleitbar ist. Hierzu wird $P(u):=(x=u)$ als Prädikat gewählt, womit
$x=x$ als äquivalent zu $x=y$ vorausgesetzt wird. Weil $x=x$ gemäß
Reflexivität vorliegt, erhält man wie gewünscht $x=y$.

\subsection{Eindeutige Existenz}

Gelegentlich tritt in der Mathematik eine Aussage der eindeutigen
Existenz auf, dass heißt, eine Aussage, laut der das Objekt mit der
geforderten Eigenschaft existiert und zudem eindeutig festgelegt ist.
Beispielsweise existiert bei einer Funktion zu jedem Argument genau ein
Wert. Weiterhin tritt bei sogenannten universellen Eigenschaften die
Forderung eindeutiger Existenz auf. Sie führen in die Kategorientheorie,
und der Leser mag daraus schließen, auch wenn er noch nie von ihr gehört
hat, dass diese anscheinend die Prädikatenlogik mit Gleichheit als
logisches System enthalten muss. Zumindest brauchen wir sie für die
folgenden Ausführungen.

Formal ist die eindeutige Existenz fassbar als der Quantor
\[(\exists! x\colon A(x)):\bicond
(\exists x\colon A(x)\land\forall y\colon A(y)\cond x=y).\]
Diese Aussage ist in Existenz und Eindeutigkeit zerlegbar gemäß
\begin{Satz}\label{eindeutige-Existenz-separat}
Es gilt die Äquivalenz
\[(\exists! x\colon A(x))\bicond
(\exists x\colon A(x))\land(\forall x,y\colon A(x)\land A(y)\cond x=y).\]
\end{Satz}
\begin{Beweis}
Die linke Seite gelte. Dann liegt ein $u$ mit $A(u)$
und $\forall y\colon A(y)\cond u=y$ vor. Der Existenzaussage wird
mit $x:=u$ genügt. Die Eindeutigkeit geht via
\[(\forall y\colon A(y)\cond u=y), A(x), A(y)\vdash x=y.\]
Wir spezialisieren die gegebene Allaussage einmal mit $y:=x$ und einmal
mit $y:=y$. Mit $A(x)$ und $A(y)$ bekommt man daraufhin $u=x$ und $u=y$. Ergo
folgt $x=y$ per Symmetrie und Transitivität der Gleichheit.

Die rechte Seite gelte. Dann liegt ein $u$ mit $A(u)$ vor. Wir wählen
$x:=u$ für die Existenzaussage. Die Aussage $A(u)$ liegt bereits vor.
Verbleibt $\forall y\colon A(y)\cond u=y$ zu bestätigen. Wir nehmen
also $A(y)$ an. Aus der Spezialisierung der gegebenen Allaussage mit
$x:=u$ und $y:=y$ wird $u=y$ via $A(u)\land A(y)$ abgetrennt.\,\qedsymbol
\end{Beweis}

\noindent
Vorsorglich erwähnen möchte ich
\[(\forall x,y\colon A(x)\land A(y)\cond x=y)\bicond
(\forall x\colon A(x)\cond\forall y\colon A(y)\cond x = y).\]
Der Beweis sollte dem Leser nicht viel Mühe bereiten.

\newpage
\section{Induktion}

\subsection{Einfache Induktion}

In der Philosophie bezeichnet man als \emph{Induktion} eine Art von
Schlussfolgerung, die da ist der Schluss vom Speziellen auf das
Allgemeine. Folgendes Beispiel verdeutlicht die Idee der Überlegung. Ein
Gegenstand wird einmal fallen gelassen, man beobachtet wie dieser zu
Boden fällt. Wiederholung des Experiments führt abermals zum selben
Resultat. Induktiv schließt man daraus, dass dieses Resultat \emph{immer}
eintreten wird. Jedoch kann Induktion zu Fehlschlüssen führen, weshalb
es nicht als mathematisches Beweisverfahren brauchbar ist. Nur weil
bereits dreimal eine Toastbrotscheibe auf die Marmeladenseite gefallen
ist, heißt das nicht, dass dieses Resultat immer eintreten müsse. In der
Mathematik bieten die Borwein"=Integrale ein prägnantes Beispiel, wo
leichtfertige Argumentation verfänglich wäre.

Mit der bedenklichen Induktion in der Philosophie teilt sich die
\emph{vollständige Induktion} den Namen. Sie ist allerdings unfehlbar.
Das Verfahren ist für die moderne Mathematik und Informatik von
wesentlicher Bedeutung.

Die vollständige Induktion wird vermittelt durch das Axiomenschema
\[\vdash A(0)\land (\forall n\in\N\colon A(n)\cond A(n+1))
\cond (\forall n\in\N\colon A(n)).\]
Es induziert die Regel
\[\dfrac{\Gamma\vdash A(0)\qquad\Gamma',n\in\N,A(n)\vdash A(n+1)}{
\Gamma,\Gamma'\vdash\forall n\in\N\colon A(n)}.\]
Zur Veranschaulichung wird gerne eine endlose Dominoreihe herangezogen.
Fällt der erste Dominostein um, und ist sicher, dass mit einem
Dominostein ebenso dessen Nachfolger umfällt, muss \emph{jeder}
Dominostein irgendwann umfallen.

Man bezeichnet $A(0)$ als den \emph{Induktionsanfang}\index{Induktionsanfang}.
Die Ableitung von $A(n+1)$ aus $A(n)$ heißt \emph{Induktionsschritt}\index{Induktionsschritt},
wobei $A(n)$ darin die Bezeichnung \emph{Induktionsvoraussetzung}\index{Induktionsvoraussetzung} trägt.

Ein erstes Beispiel. Man definiert die Potenz einer Zahl $a$ rekursiv als
\[a^0 := 1,\qquad a^{n+1} := aa^n.\]
Zu beweisen sei das Potenzgesetz
\[A(n)\defiff (ab)^n = a^n b^n.\]
Der Anfang $A(0)$ bestätigt sich via
\[(ab)^0 = 1 = 1\cdot 1 = a^0 b^0.\]
Den Induktionsschritt $(A(n)\cond A(n+1))$ bestätigt
die Rechnung
\[(ab)^{n+1}\stackrel{\text{(1)}}= (ab)(ab)^n
\stackrel{\text{IV}}= aba^n b^n = aa^n bb^n
\stackrel{\text{(2)}}= a^{n+1} b^{n+1}.\]
Die Stelle, wo $A(n)$ zur Anwendung kam, wurde mit IV annotiert,
was für \emph{Induktionsvoraussetzung} steht. Die Umformungen
(1), (2) gelten laut Definition.

Bislang trat die Induktion so auf, dass der Anfang in der
Zahl Null liegt. Aus der Regel folgt aber bereits, dass man den
Anfang in jede beliebige natürliche Zahl legen kann.
\begin{Satz}
Es gilt für $n,n_0\in\N$ das allgemeine Schema
\[\vdash A(n_0)\land (\forall n\ge n_0\colon A(n)\cond A(n+1))
\cond (\forall n\ge n_0\colon A(n)).\]
\end{Satz}
\begin{Beweis}
Die Prämisse wird spezialisiert mit $n:=u+n_0$, wobei $u$ beliebig sein
darf, da $u+n_0\ge n_0$ für jedes $u\in\N$
erfüllt ist. Es sei nun $B(u):\bicond A(u+n_0)$. Demnach ist
$A(n_0)$ äquivalent zu $B(0)$. Und es ist $A(u+n_0+1)$ äquivalent
zu $B(u+1)$. Insgesamt erhält man aus der Prämisse also
\[B(0)\land (\forall u\colon B(u)\cond B(u+1)).\]
Per herkömmlicher Induktion gilt also $B(u)$ bzw. $A(u+n_0)$ für
jedes natürliche $u$. Wird dies nun mit der Resubstitution $u:=n-n_0$
mit $n\ge n_0$ spezialisiert, erhält man $A(n)$, und somit schließlich die
gesuchte Konklusion $(\forall n\ge n_0\colon A(n))$.\,\qedsymbol
\end{Beweis}

\noindent
Spezialisieren ist hier formal zu verstehen. Das Resultat
der Substitution ist eigentlich nicht weniger allgemein als die
ursprüngliche Aussage.

Befasst man sich im Weiteren mit der Mengenlehre, wo gemeinhin mit
Mengen argumentiert wird, bietet sich an, auch die Induktion bezüglich
einer Menge zu formulieren. Sei hierzu $M\subseteq\N$ definiert als
die Aussonderung
\[M := \{n\in\N\mid A(n)\}.\]
Mit $A(n)\bicond n\in M$ nimmt das Schema der Induktion daraufhin die Form
\[\vdash 0\in M\land (\forall n\in\N\colon n\in M\cond n+1\in M)\cond M = \N\]
an. Man verwendet diese Variante vorwiegend, um die axiomatische
Charakterisierung der natürlichen Zahlen in Bezug zur Mengenlehre zu
bringen. Sie wurde Ende des 19. Jahrhunderts von Richard Dedekind und
Giuseppe Peano erdacht. Das Axiom der Induktion ist darin als letzte
und komplizierteste. Es lautet
\[\vdash \forall P\colon P(0)\land (\forall n\colon P(n)\cond P(S(n)))
\cond (\forall n\colon P(n)).\]
Hiermit wird allerdings die Prädikatenlogik zweiter Stufe vorausgesetzt,
weil über alle Prädikate $P$ quantifiziert wird. Ein Verzicht auf die
Logik zweiter Stufe ist aber möglich, indem das Axiom wie bisher als
Schema formuliert wird. Das so in die Prädikatenlogik erster Stufe gebrachte
System nennt man die \emph{Peano"=Arithmetik}.

\subsection{Starke Induktion}

Die starke Induktion verstärkt die Induktionsvoraussetzung
dahingehend, dass sie nicht nur für den unmittelbaren Vorgänger, sondern
für sämtliche Vorgänger vorausgesetzt werden darf. Damit wird der
Induktionsbeweis unter Umständen erleichtert. Die starke muss nicht
extra axiomatisch gefordert werden, sie ist aus der herkömmlichen
ableitbar.
\begin{Satz}[Starke Induktion]
Es gilt das Schema
\[\vdash A(0)\land (\forall n\in\N\colon (\forall k\le n\colon A(k))\cond A(n+1))
\cond (\forall n\in\N\colon A(n))\]
\end{Satz}
\begin{Beweis}
Sei hierzu
\[C(n) :\bicond (\forall k\le n\colon A(k)).\]
Die Prämissen seien angenommen. Es ist $A(0)$
gleichbedeutend mit $C(0)$. Wir überzeugen uns nun von der Folgerung
\[(\forall n\in\N\colon C(n)\cond A(n+1))\cond (\forall n\in \N\colon C(n)\cond C(n+1)).\]
Sei also $n\in\N$ und $C(n)$ angenommen, dann haben wir auch $A(n+1)$
und somit $C(n)\land A(n+1)$, was äquivalent zu $C(n+1)$ ist. Per
herkömmlicher Induktion gilt also $C(n)$ für jedes natürliche
$n$. Aus $C(n)$ folgt aber $A(n)$, womit erst recht $A(n)$ für jedes
natürliche $n$ gilt.\,\qedsymbol
\end{Beweis}

\noindent
Ein wenig eleganter formuliert sich das Schema auch in der Form
\[\vdash (\forall n\in\N\colon (\forall k < n\colon A(k))\cond A(n))
\cond (\forall n\in\N\colon A(n)).\]
Weil keine natürliche Zahl kleiner als null existiert, greift hier
das Prinzip der leeren Wahrheit, womit $A(0)$ trotzdem zu bestätigen
ist.

\subsection{Strukturelle Induktion}

Die herkömmliche Induktion verläuft über die natürlichen Zahlen.
Sie beginnt ohne Beschränkung der Allgemeinheit in der Null, und setzt
sich dann schrittweise auf den Nachfolger fort. Es wird also $A(0)$
bestätigt, und $A(n+1)$ unter Annahme von $A(n)$.

Strukturelle Induktion verläuft allgemeiner über Knoten. Es gibt ein
oder mehrere Anfangsknoten $v_0$, für die jeweils der Induktionsanfang $A(v_0)$
zu bestätigen ist. Außerdem liegen Regeln vor, wie aus bereits vorhandenen
Knoten neue Knoten abzuleiten sind. Die neuen Knoten nehmen die Rolle der
Nachfolger ein. So entsteht ein Baum oder ein gerichteter Graph.
Leitet sich $v$ aus $v_1$ bis $v_n$ ab, so besteht der zugehörige
Induktionsschritt darin, dass $A(v)$ unter Annahme von $A(v_1)$
bis $A(v_n)$ gezeigt wird. Bei einem Baum verläuft die Induktion von
den Blättern aus zu einer Wurzel hin. Die gewöhnliche Induktion verlief
von der Null zu einer Zahl hin. Wurde alles gezeigt, ist die Induktion
also \emph{vollständig}, darf die Wurzel wie die Zahl beliebig sein.

Ich will versuchen, das Prinzip so mit Dominosteinen zu veranschaulichen,
wie die herkömmliche Induktion als Fallen einer Dominoreihe verständlich
wird. Man kann sich eine Regel als ein Plättchen bestimmter Art vorstellen.
Sofern alle notwendigen Dominosteine das Plättchen erreichen, fällt es
um, womit auch der auf das Plättchen folgende Dominostein umfällt.
Abermals erreichen notwendige Steine, worunter auch oder nur Nachfolger
zu finden sind, ein weiteres Plättchen, worauf auch dieses fällt.
Das läuft zumindest solange, bis man das gewünschte Fallen des
begehrten Wurzelsteins beobachtet. Als dynamischer Prozess gesehen,
hat man hier eine kausale Struktur, bei der Wirkungen nur unter einer
oder mehreren passenden Ursachen auftreten.

Die herkömmliche Induktion stellt einen Spezialfall der strukturellen
Induktion dar, bei der die Bäume endliche Listen sind. Bei
der strukturellen Induktion über den Formelaufbau verläuft die Induktion
über Formeln. Sie beginnt in den atomaren Formeln. Die Regeln zur
Ableitung sind die Produktionsregeln. Bei der strukturellen Induktion
über die Konstruktion eines Beweises verläuft die Induktion über
Sequenzen. Sie beginnt in den Grundsequenzen. Die Regeln zur Ableitung
sind die Schlussregeln.

Ein weiteres Beispiel bietet die Induktion über die Punkte des
Gitters $\Z_{\ge 0}\times Z_{\ge 0}$. Sagen wir, der einzige
Anfang ist $A(0,0)$. Die erste Schrittweise bestehe in $A(m+1,n)$
unter Annahme $A(m,n)$. Die zweite Schrittweise bestehe in $A(m,n+1)$
unter Annahme $A(m,n)$. Diese Art von struktureller Induktion ist
gleichwohl gegen die herkömmliche ersetzbar. Der Beweis untergliedert
sich dabei in zwei Induktionsbeweise. Es wird zunächst
$A(m,0)$ für jedes $m$ bestätigt und $A(m,0)$ anschließend als
Anfang für $A(m,n)$ benutzt. Der zweite Induktionsbeweis ist so gesehen
durch $m$ parametrisiert, was nichts Schlimmes ist, denn
parametrisierte Argumentation kennzeichnet ja das übliche Vorgehen
zur Bestätigung allquantifizierter Aussagen.

\newpage
\section{Modallogik}

\subsection{Das System K}

Die Modallogik handelt von den Weisen, wie eine Aussage ausgeprägt sein
kann, was durch modalisierende Operatoren ausgedrückt wird. Es gibt
unterschiedliche Systeme der Modallogik. In vielen Systemen finden sich
zwei Modalitäten, die \emph{Notwendigkeit} der Aussage und die
\emph{Möglichkeit} der Aussage. Allerdings artikulieren diese Sprechweisen
nur die \emph{alethische} Deutung der Modalitäten. Je nach System und
Anwendung sind unterschiedliche Deutungen der Modalitäten zuträglich.

Ist $A$ die Aussage »Es regnet«, drückt $\lnec A$ die Aussage
»Es regnet notwendigerweise« und $\lpos A$ die Aussage
»Es regnet möglicherweise« aus.

Die Modallogik scheint wichtiger für Philosophen als für
Mathematiker. Indessen kamen mit der Zeit Anwendungen in der
Mathematik und der Informatik zum Vorschein. Dem Basiswissen besonders
dienlich ist meines Erachtens die \emph{dynamische Logik} mit
ihrer engen Beziehung zum Hoare"=Kalkül bzw. zum dijkstraschen wlp"=Kalkül.
Diese Kalküle geben uns die Mittel in die Hand, Algorithmen auf ihre Korrektheit
hin zu untersuchen. Die Funktion, die der Algorithmus darstellt, bekommt
hierzu eine \emph{Spezifikation}, bestehend aus einer \emph{Vorbedingung}
und einer \emph{Nachbedingung}. Man zeigt nun, dass der Algorithmus auch
das tut, was er soll, dergestalt dass der Wert der Funktion immer die
Nachbedingung erfüllt, sofern ihre Argumente die Vorbedingung erfüllen.

Der Modallogik kommt somit auch eine gewisse Bedeutung für die praktische
Arbeit zu, nicht nur für höhere mathematische respektive philosophische
Erwägungen und Betrachtungen.

Wir wollen uns zunächst mit dem \emph{System K} beschäftigen, dem
Grundsystem der \emph{normalen Modallogiken}. Spezifischere Systeme
entstehen durch Hinzunahme weiterer Axiome. Das System K enthält
sämtliche Regeln und Axiome der klassischen Aussagenlogik. Hinzu kommt
die Regel
\[\dfrac{\vdash A}{\vdash\lnec A}\qquad\text{(Nezessisierungsregel)}\]
und das Schema
\[\vdash\lnec(A\cond B)\cond (\lnec A\cond\lnec B).\qquad\text{(Axiomenschema K)}\]
Wie gehabt induziert das Schema sogleich eine zulässige Regel,
\[\dfrac{\Gamma\vdash\lnec(A\cond B)}{\Gamma\vdash\lnec A\cond\lnec B}.\qquad\text{(Regel K)}\]
Wichtig ist, dass nur Theoreme Nezessisierung erfahren dürfen.
Dagegen ist
\[\dfrac{a\vdash a}{a\vdash\lnec a}\qquad \text{(verboten)}\]
ein unzulässiger Schluss. So ist $a\cond\lnec a$ kein Theorem. Man wird
mit der Semantik für das System K leicht ein Gegenmodell dieser Formel
finden.

Man definiert $\lpos A$ als äquivalent zu $\lnot\lnec\lnot A$.

Statt der einfachen Nezessisierung und Schema K kann man auch eine
einzige allgemeine Nezessisierungsregel voraussetzen. Mit $n\ge 0$
lautet sie
\[\dfrac{A_1,\ldots,A_n\vdash B}{\lnec A_1,\ldots,\lnec A_n\vdash\lnec B}.\]
Man beweist die Regel per Induktion über $n$ als zulässig. Im Anfang $n=0$
nimmt sie schlicht die Form der Nezessisierungsregel an. Der
Induktionsschritt wird durch den Beweisbaum
\[
\infer[\infernote{Modus ponens}]{\lnec A_1,\ldots,\lnec A_n,\lnec A_{n+1}\vdash\lnec B}{
  \infer[\infernote{K}]{\lnec A_1,\ldots,\lnec A_n\vdash\lnec A_{n+1}\cond\lnec B}{
    \infer[\infernote{IV}]{\lnec A_1,\ldots,\lnec A_n\vdash\lnec (A_{n+1}\cond B)}{
      \infer{A_1,\ldots,A_n\vdash A_{n+1}\cond B}{
        A_1,\ldots,A_n,A_{n+1}\vdash B}}}
& \infer{\lnec A_{n+1}\vdash\lnec A_{n+1}}{}}
\]
bestätigt.

Auch dem Fitch"=Style wurde eine Darstellung der Schlüsse der Modallogik
hinzugefügt. Sie ist meines Erachtens etwas schwieriger zu durchschauen
als das Schließen von Sequenzen. Ich will in diesem Buch nicht näher
darauf eingehen.

\subsection{Das System S4}

\begin{table}
\begin{center}
\caption{Übersicht über zusätzliche Schemata}
\label{tab:Modallogik-Schemata}
\begin{tabular}{clll}
\toprule
& \strong{Schema} & \strong{Relationen} & \strong{Formel}\\
\midrule[\heavyrulewidth]
K & $\lnec (A\cond B)\cond(\lnec A\cond\lnec B)$ & sämtliche & keine Einschränkung\\
T & $\lnec A\cond A$ & reflexive & $\forall x\colon R_{xx}$\\
B & $A\cond \lnec\lpos A$ & symmetrische & $\forall x,y\colon R_{xy}\cond R_{yx}$\\
D & $\lnec A\cond\lpos A$ & serielle & $\forall x\colon\exists y\colon R_{xy}$\\
4 & $\lnec A\cond\lnec\lnec A$ & transitive & $\forall x,y,z\colon R_{xy}\land R_{yz}\cond R_{xz}$\\
5 & $\lpos A\cond\lnec\lpos A$ & euklidische & $\forall x,y,z\colon R_{xy}\land R_{xz}\cond R_{yz}$\\
\bottomrule
\end{tabular}
\end{center}
\end{table}
Das System S4 formt sich aus den Schemata KT4, siehe Tabelle
\ref{tab:Modallogik-Schemata}.

Die Übersetzung nach Gödel-McKinsey-Tarski ist
\begin{align*}
v' &= \lnec v, & (A\land B)' &= A'\land B', & (A\cond B)' &= \lnec (A'\cond B'),\\
\bot' &= \bot, & (A\lor B)' &= A'\lor B', & (\lnot A)' &= \lnec\lnot A'.
\end{align*}
Man deutet $\lnec A$ als »A ist beweisbar«. Es ist $A$ genau
dann ein Theorem der intuitionistischen Logik, wenn $A'$ ein
Theorem im System S4 ist.

\section{Beweistheoretische Überlegungen}

\subsection{Ableitbarkeit}

Auf das Urteil $\Gamma\vdash A$ blickt man aus zwei Sichtweisen. Zum
einen ein stellt es sich als syntaktisches Konstrukt dar, das Gegenstand
eines formalen Systems ist. Zum anderen stellt es sich als die
metasprachliche Aussage dar, dass $A$ aus $\Gamma$ ableitbar ist. Zur
Schaffung von Klarheit wollen wir bei beweistheoretischen Betrachtungen
näher zwischen den beiden Sichtweisen unterscheiden. Dazu notieren
wir $\Gamma\succ A$ für die Sequenz als syntaktisches Konstrukt,
und im Unterschied dazu $\Gamma\vdash A$ für die metasprachliche Aussage.
\begin{Definition}[Ableitbarkeit]\newlinefirst
Man nennt $A$ aus der Formelmenge $\Gamma$ ableitbar, kurz
$\Gamma\vdash A$, wenn ein endliche Menge $\Gamma_0\subseteq\Gamma$
existiert, so dass die Sequenz $\Gamma_0\succ A$ ableitbar ist.
\end{Definition}
Zu einer endlichen Formelmenge $\Gamma_0$ gilt $\Gamma_0\vdash A$
demzufolge genau dann, wenn die Sequenz $\Gamma_0\succ A$ ableitbar ist,
symbolisch%
\[(\Gamma_0\vdash A) \bicond (\vdash \Gamma_0\succ A).\]
Die Endlichkeit ist genau deshalb erfordlich, weil Sequenzen nur für
endliche Kontexte erklärt sind. Für die Ableitbarkeit besteht damit
verbunden Kompaktheit, dergestalt dass $\Gamma\vdash A$ genau dann gilt,
wenn ein endliches $\Gamma_0\subseteq\Gamma$ mit $\Gamma_0\vdash A$
existiert.

Weiterhin sind die Schlussregeln nun als metasprachliche Aussagen
fassbar. So ist die Einführung der Konjunktion beschrieben durch%
\[(\Gamma\vdash A)\land (\Gamma'\vdash B)\cond (\Gamma\cup\Gamma'\vdash A\land B).\]
Anders als in der Regel dürfen $\Gamma,\Gamma'$ hierbei auch
unendlich sein.

Vermittels der Einführung von Grundsequenzen, der Abschwächungsregel
und des Modus ponens gelangt man zu den drei Einsichten%
\begin{align*}
&A\in\Gamma\cond (\Gamma\vdash A), &&\text{(Extensivität)}\\
&\Gamma\subseteq\Gamma'\land (\Gamma\vdash A)\cond (\Gamma'\vdash A), &&\text{(Monotonie)}\\
&(\Gamma\cup\{A\}\vdash B)\land (\Gamma\vdash A)\cond (\Gamma\vdash B). &&\text{(Schnittregel)}
\end{align*}
Eine Darstellung aus der Sichtweise der Mengenlehre fördert die Einordnung
dieser Sachverhalte. Als Hilfsmittel dient hierbei der
\emph{tarskische Konsequenzoperator}%
\[\Cn(\Gamma) := \{A\mid \Gamma\vdash A\},\]
der $\Gamma$ die Menge aller Formeln zuordnet,
die aus $\Gamma$ ableitbar sind. Man spricht auch vom
\emph{Inferenzoperator} oder vom \emph{deduktiven Abschluss}.
Dieser genügt den drei Axiomen
\begin{align*}
&\Gamma\subseteq\Cn(\Gamma), &&\text{(Extensivität)}\\
&\Gamma\subseteq\Gamma'\cond \Cn(\Gamma)\subseteq\Cn(\Gamma'), &&\text{(Monotonie)}\\
&\Cn(\Cn(\Gamma)) \subseteq \Cn(\Gamma). &&\text{(Abgeschlossenheit)}
\end{align*}
Das heißt, es handelt sich bei $\Cn$ um einen Hüllenoperator. Aus der
Anfügung der Monotonie an die Extensivität folgt auch die Umkehrung
der Abgeschlossenheit. Somit gilt mehr noch die Idempotenz
\[\Cn(\Cn(\Gamma)) = \Cn(\Gamma).\]
Die vormals
vorhandene Schnittregel leitet sich aus den drei Axiomen ab.
Sie wird zunächst in die Form%
\[A\in\Cn(\Gamma)\cond\Cn(\Gamma\cup\{A\})\subseteq\Cn(\Gamma)\]
gebracht. Es gilt $\Gamma\cup\{A\}\subseteq\Cn(\Gamma)$, denn
$\Gamma\subseteq\Cn(\Gamma)$ gilt gemäß Extensivität und
$\{A\}\subseteq\Cn(\Gamma)$ gilt gemäß der Prämisse.
Laut Monotonie und Idempotenz folgt%
\[\Cn(\Gamma\cup\{A\})\subseteq\Cn(\Cn(\Gamma)) = \Cn(\Gamma).\]
Obgleich die Abgeschlossenheit intuitiv klar erscheint, ist sie
umgekehrt aus der Schnittregel zu gewinnen. Prämisse sei also
$A\in\Cn(\Cn(\Gamma)$, womit $\Cn(\Gamma)\vdash A$. Das heißt aber,
es existiert eine endliche Teilmenge
\[\{A_1,\ldots,A_n\}\subseteq\Cn(\Gamma)\;\text{mit}\;
\{A_1,\ldots,A_n\}\vdash A.\]
Mithin gilt $\Gamma\vdash A_k$ zu jedem $k$.
Nun unternimmt man der Reihe nach den Schnitt der Form
\[\dfrac{\Gamma\cup\{A_1,\ldots,A_k\}\vdash A\qquad\Gamma\vdash A_k}
{\Gamma\cup\{A_1,\ldots,A_{k-1}\}\vdash A}\]
zu den $k$ von $k=n$ bis $k=1$. Man gelangt damit schließlich zu
$\Gamma\vdash A$, also zum gewünschten $A\in\Cn(\Gamma)$.
