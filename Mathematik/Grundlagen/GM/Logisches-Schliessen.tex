
\chapter{Logisches Schließen}

\section{Auf dem Weg zur formalen Logik}

\subsection{Argumente}

Die Logik befasst sich damit, nach welchen Prinzipien stimmiges Denken
abläuft, wobei an erster Stelle steht, wie geklärt wird, ob ein
Gedankengang von Irrtümern befreit ist. Eine umfassende Antwort darauf
besteht zumindest für die Art von Schlussfolgerungen, die in den exakten
Wissenschaften geführt werden. Hierbei hat es sich als förderlich
herausgestellt, künstliche symbolische Sprachen festzulegen und
Schlussfolgerungen daraufhin in mechanischer Weise nach haargenauen
Regeln zu führen. Man spricht von einem \emph{formalen System}, wobei
die künstliche Sprache, mit der dieses arbeitet, als \emph{formale Sprache}
bezeichnet wird. Daraus ging die formale Logik, speziell die
mathematische Logik hervor. Wir streben ebenfalls an, diesen Pfad zu
verfolgen, weil er uns ein großes Maß an Klarheit und Sicherheit bietet.
Besonders wichtig finde ich diesbezüglich die Reduzierung der kognitiven
Belastung. Durch die Mechanisierung müssen wir uns nämlich nicht mehr
damit befassen, ob die gemachten Überlegungen möglicherweise unbedachten
Irrtümern unterliegen oder mit Flüchtigkeitsfehlern behaftet sind.

Statt die Logik direkt einer vollständigen Formalisierung zu unterziehen,
wollen wir uns dem aus einem allgemeinen Bezugspunkt heraus etappenweise
nähern. Auf diese Weise bekommt man ein besseres Gefühl dafür, wie die
formale im Bezug zur traditionellen Logik steht.

Zunächst besteht der wesentliche Ansatz zur Klärung des Denkens darin,
den Gedankengang in Schritte aufzuteilen, die \emph{Argumente} heißen.
Man mag dies als Durchführung einer Analyse sehen, bei der ein Gegenstand,
um diesen verstehen zu können, in Bestandteile zerlegt wird. Zum Beispiel
gelingt es unter Umständen, ein Problem in Teilprobleme zu zerlegen,
oder ein Programm in Prozeduren.

Ein Argument artikuliert die Ableitung einer \emph{Konklusion} aus
einer oder mehreren \emph{Prämissen}. Dieser Begriff von Argument
unterscheidet sich vom umgangssprachlichen durch seine starre Form, bleibt
zu diesem aber zumindest in einem gewissen Bezug. Die Notwendigkeit für
die Verengung des Begriffs besteht darin, dass umgangssprachliche Argumente
zu komplex werden mögen, als dass sie noch vernünftig analysierbar bleiben.

Man mag ein Argument mit zwei Prämissen in einer der Schreibweisen
\[\begin{array}{@{}l@{}}
\text{Prämisse 1}\\
\text{Prämisse 2}\\
\midrule[\inferrulewidth]
\therefore\text{Konklusion}
\end{array}
\qquad\quad
\begin{array}{@{}l@{}}
\text{Prämisse 1}\\
\text{Prämisse 2}\\
\midrule[\inferrulewidth]
\text{Konklusion}
\end{array}
\qquad\quad
\begin{array}{@{}l@{}}
\text{Prämisse 1}\\
\text{Prämisse 2}\\
\therefore\text{Konklusion}
\end{array}
\]
notieren. Das Zeichen $\therefore$ wird hierbei als \emph{also} oder
\emph{folglich} gelesen. Die zweite und dritte Schreibweise darf man
als nachlässige Form der ersten sehen.

Ein Beispiel für ein Argument:
\[\begin{array}{@{}l@{}}
\text{Alice oder Bob entwendete gestern Kuchen aus dem Kühlschrank.}\\
\text{Bob war gestern den gesamten Tag bei jemand anderem.}\\
\midrule[\inferrulewidth]
\text{Also entwendete Alice gestern Kuchen aus dem Kühlschrank.}
\end{array}\]
Die Konklusion und jede der Prämissen muss ein wahrheitswertfähiges
sprachliches Gebilde sein, das wir \emph{Aussage} oder \emph{Satz}
nennen, wobei ich die zweite Bezeichnung vermeiden möchte, um nicht
mit den Begrifflichkeiten der Linguistik durcheinander zu kommen. Es
spielt hierbei keine Rolle, ob der Wahrheitswert der Aussage jemandem
bekannt oder auf irgendeine Weise ermittelbar ist.
Außerdem hält uns nichts davon ab, unterschiedliche fiktive Wirklichkeiten
zu betrachten. In der einen mag die Aussage wahr sein, während sie in der
anderen falsch ist. Keine Aussage wäre die Frage »Wie spät ist es?«, oder
»Pfui!«, -- ein Satz, der aus einer einzigen Interjektion
besteht.

\subsection{Arten von Argumenten}

Unterschieden wird zwischen \emph{deduktiven} und \emph{nichtdeduktiven}
Argumenten.

Bei einem \emph{deduktiven} Argument stellt die Konklusion eine logische
Konsequenz der Prämissen dar. Damit ist gemeint, dass die Wahrheit der
Konklusion zwangsläufig aus der Wahrheit der Prämissen folgt. Hier liegt
die Information, zu der man gelangen möchte, bereits in den Prämissen
verborgen.

Bei einem \emph{nichtdeduktiven} Argument erscheint die Konklusion
plausibel, folgt aber nicht mehr zwangsläufig aus den Prämissen. Dies
wird zum zum Beispiel zum Ausdruck gebracht, indem gesagt wird, die
Konklusion sei vermutlich, wahrscheinlich oder höchstwahrscheinlich wahr.
Innerhalb der nichtdeduktiven Argumente gibt es noch feinere Abgrenzungen,
unter Anderem von induktiven. Ein \emph{induktives} Argument stellt
Schließen vom Speziellen aufs Allgemeine dar. Zum Beispiel mag man
aus dem Umstand heraus, bislang nur weiße Schwäne beobachtet zu haben,
die Existenz schwarzer Schwäne ausschließen.

Gegenstand aller weiteren Studien werden ausschließlich deduktive
Argumente sein. Zur Klärung nichtdeduktiver Argumente mag es förderlich
sein, zunächst Grundbegriffe der Wahrscheinlichkeitsrechnung
zu erläutern, was auf später verschoben werden muss.

\subsection{Gültigkeit}

Ein deduktives Argument wird als \emph{gültig} angesehen, wenn die
Konklusion inhaltlich aus den Prämissen folgt. Dies bedeutet, dass es
unmöglich ist, aus wahren Prämissen eine falsche Konklusion zu ziehen.
Der Blick auf die Gültigkeit hilft uns, fehlerhafte Argumente zu
identifizieren und auszuschließen.

Ein deduktives Argument heißt \emph{korrekt}, wenn es gültig ist und
zudem sämtliche Prämissen wahr sind. Entsprechend nennt man eine
Argumentation korrekt, wenn alle ihre Argumente gültig und alle
ursprünglichen Prämissen wahr sind. Eine korrekte Argumentation setzt
sich somit sukzessive aus korrekten Argumenten zusammen.

Einige Argumente sind ihrer \emph{Form} nach gültig. Dies bedeutet,
dass das Argument unabhängig davon gültig ist, was die Aussagen
inhaltlich bedeuten. Häufig treten Argumente wie
\[\begin{array}{@{}l@{}}
\text{Wenn es regnet, dann wird die Straße nass.}\\
\text{Es regnet.}\\
\midrule[\inferrulewidth]
\text{Also wird die Straße nass.}
\end{array}\]
auf. Diese Argumente sind von der Form des \emph{Modus ponens}:
\[\begin{array}{@{}l@{}}
\text{Wenn $A$, dann $B$}\\
\text{$A$}\\
\midrule[\inferrulewidth]
\therefore\text{$B$}
\end{array}\]
Hierbei stehen $A,B$ für Platzhalter, für die beliebige Aussagen
eingesetzt werden dürfen. Zu jeder Setzung ergibt sich ein gültiges
Argument. Hierbei spielt es keine Rolle, welche inhaltliche Bedeutung
$A,B$ zugemessen wird.

Anders verhält es sich mit diesem Argument:
\[\begin{array}{@{}l@{}}
\text{Draußen liegt seit gestern Schnee.}\\
\midrule[\inferrulewidth]
\text{Folglich ist es draußen kalt.}
\end{array}\]
Um es als gültig zu erkennen, muss die Bedeutung der Prämisse und der
Konklusion inhaltlich verstanden werden. Zwar ist es gültig,
aber nicht der Form nach.

Darf man solchen Argumenten zweifellos vertrauen? Wenn man Logik
einsetzt, um Aussagen über Sachverhalte in der Wirklichkeit zu machen,
muss man eigentlich zwangsläufig, auch wenn dies wortlos passiert,
ein vereinfachtes Modell der Wirklichkeit aufstellen oder sich auf
einen Teilbereich ihrer beschränken, in dem Argumente als allgemeingültig
befunden werden können. Es kann ja sein, dass draußen aufgrund eines
Unwetters zwei Meter Schnee liegen, die bei einer milden Temperatur
stetig am schmelzen sind. Wie wird kalt überhaupt von mild abgegrenzt?
Gehört eine überdachte Terrasse zu draußen? In welchem Umkreis endet
draußen?

Die Modellierung der Wirklichkeit scheint mit Schwierigkeiten
verbunden. Es lässt sich mutmaßen, dass sich in der Ferne bereits
philosophische Untiefen auftun. Daher werden wir uns aus der Affäre
ziehen, indem wir uns zunächst auf die Untersuchung mathematischer
Strukturen beschränken. Dies soll jedoch nicht den Eindruck einer
Umgehung erwecken, denn zur Entwicklung guter Modelle benötigt man nicht
nur Logik, sondern auch mathematische Mittel.

\subsection{Aussagenlogik}

Natürliche Sprachen schaffen eine große Ausdrucksfähigkeit. Dafür ist
ihre Struktur aber auch komplex, -- bezogen sowohl auf die Grammatik als
auch auf die inhaltliche Bedeutung von Sätzen.  Wir definieren daher
im Fortgang eine einfache künstliche Sprache, die \emph{aussagenlogische}
Sprache. In ihr kommt, anders als in der natürlichen, jedem Satz ein
Wahrheitswert zu. Daraufhin können wir die Schemata von Argumenten klar
identifizieren, die ihrer Form nach gültig sind. Im Weiteren werden
wir diese Sprache abschließend zur prädikatenlogischen erweitern. Es stellt
sich heraus, dass sie, obwohl recht klein, zur Darstellung sämtlicher
mathematischer Aussagen und ihrer Beweise genügt.

Mit $p,q,r$ bezeichnen wir eine Reihe von atomaren Aussagen, die so
heißen, weil sie im Rahmen der Aussagenlogik nicht weiter zerlegbar
sind. Vermittels Junktoren lassen sich aus diesen zusammengesetzte
Aussagen formen. Es stehe zum Beispiel $p$ für »Es regnet« und $q$ für
»Die Straße wird nass«. Dann soll $p\cond q$ bedeuten, »Wenn es regnet,
wird die Straße wird nass«. Weiterhin notieren wir $p\land q$ für
»Es regnet und die Straße wird nass«. Entsprechend steht
$p\land q\cond p$, was als $(p\land q)\cond q$ gelesen wird, für
»Wenn es regnet und die Straße nass wird, dann regnet es«.
Mit den Junktoren lassen sich aus Aussagen nach belieben sukzessive
immer kompliziertere Zusammensetzungen formen. Damit erhält man
allerdings bereits alle aussagenlogischen Formeln, die möglich sind.

Der Modus ponens ist nun in folgender verkürzter Form darstellbar.
Sind $P,Q$ beliebige atomare Aussagen, gilt das Schema:
\[\begin{array}{@{}l@{}}
P\cond Q\\
P\\
\midrule[\inferrulewidth]
\therefore Q
\end{array}\]
Etwa erhält man für die Setzung $P:=p$ und $Q:=q$ ein gültiges
Argument. Der Modus ponens lässt sich aber noch allgemeiner fassen.
Stehen nun $A,B$ für beliebige Aussagen, atomar oder zusammengesetzt,
gilt er in der Form:
\[\begin{array}{@{}l@{}}
A\cond B\\
B\\
\midrule[\inferrulewidth]
\therefore A
\end{array}\]
Beispielsweise stehe $p$ für »Es regnet«, $q$ für »Die Temperatur
der Straßenoberfläche liegt unter null Grad« und $r$ für
»Die Straße wird glatt«. Mit der Setzung $A:=p\land q$ und
$B:=r$ erhält man ein gültiges Argument, obgleich $A$ keine atomare
Aussage mehr ist.

Wir werden die Prämissen später nebeneinander statt untereinander
schreiben, um Argumente zu Beweisbäumen zusammensetzen zu
können.

\subsection{Prädikatenlogik}

Im Rahmen der Aussagenlogik ist es nicht möglich, das Argument
\[\begin{array}{@{}l@{}}
\text{Alle Enten quaken.}\\
\text{Dagobert ist eine Ente.}\\
\midrule[\inferrulewidth]
\text{Also quakt Dagobert.}
\end{array}\]
als Instanz eines Schemas zu sehen, obwohl es den Anschein macht,
sich nach einem ähnlichen Muster wie der Modus ponens zu verhalten.

Um die gewünschte Überlegung fassen zu können, müssen wir die
aussagenlogische Sprache erst zur prädikatenlogischen erweitern.
Zunächst stehe $p$ für »ist eine Ente« und $q$ für »quakt«. Dies sind
nun keine Aussagen, sondern Prädikatsymbole. Man erhält erst mit
$p(x)$ für »$x$ ist eine Ente« und $q(x)$ für »$x$ quakt« atomare
Formeln, die aber streng genommen immer noch keine Aussagen, sondern
Aussageformen sind. Erst wenn der Variable $x$, zum Beispiel mit der
Setzung $x:=\text{Dagobert}$, eine Konstante zugewiesen wird, die inhaltlich
für ein Objekt steht, entsteht eine Aussage.

Außerdem entsteht eine Aussage, indem die Aussageform über die Variable
all- oder existenzquantifiziert wird. Zum Beispiel
steht $\forall x\colon p(x)\cond q(x)$ für die Aussage »Für alle $x$
gilt: Wenn $x$ eine Ente ist, dann quakt $x$«, also für
»Alle Enten quaken«.

Das obige Argument ist nun als Instanz des Schemas
\[\begin{array}{@{}l@{}}
\forall x\colon P(x)\cond Q(x)\\
P(u)\\
\midrule[\inferrulewidth]
\therefore Q(u)
\end{array}\]
verstehbar. Dazu wird $P:=p$ und $Q:=q$ gesetzt und dem Parameter
$u$ die Konstante $u:=\text{Dagobert}$ zugewiesen.

In der prädikatenlogischen Sprache liegt der Schlüssel zur Formalisierung
mathematischer Aussagen. Zum Beispiel ist das Kommutativgesetz der
Addition als die Aussage
\[\forall x\colon\forall y\colon x+y=y+x\]
darstellbar. Genau genommen muss man auch noch sagen, über welches
Diskursuniversum sich die Allquantifizierung erstrecken soll. Sagen wir,
hier will es eins sein, das aus ganzen Zahlen besteht. Das heißt, in
die Variablen $x,y$ sollen ganze Zahlen eingesetzt werden dürfen,
bzw. Terme, die für solche stehen.

\section{Grundbegriffe}

\subsection{Schlussregeln}

% Alles Schlussregeln und Axiome müssen unmittelbar einsichtig sein,
% müssen unbedenklich sein. Normalerweise will man in der Wissenschaft
% nicht so informell sein, aber hier sind wir am basalen Grund.
% Selbst in der Semantik wird der hier aufgestellte Formalismus
% benötigt. Der Korrektheitsbeweis würde somit zirkulär.

% Am Anfang ist noch nicht allzu viel zu rechnen. Wir müssen uns
% erst um die Schlussregeln und Axiome bemühen, bevor das erste Theorem
% bewiesen werden kann.

% #todo
% Subjunktion oder Konditional, Bijunktion oder Bikonditional?

Logisches Schließen findet in einzelnen Schritten statt. Ein Schritt
stellt hierbei immer die Ableitung einer Schlussfolgerung aus einer
oder mehreren Voraussetzungen dar. Die Voraussetzungen heißen
\emph{Prämissen}\index{Praemisse@Prämisse}, die Schlussfolgerung
\emph{Konklusion}\index{Konklusion}. Darstellen wollen wir den Schritt
durch eine waagerechte Linie, wobei die Prämissen oberhalb befindlich
sein sollen, und die Konklusion unterhalb. Der Schritt
\[\dfrac{\text{Wenn es regnet, wird die Straße nass}\qquad\text{Es regnet}}
{\text{Die Straße wird nass}}\]
beschreibt beispielsweise, dass aus den Prämissen »Wenn es regnet, wird
die Straße nass« und »Es regnet« die Konklusion »Die Straße wird nass«
gefolgert wird.

Schlüsse wie der Obige treten in der Mathematik ständig auf. Ihnen allen
liegt ein bestimmtes Muster zugrunde, welches sich durch eine als
\emph{Modus ponens}\index{Modus ponens} oder
\emph{Abtrennungsregel}\index{Abtrennungsregel}
bezeichnete schematische \emph{Schlussregel}\index{Schlussregel}
beschreiben lässt. Es bezeichne hierzu $A\cond B$ die Implikation
»wenn $A$, dann $B$«. Es dürfen nun in
\[\dfrac{A\cond B\qquad A}{B}\]
für $A,B$ beliebige Aussagen eingesetzt werden. So darf »Es regnet«
für $A$ und »Die Straße wird nass« für $B$ eingesetzt werden.

\subsection{Sequenzen}

Das Schließen von Aussagen allein genügt nicht. Um freier argumentieren
zu können, würden wir gerne den Umstand beschreiben können, dass eine
Aussage unter bestimmten Annahmen abgeleitet werden konnte. Diese
Annahmen $A_k$ sind selbst Aussagen. Wir fassen sie zu einer endlichen
Ansammlung
\[\Gamma := [A_1,A_2,\ldots,A_n]\]
zusammen, worunter wir eine endliche Liste, oder auch eine endliche
Menge verstehen wollen, denn man soll mit dieser Liste umgehen können
wie mit einer Menge. Das heißt, es ist nicht von Bedeutung, wie
oft eine Aussage vorkommt oder in welcher Reihenfolge die Aussagen
stehen. Wir bezeichnen die Symbolik
\[\Gamma\vdash A\]
als \emph{Sequenz}\index{Sequenz}. Sie drückt das \emph{Urteil}%
\index{Urteil} aus, dass die Aussage $A$ aus den Annahmen
vermittels Schlussregeln ableitbar ist. Darin nennt man $A$ die
\emph{Hinterformel} oder \emph{Sukzedenz}. Man bezeichnet $\Gamma$ als
die \emph{Vorderformeln}, die \emph{Antezedenz},
\index{Antezedenz} oder die Liste der \emph{Antezedenzen}. Es wird
$\Gamma$ auch der \emph{Kontext}\index{Kontext}
oder die \emph{Umgebung}\index{Umgebung} genannt, das sind auf die
Typentheorie zurückzuführende Sprechweisen, die einen ganz ähnlichen
Formalismus besitzt.
Der Modus ponens\index{Modus ponens} wird nun in der allgemeinen Form
\[\dfrac{\Gamma\vdash A\cond B\qquad\Gamma\vdash A}{\Gamma\vdash B}\]
beschrieben. Wir argumentieren beim Schließen also ab jetzt nicht mehr
mit den Aussagen selbst, sondern mit den Sequenzen. Dies hat einen wichtigen
Grund, nämlich dass die Berücksichtigung der Abhängigkeit von Annahmen
expliziter Teil des Schließens wird.

Ein Kontext kann auch eine leere Liste sein. Besitzt eine vermittels
Schlussregeln ableitbare Sequenz einen leeren Kontext, so bezeichnet
man die Sukzedenz als ein \emph{Theorem}\index{Theorem} im engeren
Sinne. Ein Theorem ist also eine Aussage, die für sich allein gilt,
ohne dass dafür irgendwelche Annahmen getroffen werden müssen.

Für Sequenzen gilt die \emph{Abschwächungsregel}%
\index{Abschwaechungsregel@Abschwächungsregel}. Sie besagt, dass
falls die Aussage $A$ bereits aus $\Gamma$ ableitbar ist, diese
Aussage erst recht ableitbar ist, wenn zu $\Gamma$ weitere Annahmen
$\Gamma'$ hinzugefügt werden. Kurzum gilt die Regel
\[\dfrac{\Gamma\vdash A}{\Gamma,\Gamma'\vdash A}.\]
Hierbei bedeutet $\Gamma,\Gamma'$ die Konkatenation der Listen
$\Gamma$ und $\Gamma'$, also im Wesentlichen dasselbe wie die
Vereinigung $\Gamma\cup\Gamma'$, insofern man die Kontexte als
Mengen betrachtet.

Der Umstand, dass mit dem Kontext umgegangen werden darf, wie mit
einer Menge, drückt sich in zwei weiteren Regeln aus. Es gilt die
\emph{Kontraktionsregel}
\[\dfrac{\Gamma,A,A\vdash B}{\Gamma,A\vdash B},\;\;
\text{oder allgemein}\;\;\dfrac{\Gamma,\Gamma',\Gamma'\vdash B}{\Gamma,\Gamma'\vdash B}\]
und die \emph{Vertauschungsregel}
\[\dfrac{\Gamma,A,B,\Gamma'\vdash C}{\Gamma,B,A,\Gamma'\vdash C},\]
wobei $\Gamma,\Gamma'$ leer sein dürfen. Schließlich gelten für Mengen
die Termumformungen
$\Gamma\cup\Gamma'\cup\Gamma' = \Gamma\cup\Gamma'$
und
\[\Gamma\cup\{A\}\cup\{B\}\cup\Gamma' = \Gamma\cup\{B\}\cup\{A\}\cup\Gamma'.\]


\subsection{Zulässige Schlussregeln}

Wiewohl die Regeln des Schließens den Mechanismus zum Beweis
von Aussagen bilden, ist ihre Rolle sogar noch ein wenig tiefgreifender.
Wir können sie nämlich ebenfalls zur Ableitung \emph{weiterer Regeln}
nutzen. Das heißt, wir können sie dazu nutzen, den logischen Kalkül
selbst zu erweitern. Erweiterungen dieser Art nennen wir
\emph{zulässige Schlussregeln}%
\index{zulaessige Schlussregel@zulässige Schlussregel}.

Mit den bisherigen Regeln ist bereits die zulässige Regel
\[\dfrac{\Gamma\vdash A\cond B\qquad\Gamma'\vdash A}
{\Gamma,\Gamma'\vdash B}\]
ableitbar, die eine allgemeinere Form des Modus ponens darstellt. Man
erhält sie kurzerhand, indem den Prämissen des Modus
ponens jeweils die Abschwächungsregel vorgesetzt wird:
\[\begin{prooftree}
    \hypo{\Gamma\vdash A\cond B}
  \infer1{\Gamma,\Gamma'\vdash A\cond B}
    \hypo{\Gamma'\vdash A}
  \infer1{\Gamma,\Gamma'\vdash A}
\infer2{\Gamma,\Gamma'\vdash B}
\end{prooftree}\]
Die einfache Form des Modus ponens erhält man mit $\Gamma':=\Gamma$ als
Spezialfall unter Anwendung der Kontraktionsregel.

\subsection{Subjunktionseinführung}

Ich möchte mich nun der Frage zuwenden, wie eine Subjunktion $A\cond B$
bewiesen wird. Intuitiv ist hierzu aus der Annahme $A$ die
Aussage $B$ zu folgern. Das heißt, es genügt die Ableitung
der Sequenz $A\vdash B$. Ein weiteres Mal gilt es noch zu
berücksichtigen, dass ein Beweis auch auf einen vorausgesetzten
Kontext $\Gamma$ beschränkt sein dürfen soll. Reflektiert man darüber
eine Weile, dürfte es der Überlegung nach wohl genügen, dass $A$
einfach dem Kontext $\Gamma$ hinzugefügt wird, woraus $B$ zu folgern
ist. Man gelangt zur Regel
\[\dfrac{\Gamma,A\vdash B}{\Gamma\vdash A\cond B}.\]
Wer diese Regel nicht so leicht fassbar findet, insbesondere nicht
direkt plausibel, ob sie bedenkenlos eingesetzt werden darf, der
ist nicht allein. Es gibt auch logische Kalküle, die diese Regel nicht
explizit enthalten. Sie tritt dennoch als \emph{Deduktionstheorem} in
Erscheinung, ein metalogisches Theorem, dessen Beweis erst erbracht
werden muss. Ich möchte diesen Weg allerdings aus einem bestimmten Grund
nicht gehen. Nämlich ist beim Beweis eigentlich natürliches Schließen
auf der metalogischen Ebene zu verwenden, obgleich dies nicht in formaler
Weise stattfinden mag. Aber nicht jeder Leser weiß zu diesem Zeitpunkt,
wie akkurates logisches Schließen geht. Der Leser benötigt am Anfang
etwas, um sich an den eigenen Haaren aus dem Sumpf zu ziehen.

\subsection{Axiome}

Zur Komplettierung des Kalküls gesellen sich schließlich auch noch
\emph{Axiome}\index{Axiom} hinzu, das sind gemachte Grundannahmen, die
nicht weiter bewiesen werden müssen. Sie sollten daher möglichst
plausibel, oder besser noch zweifelsfrei einsichtig sein. Für die Logik
selbst genügt das Axiom
\[A\vdash A.\]
Der Kalkül funktioniert dergestalt, dass für $A$ eine beliebige Aussage
eingesetzt werden darf, worunter auch zusammengesetzte Aussagen
fallen. Eine gern gewählter Weg der Definition des logischen
Kalküls sieht $A$ als eine metasprachliche Variable, für die eine
beliebige Formel eingesetzt werden darf. Unter dieser Sichtweise
spricht man von einem \emph{Axiomenschema}\index{Axiomenschema}. Wie
eine Schablone produziert es für jede Einsetzung einer konkreten
logischen Formel ein eigenes Axiom.

Statt $A,B,C$ sind für metasprachliche Variablen auch
die griechischen Buchstaben $\varphi,\psi,\chi$ geläufig. Man muss sie
von atomaren logischen Variablen unterscheiden, für die ich in diesem
Buch, um Missverständnissen aus dem Weg zu gehen, kleine Buchstaben
$a,b,c$ oder $p,q,r$ verwenden werde. Sprachlich suggestiv steht
$\varphi$ für \emph{Formel}\index{Formel} oder \emph{formula}, $a$ für
\emph{Aussage} und $p$ für \emph{proposition}.

Streng genommen notiert man nur konkrete atomare Variablen als $p,q,r$,
wogegen $P,Q,R$ metasprachliche Variablen sind, für die konkrete atomare
Variablen eingesetzt werden dürfen. Hier ergibt sich allerdings eine
Überschneidung mit der Prädikatenlogik, wo $P,Q,R$ Prädikate bezeichnen.
Das ist aber eigentlich nicht sonderlich schlimm, da atomare Variablen
nichts anderes als nullstellige Prädikate sind. Statt von atomaren
Variablen wird daher auch von logischen Konstanten gesprochen, um sie
von den Individuenvariablen zu unterscheiden, die nicht für logische
Aussagen, sondern für Objekte wie Zahlen oder Mengen stehen.

In diesem Sinne sind auch die Schlussregeln Schemata. Sofern man
Schlussregeln mit null Prämissen gestattet, lässt sich das
Axiomenschema auch als Regel
\[\begin{prooftree}\infer0{A\vdash A}\end{prooftree}\]
auffassen. In dieser Weise wollen wir die Anwendung von Axiomen in den
Beweisbäumen darstellen.

Axiome in der Form von Sequenzen heißen auch
\emph{Grundsequenzen}\index{Grundsequenz}.

Wir haben nun die Mittel in der Hand, um erste Theoreme beweisen
zu können. Es ist $A\cond A$ ein Theorem. Der Beweisbaum ist:
\[\begin{prooftree}
  \infer0[Axiom]{A\vdash A}
\infer1[Subjunktionseinführung]{\vdash A\cond A}
\end{prooftree}\]
Unter der Lesung, dass $A$ eine Metavariable ist, handelt es
eigentlich nicht nur um ein Theorem, sondern um ein Schema von
Theoremen\index{Theoremschema}. Setzt man für $A$ bspw. die konkrete
Formel $p\cond q$ ein, bekommt man das konkrete Theorem
\[(p\cond q)\cond (p\cond q).\]
% #todo
% Es ist zu bemerken, dass die Unterscheidung zwischen Metavariablen
% und atomaren logischen Variablen später durch die Einsetzungsregel
% mehr oder weniger hinfällig wird. Hierbei handelt es sich aber um eine
% höhere Überlegung, deren Beweis der Logiker erbringen will. In der
% Wissenschaft, insbesondere in der Logik, will man den Dingen auf den
% Grund gehen, will alles genau auseinandernehmen. Da möchte man bestimmte
% Regeln nicht einfach so als gegeben voraussetzen.

\subsection{Junktoren}

Bisher traten zusammengesetzte Aussagen alleinig in Form einer
Implikation auf. Will man logische Zusammenhänge beschreiben können,
muss die logische Sprache um weitere Junktoren bereichert werden.
Unter einem \emph{Junktor}\index{Junktor} versteht man einen logischen
Operator, der Aussagen zu einer zusammengesetzten Aussage verknüpft.
Von Belang sind zunächst fünf Stück.

Wir werden einen Junktor durch \emph{Einführungsregeln}%
\index{Einfuehrungsregel@Einführungsregel} und
\emph{Beseitigungsregeln}\index{Beseitigungsregel}
charakterisieren. Die Regeln der Implikation
wurden bereits beschrieben; die Einführung geschieht per
Implikationseinführung, die Beseitigung per Modus ponens.
Für die restlichen Junktoren der Aussagenlogik lassen sich die Regeln
wahlweise in Form von Axiomenschemata oder in Form von Schlussregeln
darstellen. Ich möchte das per Schemata machen, weil diese ein wenig
kompakter sind, was sie hoffentlich ein wenig leichter einsichtig macht.
Die entsprechenden Schlussregeln leiten wir anschließend als zulässige
Regeln ab.

Die Konjunktion\index{Konjunktion} $A\land B$, auch Und"=Verknüpfung
genannt, sprich »$A$ und $B$«, ist charakterisiert durch die Sequenzen
\[A,B\vdash A\land B;\qquad A\land B\vdash A;\qquad A\land B\vdash B.\]
Aus dem Fall von sowohl Regen als auch Schnee ist der Fall von
Schneeregen ableitbar. Aus dem Fall von Schneeregen ist der Fall
von Regen ableitbar. Aus dem Fall von Schneeregen ist der Fall
von Schnee ableitbar. So sind diese Sequenzen zu verstehen.

Die Einführung der Konjunktion geschieht mit der Regel
\[\dfrac{\Gamma\vdash A\qquad\Gamma'\vdash B}{\Gamma,\Gamma'\vdash A\land B}.\]
Denn es findet sich der Beweisbaum:
\[\begin{prooftree}
        \infer0[Axiom]{A,B\vdash A\land B}
      \infer1[Subj-Einf.]{A\vdash B\cond A\land B}
    \infer1[Subj-Einf.]{\vdash A\cond (B\cond A\land B)}
    \hypo{\Gamma\vdash A}
  \infer2[MP]{\Gamma\vdash B\cond A\land B}
  \hypo{\Gamma'\vdash B}
\infer2[MP]{\Gamma,\Gamma'\vdash A\land B}
\end{prooftree}\]
Es steht MP als Abkürzung für Modus ponens, und Sub-Einf. für
Subjunktionseinführung. Man schreibt alternativ auch das Kürzel
$\cond$E statt Subj-Einf. und $\cond$B statt MP. Hierbei steht E
offenkundig für \emph{Einführung} und B für \emph{Beseitigung}. Aber
Vorsicht, in der englischsprachigen Literatur sind das I für
\emph{introduction} und E für \emph{elimination}.
Unmissverständlich wären subj-intro und subj-elim.

Die beiden Regeln zur Beseitigung der Konjunktion sind
\[\dfrac{\Gamma\vdash A\land B}{\Gamma\vdash A},
\qquad\dfrac{\Gamma\vdash A\land B}{\Gamma\vdash B}.\]
Denn es findet sich:
\[\begin{prooftree}
    \infer0[Axiom]{A\land B\vdash A}
  \infer1[Subj-Einf.]{\vdash A\land B\cond A}
  \hypo{\Gamma\vdash A\land B}
\infer2[MP]{\Gamma\vdash A}
\end{prooftree}\]
Die Disjunktion\index{Disjunktion} $A\lor B$, auch Oder"=Verknüpfung
genannt, sprich »$A$ oder $B$«, ist charakterisiert durch die Sequenzen
\[A\vdash A\lor B;\qquad B\vdash A\lor B;\qquad
A\lor B, (A\cond C), (B\cond C)\vdash C.\]
So ist »Die Erde des Beetes ist nass« ableitbar aus »Es hat geregnet
oder das Beet wurde gegossen«. Denn sowohl »Es hat geregnet« als auch
»Das Beet wurde gegossen« impliziert »Die Erde des Beetes ist nass«.

Die beiden Regeln zur Einführung der Disjunktion sind
\[\dfrac{\Gamma\vdash A}{\Gamma\vdash A\lor B},\qquad
\dfrac{\Gamma\vdash B}{\Gamma\vdash A\lor B}.\]
Die Regel zur Beseitigung der Disjunktion ist
\[\dfrac{\Gamma\vdash A\lor B\qquad\Gamma',A\vdash C\qquad\Gamma'',B\vdash C}
{\Gamma,\Gamma',\Gamma''\vdash C}.\]
Die Beweise dieser Regeln seien dem Leser überlassen.

Eine Aussage wie »Bertram wird seine Hausaufgaben nicht machen«
formuliert man gern in der Form »Wenn Bertram seine Hausaufgaben macht,
färbt sich der Mond grün«. In gleichartiger Weise lässt sich die
Verneinung auch in der formalen Logik definieren. Hierzu legt man als
Hilfsbegriff zunächst $\bot$ als die \emph{Kontradiktion}%
\index{Kontradiktion}\index{Widerspruch} fest, sie steht für eine
widersprüchliche Formel.

Die Negation\index{Negation} $\lnot A$, auch Verneinung genannt, sprich
»nicht $A$«, definiert man als identisch mit $A\cond\bot$. Hierdurch
sind die Regeln zu ihrer Einführung und Beseitigung auf die der
Implikation zurückführbar. Es ergibt sich
\[\dfrac{\Gamma,A\vdash\bot}{\Gamma\vdash\lnot A},
\qquad\dfrac{\Gamma\vdash\lnot A\qquad\Gamma'\vdash A}
{\Gamma,\Gamma'\vdash\bot}.\]
Alternativ ließe sich die Negation durch die Sequenzen
\[(A\cond\bot)\vdash\lnot A;\qquad A,\lnot A\vdash\bot\]
charakterisieren. Man überzeuge sich, dass dies aufs selbe hinausläuft.

Die Äquivalenz\index{Aequivalenz@Äquivalenz} $A\bicond B$, sprich
»$A$ genau dann, wenn $B$«, definiert man als identisch mit
$(A\cond B)\land (B\cond A)$. Insofern sind die Regeln zu ihrer
Einführung und Beseitigung auf die der Konjunktion zurückführbar.
Es ergibt sich
\[\dfrac{\Gamma\vdash A\cond B\qquad\Gamma'\vdash B\cond A}
{\Gamma,\Gamma'\vdash A\bicond B},\qquad
\dfrac{\Gamma\vdash A\bicond B}{\Gamma\vdash A\cond B},\qquad
\dfrac{\Gamma\vdash A\bicond B}{\Gamma\vdash B\cond A}.\]
Die entsprechenden charakterisierenden Sequenzen sind
\[(A\cond B),(B\cond A)\vdash A\bicond B;\qquad (A\bicond B),A\vdash B;
\qquad (A\bicond B),B\vdash A.\]

\subsection{Quantoren}

Eine logische Sprache, die der freien Formulierung mathematischer
Zusammenhänge dienlich sein soll, muss hinreichend reichhaltig sein.
Ebenfalls schrieb der Philosoph Ludwig Wittgenstein in seinem
\emph{Tractatus} den ähnlichen Gedanke »Die Grenzen meiner Sprache
bedeuten die Grenzen meiner Welt.« Bislang fehlt das wichtige Konzept
der Quantifizierung, das die Aussagenlogik zur Prädikatenlogik
erweitert.

In der Prädikatenlogik treten \emph{Aussageformen}\index{Aussageform}
auf. Das sind Formeln, die \emph{freie Variablen}\index{freie Variable}
enthalten. Erst wenn jede der freien Variablen mit einem Wert belegt
wird, entsteht eine Aussage. Außerdem treten Quantoren auf. Ein
\emph{Quantor}\index{Quantor} bindet eine freie Variable, und macht
eine Aussageform dabei ebenfalls zu einer bestimmten Aussage.

% #todo
% Enthalten dürfen. Wenn jede (möglicherweise von null) belegt wurde,
% entsteht eine Aussage.

Es sei $A(x)$ eine Aussageform mit der freien Variable $x$. Anstelle
von $A(x)$ schreibt man auch kurzum $A$. Anstelle von $A(t)$ schreibt
man auch $A[x:=t]$ oder $A[t/x]$, womit die Ersetzung jedes Vorkommens
von $x$ durch den Term $t$ gemeint ist. Genauer gesagt die Ersetzung
jedes \emph{freien} Vorkommens, wobei man außerdem einer möglichen
Überschattung einer der in $t$ enthaltenen Variablen aus dem Weg gehen
muss. Diese Spitzfindigkeiten tauchen allerdings erst auf, wenn man mit
Verschachtelungen von Quantoren hantiert. Ich will später
näher darauf eingehen.

Die wesentlichen beiden Quantoren sind der \emph{Allquantor}%
\index{Allquantor}\index{Universalquantor} $\forall$ und der
\emph{Existenzquantor}\index{Existenzquantor} $\exists$. Man ließt
$\forall x\colon A(x)$ als »für alle $x$ gilt $A(x)$« oder »jedes $x$
hat die Eigenschaft $A(x)$«. Man ließt $\exists x\colon A(x)$ als »es
gibt mindestens ein $x$, für das $A(x)$ gilt« oder »mindestens ein $x$
hat die Eigenschaft $A(x)$«.

Quantifiziert wird immer über ein bestimmtes \emph{Diskursuniversum}%
\index{Diskursuniversum}\index{Universum!Diskursuniversum}.
Darunter versteht man die Gesamtheit der
Objekte, auf die sich »für alle« und »es gibt« bezieht. Um bestimmten
Komplikationen aus dem Weg zu gehen, muss es nichtleer sein. Zur
Veranschaulichung des Übergangs von der Aussagenlogik zur Prädikatenlogik
wählen wir ein endliches, das lediglich die Zahlen von eins bis vier
enthält. Die Aussage $\forall x\colon A(x)$ bedeutet nun insofern
dasselbe wie
\[A(1)\land A(2)\land A(3)\land A(4).\]
Diese schlichte Konjunktion gibt Anlass zu der Schlussregel
\[\dfrac{\Gamma\vdash\forall x\colon A(x)}{\Gamma\vdash A(t)}.
\qquad(\text{$t$ muss eine der Zahlen von eins bis vier sein})\]
Die Beseitigung der Allquantifizierung darf insofern als Analogon zur
Beseitigung der Konjunktion verstanden werden.

Im Fortgang soll $\Gamma\vdash A(a)$ bedeuten, dass die Aussageform
$A(a)$ aus dem Kontext $\Gamma$ ableitbar ist, wobei $a$ beliebig
gelassen wird. Man leitet die vier Sequenzen
\[\Gamma\vdash A(1);\quad\Gamma\vdash A(2);\quad\Gamma\vdash A(3);
\quad\Gamma\vdash A(4)\]
sozusagen in einen Zug ab. Es stellt sich nun die Frage
\[\dfrac{\Gamma\vdash A(a)}{\Gamma\vdash\forall x\colon A(x)}?\]
Betrachten wir dazu $a=1\vdash A(a)$. Mit der bedenklichen Regel erhielte
man aus ihr $a=1\vdash\forall x\colon A(x)$. Diese trifft insbesondere
im Fall $a:=1$ zu. Nun braucht man $1=1$ nicht vorauszusetzen, womit
man $\vdash\forall x\colon A(x)$ erhält. Den Quantor beseitigen
wir nun noch mit $x:=a$, und erhalten $\vdash A(a)$. Die Annahme
wurde also aus der Sequenz herausgemogelt. Um dies zu unterbinden,
legen wir fest, dass $a$ keine freie Variable einer der Antezedenzen
sein darf.

Die Regel zur Einführung ist demnach zu formulieren als
\[\dfrac{\Gamma\vdash A(a)}{\Gamma\vdash\forall x\colon A(x)}
(a\notin\mathrm{FV}(\Gamma,\forall x\colon A(x))).\]
Hierbei steht die Symbolik $\mathrm{FV}(\Gamma)$ für die Menge der
freien Variablen von $\Gamma$. Mit elementarer Mengenlehre definiert
man sie präzise als Rekursion über den Formelaufbau. Man legt fest
\begin{gather*}
\mathrm{FV}(A\land B) = \mathrm{FV}(A\lor B)
= \mathrm{FV}(A\cond B) = \mathrm{FV}(A\bicond B)
= \mathrm{FV}(A)\cup\mathrm{FV}(B),\\
\mathrm{FV}(\lnot A) = \mathrm{FV}(A),\quad
\hspace{13pt}\mathrm{FV}(\forall x\colon A) = \mathrm{FV}(\exists x\colon A)
= \mathrm{FV}(A)\setminus\{x\},\\
\mathrm{FV}(\bot) = \mathrm{FV}(\top) = \emptyset,\quad
\mathrm{FV}(P(t_1,\ldots,t_n)) = \mathrm{FV}(t_1)\cup\ldots\cup\mathrm{FV}(t_n).
\end{gather*}
Hierbei steht $P$ für ein $n$-stelliges Prädikat. Und es ist $\mathrm{FV}(t)$
die Menge der Variablen des Terms $t$. Man legt sie abermals
rekursiv fest als
\begin{gather*}
\mathrm{FV}(t_1+t_2) = \mathrm{FV}(t_1-t_2) = \mathrm{FV}(t_1\cdot t_2)
= \mathrm{FV}(t_1)\cup\mathrm{FV}(t_2),\\
\mathrm{FV}(-t)=\mathrm{FV}(t),\quad
\mathrm{FV}(v)=\{v\},\quad\mathrm{FV}(c) = \emptyset,
\end{gather*}
wobei $v$ für eine Variable und $c$ für eine Konstante steht.

Die Aussage $\exists x\colon A(x)$ ist gleichwertig mit
\[A(1)\lor A(2)\lor A(3)\lor A(4).\]
Diese Perspektive gibt Anlass zur Einführungsregel
\[\dfrac{\Gamma\vdash A(t)}{\Gamma\vdash\exists x\colon A(x)}.\quad
(\text{$t$ muss eine der Zahlen von eins bis vier sein})\]
Bei der Beseitigung müssen wir nun gewissermaßen eine Fallunterscheidung
in die vier Fälle vornehmen und bestätigen, dass jeder Fall dieselbe
Aussage $B$ impliziert. Dies soll allerdings parametrisch in einer
einzigen Ableitung stattfinden. Man gelangt zu
\[\dfrac{\Gamma\vdash\exists x\colon A(x)\qquad\Gamma',A(u)\vdash B}
{\Gamma,\Gamma'\vdash B}(u\notin\mathrm{FV}(\Gamma,\Gamma',B,\exists x\colon A(x))).\]
Diese Regel wird wie folgt interpretiert. Mit der Existenzaussage
$\exists x\colon A(x)$ liegt ein Zeuge $u$ mit $A(u)$ vor. Unter
Verwendung von $A(u)$ wird nun eine Aussage $B$ abgeleitet, in der $u$
nicht frei vorkommt. Somit gilt $B$ unabhängig vom gewählten Zeugen,
was notwendig ist, da unbekannt bleibt, welche der Zahlen von eins
bis vier als Zeuge vorliegt.

Ohne die Bedingung an $u$ ließe sich leicht Schabernack vollführen.
Man könnte beispielsweise kurzerhand eine Existenzaussage zu einer
Allaussage machen:
\[\begin{prooftree}
    \hypo{\vdash\exists x\colon A(x)}
    \infer0{A(u)\vdash A(u)}
  \infer2{\vdash A(u)}
\infer1{\vdash\forall x\colon A(x)}
\end{prooftree}\]
Abschließend verbleibt noch näher zu erläutern, wie Substitution
vonstatten geht. Ersetzt wird nur jedes freie Vorkommen einer
Variablen. Ein durch einen Quantor gebundenes Vorkommen der Variable
bleibt dagegen erhalten. So resultiert die Substitution
\[(P(x)\land\forall x: Q(x))[x:=y]\quad\text{in}\quad
P(y)\land\forall x\colon Q(x).\]
Außerdem darf es bei einer Substitution nicht zu einer Überschattung
durch eine Variablenbindung kommen, engl. \emph{capture-avoiding substitution},
was bedeuten soll, dass bei $A[x:=t]$ keine der freien Variablen des Terms $t$
durch eine in $A$ befindliche Variablenbindung eingefangen wird. Die
Substitution
\[(\forall y\colon P(x)\land Q(y))[x:=y]\]
darf beispielsweise nicht direkt ausgeführt werden. Man geht der
Überschattung aus dem Weg, indem die gebundene Variable $y$ zuerst
in eine frische, nehmen wir $z$, umbenannt wird. Das Resultat der
Substitution ist also
\[\forall z\colon P(y)\land Q(z),\quad\text{nicht}\quad
\forall y\colon P(y)\land Q(y).\]

\subsection{Substitution}

Es ist noch zu präzisieren, wie Substitution genau vonstatten geht.
Substituiert werden können sowohl atomare logische Variablen gegen
Formeln als auch Individuenvariablen gegen Terme. Betrachten wir
zunächst die logischen.

Allgemein definiert man ihre Substitution
\[A[P_1:=C_1,\ldots P_n:=C_n],\,\text{kurz $A[S]$ oder $S(A)$}\]
rekursiv über den Formelaufbau als
\begin{align*}
(\lnot A)[S] &:= \lnot (A[S]), & (\forall x\colon A)[S] &:= (\forall x\colon (A[S])),\\
(A\circ B)[S] &:= ((A[S])\circ (B[S])), & (\exists x\colon A)[S] &:= (\exists x\colon (A[S])).
\end{align*}
wobei $\circ$ jeder der zweistelligen Junktoren
$\land,\lor,\cond,\bicond$ ist. Die Basisfälle sind für die atomaren
Variablen $P_1,\ldots, P_n$ und $Q$ definiert gemäß
\[Q[P_1:=C_1, \ldots, P_n:=C_n] := \begin{cases}
C_k, & \text{wenn sich $k$ mit $P_k=Q$ findet},\\
Q, & \text{sonst}.
\end{cases}\]
Für $n\ge 2$ spricht man von \emph{simultaner Substitution}.

Für $n=1$ vereinfacht sich die Substitution zu
\begin{align*}
Q[P:=C] := \begin{cases}
C, & \text{wenn}\;P=Q,\\
Q, & \text{wenn}\;P\ne Q.
\end{cases}
\end{align*}
Beispielsweise ist
\[(a\land b\cond a)[a:=a\lor b] = ((a\lor b)\land b\cond (a\lor b)).\]
Simultane Substitution darf im Allgemeinen nicht schrittweise
durchgeführt werden, weil dadurch ein anderes Resultat entstehen kann.
Zum Beispiel ist
\begin{alignat*}{2}
& (a\land b)[a:=b,b:=c] &&= (b\land c),\\
& (a\land b)[a:=b][b:=c] &&= (c\land c).
\end{alignat*}
Implementiert man die Substitution als Computerprogramm, bildet sie
üblicherweise abstrakte Syntaxbäume auf abstrakte Syntaxbäume ab.

Die Substitution von Individuenvariablen gegen Terme definiert man
ganz analog. Hier ist allerdings hinsichtlich der Quantoren zu beachten,
dass nur freie Variablen substituiert werden, und man eine unter
Umständen entstehende Überschattung durch Umbenennung der gebundenen
Variablen umgehen muss.

\subsection{Zur Syntax}

So wie »Punktrechnung vor Strichrechnung« gilt, legt man für jeden
Junktor zur Einsparung von Klammern eine Stufe der Priorität fest. In
absteigender Rangfolge sind dies ${\lnot}, {\land}, {\lor}, {\cond},
{\bicond}$. So wird die Formel
\[\lnot A\land B\lor C\cond D\quad\text{gelesen als}\quad
(((\lnot A)\land B)\lor C)\cond D.\]
Weiterhin legt man die Implikation als rechtsassoziativ%
\index{rechtsassoziativ} fest. So wird
\[A\cond B\cond C\quad\text{gelesen als}\quad A\cond (B\cond C).\]
Wie den Junktoren kommt auch den Quantoren eine Rangstufe zu. Weil
diese aber präfix sind, ist bei ihnen lediglich die rechte Seite zu
berücksichtigen. Hier sind zwei Varianten verbreitet. In der
Schreibweise $(\forall x)A(x)$ oder kurz $\forall xA(x)$ haben
sie wie die Verneinung die höchste Rangstufe. In der Schreibweise
$\forall x\colon A(x)$ oder $\forall x.\, A(x)$ dagegen die niedrigste,
also eine Stufe niedriger als das Bikonditional, so dass alles
hinter dem Doppelpunkt in den Wirkungsbereich des Quantors fällt.
So wird
\[\forall x\colon A(x)\land B\cond C\quad\text{gelesen als}\quad
\forall x\colon ((A(x)\land B)\cond C).\]
Manche Schüler haben Schwierigkeiten, die Struktur von Termen zu
durchschauen. Infolge kann es bei ihnen zu Flüchtigkeitsfehlern
bei der Ersetzung von Variablen durch Terme kommen. Sie vergessen,
dass ein Term vor der Einfügung zunächst geklammert werden muss.
Erst die Operatorrangfolge gewährt es, die Klammern unter Umständen
nachträglich entfallen zu lassen. Für diese Schüler mag es förderlich
sein, einen Term als \emph{abstrakten Syntaxbaum} darzustellen.
Gleichermaßen verhält es sich mit der Programmiersprache Lisp, die
Terme als Schachtelung von Listen darstellt, deren Klammern
obligatorisch sind.  Die Aussage $A\land B\cond C$ ist beispielsweise
beschreibbar als
\[\text{\texttt{(implies (and A B) C)}}.\]
Im Wesentlichen veranschaulicht diese Schachtelung
nichts anderes als den abstrakten Syntaxbaum. Man kann gewissermaßen
sagen, dass Lisp eine Programmiersprache ohne Syntax ist. Fast ohne,
im höheren Sinne ohne.

Um sich unmissverständlich auszudrücken, formalisieren Logiker die
logische Sprache gern. Es wird hierzu eine \emph{formale Sprache} spezifiziert,
was vermittels sogenannter \emph{Produktionsregeln} gemacht werden kann.
Insofern Produktionsregeln etwas kryptisch anmuten mögen, beschreiben
Logiker die syntaktische Struktur auch in Worten.
Für die Aussagenlogik üblicherweise folgendermaßen.

\begin{enumerate}\setlength\itemsep{0em}
\item Die atomaren Variablen $a,b,c$ usw. sind Formeln.
\item Die Symbole $\bot,\top$ sind Formeln.
\item Ist $A$ eine Formel, so ist auch $(\lnot A)$ eine.
\item Sind $A,B$ Formeln, so ist auch $(A\land B)$ eine.
\item Sind $A,B$ Formeln, so ist auch $(A\lor B)$ eine.
\item Sind $A,B$ Formeln, so ist auch $(A\cond B)$ eine.
\item Sind $A,B$ Formeln, so ist auch $(A\bicond B)$ eine.
\item Nichts anderes ist eine Formel.
\end{enumerate}

\noindent
Hierbei bleibt allerdings die Rangfolge und Assoziativität der Junktoren
unberücksichtigt. Um sie festzulegen, kann man eine Grammatik der Form PEG
-- dies steht für \emph{parsing expression grammar}
-- mit unterschiedlichen Nichtterminalsymbolen aufstellen. Die obige Regelung
ist diesbezüglich als spezielle Grammatik betrachtbar, in der \emph{Formel}
das einzige Nichtterminalsymbol darstellt. Daraufhin kann mit der Technik des
rekursiven Abstiegs ein Parser programmiert werden, der Formeln in
abstrakte Syntaxbäume umwandelt. Zur Vollendung kann der Parser ggf.
anschließend in einen effizienteren Bottom"=up"=Parser transformiert
werden. Ich will hier nicht darauf eingehen, zumal dies
Grundkenntnisse in der Programmierung voraussetzt. Ausführliche
Erklärungen zu formalen Grammatiken und Parsern finden sich in Büchern
über Compilerbau.

Schreibt man viele logische Formeln auf, drängt es, zumindest bei privaten
Notizen und Rechnungen, nach Kurzschreibweisen. In der Logik ist für das
Konditional $A\cond B$ auch die Schreibweise $A\rightarrow B$ gebräuchlich,
für das Bikonditional $A\bicond B$ entsprechend $A\leftrightarrow B$.
Insbesondere in der Schaltalgebra schreibt man auch $\overline A$
anstelle von $\lnot A$, und $AB$ anstelle von $A\land B$ sowie $A+B$
anstelle von $A\lor B$. Hierbei darf die Disjunktion $A+B$ allerdings
nicht mit der Kontravalenz $A\oplus B$ verwechselt werden. Für die
Quantifizierung $\forall x\colon A(x)$ bietet sich $\forall_x A_x$ oder
$\forall x.\, A_x$ als kurzschriftliche Form an.

\newpage
\section{Natürliches Schließen}

\subsection{Darstellungsformen}

Abgeleitet werden soll das Theorem
\[\vdash (A\cond B)\cond (\lnot B\cond\lnot A).\]
Meine favorisierte und in diesem Buch genutzte Form der Darstellung
des natürlichen Schließens fügt die aus den Schlussregeln erhaltenen
Schlüsse wie Legosteine zu einem Baum zusammen, dem
\emph{Beweisbaum}\index{Beweisbaum} oder \emph{Herleitungsbaum}.
Im Eigentlichen stehen in den Blättern die Grundsequenzen, und in der
Wurzel das Theorem. Wie wir es bereits getan haben, arbeitet man
allerdings auch mit Exemplaren, die irgendwelche Sequenzen in
irgendwelche Sequenzen überführen, womit man zulässige Schlussregeln
erhält. Allgemeiner ginge ferner die Formulierung als gerichteter
azyklischer Graph, die bei einigen Beweisen ein wenig den
Schreibaufwand reduzieren würde.

Der Beweisbaum des genannten Theorems ist:
\[\begin{prooftree}
        \infer0[Axiom]{\lnot B\vdash\lnot B}
          \infer0[Axiom]{A\cond B\vdash A\cond B}
          \infer0[Axiom]{A\vdash A}
        \infer2[$\cond$B]{A\cond B, A\vdash B}
      \infer2[$\lnot$B]{A\cond B, \lnot B, A\vdash\bot}
    \infer1[$\lnot$E]{A\cond B, \lnot B\vdash\lnot A}
  \infer1[$\cond$E]{A\cond B\vdash \lnot B\cond\lnot A}
\infer1[$\cond$E]{\vdash (A\cond B)\cond (\lnot B\cond\lnot A)}
\end{prooftree}\]
Die Ausformulierung der Sequenzen verlangt langwieriges erneutes
Aufschreiben der Antezedenzen. Sobald man das Prozedere einmal
verstanden hat, erscheint es überausführlich. Man kann sich daher
verkürzte Darstellungen der Beweisbäume überlegen:
\[\begin{array}{@{}l@{\qquad\quad}l}
\begin{prooftree}[center=false]
        \infer0{1\equiv\lnot B}
          \infer0{2\equiv A\cond B}
          \infer0{3\equiv A}
        \infer2{2, 3\vdash B}
      \infer2{1, 2, 3\vdash\bot}
    \infer1{1, 2\vdash\lnot A}
  \infer1{2\vdash \lnot B\cond\lnot A}
\infer1{\vdash (A\cond B)\cond (\lnot B\cond\lnot A)}
\end{prooftree}
&
\begin{prooftree}[center=false]
        \infer0[1]{\lnot B}
          \infer0[2]{A\cond B}
          \infer0[3]{A}
        \infer2{B}
      \infer2{\bot}
    \infer1[$\sim$3]{\lnot A}
  \infer1[$\sim$1]{\lnot B\cond\lnot A}
\infer1[$\sim$2]{(A\cond B)\cond (\lnot B\cond\lnot A)}
\end{prooftree}
\end{array}\]
Die linke Form kürzt die Antezedenzen durch Nummern ab. In der rechten
Form entfallen die Antezedenzen vollständig. Stattdessen tauchen sie
als in den Blättern gemachte nummerierte \emph{Annahmen} auf, die im
Fortgang zur Wurzel irgendwann zu tilgen sind. Ihre Tilgung erscheint
nun als Randnotiz.

Eine weitere, sehr systematische Darstellung setzt den Beweis aus
einer Liste von Tabellenzeilen zusammen. Allerdings ist sie ein wenig
mühevoll zu lesen. Jede Zeile enthält eine Aussage und dahinter
zusätzlich die Information, wie und woraus die Aussage abgeleitet wurde.
Jede der Aussagen bekommt eine Nummer, siehe Tabelle \ref{tab:Kontraposition}.
Die Nummerierung der Abhängigkeiten ist in derselben Reihenfolge wie
zuvor bei den Bäumen angegeben. Wer die Liste genauer betrachtet, erkennt,
dass die jeweilige Zeile nichts anderes als die Sequenz
$\text{Abh.}\vdash\text{Nr.}$ darstellt.

\begin{table}
\begin{center}
\caption{Beweis in Form einer Liste von Tabellenzeilen}
\label{tab:Kontraposition}
\begin{tabular}{cclll}
\toprule
\strong{Abh.} & \strong{Nr.} & \strong{Aussage} & \strong{Regel} & \strong{auf}\\
\midrule[\heavyrulewidth]
1 & 1 & $\lnot B$ & Axiom &\\
2 & 2 & $A\cond B$ & Axiom &\\
3 & 3 & $A$ & Axiom &\\
2, 3 & 4 & $B$ & $\cond$B & 2, 3\\
1, 2, 3 & 5 & $\bot$ & $\lnot$B & 1, 4\\
1, 2 & 6 & $\lnot A$ & $\lnot$E & 5\\
2 & 7 & $\lnot B\cond\lnot A$ & $\cond$E & 6\\
$\emptyset$ & 8 & $(A\cond B)\cond(\lnot B\cond\lnot A)$ & $\cond$E & 7\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Unabhängig von Gentzen entwickelte Stanisław Jaśkowski das natürliche
Schließen einige Jahre zuvor. Während Gentzen Beweise als Bäume darstellte,
nutzte Jaśkowski zunächst eine grafische Darstellung, die später von Frederic
Brenton Fitch adaptiert wurde und in dieser Form nun als \emph{Fitch"=Style}%
\index{Fitch-Style} bekannt ist. Die Abhängigkeit von einer Annahme
wird hier kenntlich gemacht, indem die aus der Annahme abgeleiteten
Aussagen hinter einer senkrechten Linie eingerückt stehen. Die Annahme
selbst steht am Anfang der Einrückung, und zwar bereits innerhalb,
weil sie ja von sich selbst abhängig ist.
Siehe Tabelle \ref{tab:Kontraposition-Fitch}.
% Annahmen können im Fitch-Style nur in der umgekehrten Reihenfolge
% getilgt werden, in der sie eingeführt wurden.
% In \emph{forall x: Calgary} wird das Schließen im Fitch-Style
% sehr ausführlich beschrieben. Im Buch ist das Schließen sehr
% ausführlich beschrieben, die Formalisierung gänzlich im Fitch-Style
% gehalten.

\begin{table}
\begin{center}
\caption{Beweis im Fitch-Style}
\label{tab:Kontraposition-Fitch}
$\begin{nd}
\open
  \hypo {1} {A\cond B}
  \open
    \hypo {2} {\neg B}
    \open
    \hypo {3} {A}
    \have {4} {B} \ie{1,3}
    \have {5} {\bot} \ne{2,4}
    \close
  \have {6} {\neg A} \ni{5}
  \close
\have {7} {\neg B\cond\neg A} \ii{6}
\close
\have {8} {(A\cond B)\cond (\neg B\cond\neg A)} \ii{7}
\end{nd}$
\end{center}
\end{table}

Zu guter Letzt muss die klassische Darstellung der Beweisführung
aufgeführt werden. Die in Worten. Sie zeichnet sich durch die Auslassung
mühseliger technischer Details und blumige Formulierungen aus,
soll aber genug Information enthalten, dass der Leser im Zweifel
eine Formalisierung des Beweises erstellen und verifizieren kann.

\begin{Satz}
Es gilt $(A\cond B)\cond (\lnot B\cond\lnot A)$.
\end{Satz}
\strong{Beweis.} Aus der Annahme von sowohl $A\cond B$ als
auch $\lnot B$ als auch $A$ ist ein Widerspruch abzuleiten.
Man erhält $B$ zunächst per Modus ponens aus $A\cond B$ und $A$.
Nun steht $\lnot B$ bereits im Widerspruch zu $B$.\,\qedsymbol

Als komfortablen Bonus erhält man mit dem Theorem nun im Anschluss
kurzerhand eine weitere zulässige Regel, die
\emph{Kontrapositionsregel}\index{Kontraposition}
\[\dfrac{\Gamma\vdash A\cond B}{\Gamma\vdash\lnot B\cond\lnot A},
\quad\text{denn}\quad
\dfrac{\vdash (A\cond B)\cond (\lnot B\cond\lnot A)\quad\Gamma\vdash A\cond B}
{\Gamma\vdash \lnot B\cond\lnot A}.\]
Fügt man ihr den Modus ponens an, findet sich der
\emph{Modus tollens}\index{Modus tollens}
\[\dfrac{\Gamma\vdash A\cond B\qquad\Gamma'\vdash\lnot B}
{\Gamma,\Gamma'\vdash\lnot A}.\]

% #todo
% De morgansche Regeln für die Verneinung von Quantifizierungen.
% Zum Beweis der Verneinung einer Allaussage genügt es, ein
% Gegenbeispiel zu finden.

\subsection{Theoreme der Prädikatenlogik}

In einer Formelsammlung zur Prädikatenlogik findet man eine Reihe von
Äquivalenzen und Implikationen vor, von denen wir einige beweisen
wollen.

\begin{Satz}
Es ist $A\land\forall x\colon B(x)$ äquivalent zu $\forall x\colon A\land B(x)$,
sofern die Variable $x$ nicht frei in $A$ vorkommt.
\end{Satz}
\begin{Beweis}
Zur Implikation von links nach rechts findet sich der Baum:
\begin{small}
\[
\begin{array}{@{}l@{\qquad}l}
\begin{prooftree}[center=false]
        \infer0[1]{A\land\forall x\colon B(x)}
      \infer1{A}
          \infer0[1]{A\land\forall x\colon B(x)}
        \infer1{\forall x\colon B(x)}
      \infer1{B(x)}
    \infer2{A\land B(x)}
  \infer1{\forall x\colon A\land B(x)}
\infer1[$\sim$1]{(A\land\forall x\colon B(x))\cond (\forall x\colon A\land B(x))}
\end{prooftree}
&
\begin{prooftree}[center=false]
        \infer0[1]{A\land\forall x\colon B(x)}
      \infer1{A}
          \infer0[1]{A\land\forall x\colon B(x)}
        \infer1{\forall x\colon B(x)}
      \infer1{B(u)}
    \infer2{A\land B(u)}
  \infer1{\forall x\colon A\land B(x)}
\infer1[$\sim$1]{(A\land\forall x\colon B(x))\cond (\forall x\colon A\land B(x))}
\end{prooftree}
\end{array}
\]
\end{small}%
Beide Formen sind zulässig. Die rechte Form gibt dem Parameter den
eigenen Bezeichner $u$, die linke nennt diesen ebenfalls so wie die
gebundene Variable. In Worten würde man diesen Beweis wie folgt
führen. Es sei $u$ fest, aber beliebig, woraus $A\land B(u)$ abzuleiten
ist. Laut Voraussetzung gilt sowohl $A$ als auch $\forall x\colon B(x)$.
Spezialisierung $x:=u$ liefert $B(u)$. Ergo gilt sowohl $A$ als
auch $B(u)$, und somit $A\land B(u)$.

Zur Implikation von rechts nach links findet sich:
\begin{small}
\[
\begin{array}{@{}l@{\qquad}l}
\begin{prooftree}[center=false]
        \infer0[1]{\forall x\colon A\land B(x)}
      \infer1{A\land B(x)}
    \infer1{A}
          \infer0[1]{\forall x\colon A\land B(x)}
        \infer1{A\land B(x)}
      \infer1{B(x)}
    \infer1{\forall x\colon B(x)}
  \infer2{A\land\forall x\colon B(x)}
\infer1[$\sim$1]{(\forall x\colon A\land B(x))\cond (A\land\forall x\colon B(x))}
\end{prooftree}
&
\begin{prooftree}[center=false]
        \infer0[1]{\forall x\colon A\land B(x)}
      \infer1{A\land B(u)}
    \infer1{A}
          \infer0[1]{\forall x\colon A\land B(x)}
        \infer1{A\land B(u)}
      \infer1{B(u)}
    \infer1{\forall x\colon B(x)}
  \infer2{A\land\forall x\colon B(x)}
\infer1[$\sim$1]{(\forall x\colon A\land B(x))\cond (A\land\forall x\colon B(x))}
\end{prooftree}
\end{array}
\]
\end{small}%
In Worten: Es ist $A\land\forall x\colon B(x)$ abzuleiten, also
sowohl $A$ als auch $\forall x\colon B(x)$, was sich mit $B(u)$ für ein
festes, aber beliebiges $u$ bestätigt. Laut Voraussetzung erhält man
$A\land B(u)$ per Spezialisierung $x:=u$. Ergo gilt sowohl $A$ als
auch $B(u)$.\,\qedsymbol
\end{Beweis}

\noindent
Die ausführliche Besprechung verdeutlicht nochmals, wie die Floskel
\emph{fest, aber beliebig} einen Parameter, über den die Argumentation verläuft,
einführt. Der Parameter steht für ein \emph{festes} Objekt, insofern er
während der Argumentation für nur ein Objekt steht. Weil das Objekt
\emph{beliebig} ist, also keine Einschränkungen an dessen Beschaffenheit
gemacht wurden, darf anschließend ohne Weiteres die Einführung einer
Allquantifizierung vorgenommen werden. Es besteht im logischen System,
wie es in diesem Buch dargelegt wurde, kein formaler Unterschied
zwischen einem Parameter und einer Variable. Ein Parameter verhält sich
als freie Variable.

\subsection{Bezug zum Sequenzenkalkül}

Einige Regeln des natürlichen Schließens bieten bei der Lesung eines
Beweisbaums von der Wurzel aus zu den Blättern hin routinemäßige
Zurückführung eines Ziels auf Unterziele. Jedoch nicht jede der
Regeln, womit das Schließen dennoch zum Denksport wird. Aufgrund dessen
gestaltet sich auch die Auffindung eines Algorithmus' zur automatischen
Erzeugung eines Beweisbaums als schwierig.

Der Sequenzenkalkül macht das Schließen nun gänzlich zur Routine. Mithin
ist zu diesem Kalkül ein Algorithmus zur Erzeugung von Beweisbäumen
vergleichsweise leicht zu finden.

\begin{table}
\begin{center}
\caption{Die Regeln des Sequenzenkalküls}
\label{tab:Sequenzenkalkuel}
\begin{tabular}{@{}c@{\quad\;\;}c@{\quad\;\;}c@{\quad\;\;}c@{}}
\toprule
& \strong{Linke Regel} & \strong{Rechte Regel} &\\
\midrule[\heavyrulewidth]
$\dfrac{\Gamma\vdash\Delta}{\Gamma,\Gamma'\vdash\Delta}$
& $\dfrac{\Gamma, A, B\vdash\Delta}{\Gamma, A\land B\vdash\Delta}$
& $\dfrac{\Gamma\vdash A,\Delta\qquad\Gamma'\vdash B,\Delta'}
{\Gamma,\Gamma'\vdash A\land B,\Delta,\Delta'}$
& $\dfrac{\Gamma\vdash\Delta}{\Gamma\vdash\Delta,\Delta'}$\\[14pt]
$\dfrac{\Gamma,A,A\vdash\Delta}{\Gamma,A\vdash\Delta}$
& $\dfrac{\Gamma,A\vdash\Delta\qquad\Gamma',B\vdash\Delta'}
{\Gamma,\Gamma',A\lor B\vdash\Delta,\Delta'}$
& $\dfrac{\Gamma\vdash A,B,\Delta}{\Gamma\vdash A\lor B,\Delta}$
& $\dfrac{\Gamma\vdash B,B,\Delta}{\Gamma\vdash B,\Delta}$\\[14pt]
$\dfrac{\Gamma,\top\vdash\Delta}{\Gamma\vdash\Delta}$
& $\dfrac{\Gamma\vdash A,\Delta\qquad\Gamma',B\vdash\Delta'}
{\Gamma,\Gamma',A\cond B\vdash\Delta,\Delta'}$
& $\dfrac{\Gamma,A\vdash B,\Delta}{\Gamma\vdash A\cond B,\Delta}$
& $\dfrac{\Gamma\vdash\Delta,\bot}{\Gamma\vdash\Delta}$\\[14pt]
$\dfrac{}{\Gamma,\bot\vdash\Delta}$
& $\dfrac{\Gamma\vdash A,\Delta}{\Gamma,\lnot A\vdash\Delta}$
& $\dfrac{\Gamma,A\vdash\Delta}{\Gamma\vdash\lnot A,\Delta}$
& $\dfrac{}{\Gamma\vdash\top,\Delta}$\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Man darf als dienlich erachten, dass die Darstellung des natürlichen
Schließens, wie sie in diesem Buch dargelegt wurde, mit dem
Sequenzenkalkül kompatibel ist. In ihm dürfen Sequenzen von der
allgemeineren Form
\[A_1\ldots,A_m\vdash B_1,\ldots,B_n\]
sein, die für die Aussage
\[A_1\land\ldots\land A_m\cond B_1\lor\ldots\lor B_n\]
steht. Um die Regeln des Sequenzenkalküls vermittels natürlichem Schließen
als zulässige Regeln herzuleiten, wird man daher zunächst die
Übersetzungsregeln
\[\dfrac{\Gamma\vdash B_1\lor\ldots\lor B_n}{\Gamma\vdash B_1,\ldots,B_n},\quad\;
\dfrac{\Gamma\vdash\bot}{\Gamma\vdash},\quad\;
\dfrac{\Gamma\vdash B_1,\ldots,B_n}{\Gamma\vdash B_1\lor\ldots\lor B_n},\quad\;
\dfrac{\Gamma\vdash}{\Gamma\vdash\bot}.\]
fordern. Eine Auflistung der wesentlichen Regeln zeigt die Tabelle
\ref{tab:Sequenzenkalkuel}. Sie untergliedern sich in linke und rechte
Regeln. Die linken gestatten es dabei, Ziele ebenfalls bezüglich
Antezedenzen auf Unterziele zurückzuführen. Jede der Ansammlungen
$\Gamma,\Gamma',\Delta,\Delta'$ darf leer sein. Eine algorithmische
Umsetzung der Beweissuche mag $\Gamma=\Gamma'$ und $\Delta=\Delta'$
setzen, für den Menschen entstünde dadurch aber umständlicher
Schreibaufwand. Exemplarisch soll die linke Regel zur Disjunktion als
zulässig bestätigt werden. Für sie findet sich der Baum:
\[
\begin{prooftree}
    \infer0{A\lor B\vdash A\lor B}
        \hypo{\Gamma,A\vdash\Delta}
      \infer1{\Gamma,A\vdash\Delta,\Delta'}
    \infer1{\Gamma,A\vdash C}
        \hypo{\Gamma',B\vdash\Delta'}
      \infer1{\Gamma',B\vdash\Delta,\Delta'}
    \infer1{\Gamma',B\vdash C}
  \infer3{\Gamma,\Gamma',A\lor B\vdash C}
\infer1{\Gamma,\Gamma',A\lor B\vdash \Delta,\Delta'}
\end{prooftree}
\]
Hierbei soll $C$ die Disjunktion der Aussagen von $\Delta,\Delta'$ sein.

Es folgt am Beispiel des Theoremschemas zur Kontraposition, wie das
Schließen vonstatten geht. Die Beweisbäume wären von unten nach oben
zu lesen, weil das, was weiter oben steht, eigentlich noch unbekannt ist:
\[\begin{array}{@{}l@{\qquad\quad}l}
\begin{prooftree}
        \infer0{A\vdash A}
      \infer1{\vdash A, \lnot A}
        \infer0{B\vdash B}
      \infer1{B, \lnot B\vdash}
    \infer2{A\cond B, \lnot B\vdash\lnot A}
  \infer1{A\cond B\vdash\lnot B\cond\lnot A}
\infer1{\vdash (A\cond B)\cond (\lnot B\cond\lnot A)}
\end{prooftree}
&
\begin{prooftree}
        \infer0{B\vdash B}
      \infer1{\vdash \lnot B, B}
        \infer0{A\vdash A}
      \infer1{\lnot A, A\vdash}
    \infer2{\lnot B\cond\lnot A, A\vdash B}
  \infer1{\lnot B\cond\lnot A\vdash A\cond B}
\infer1{\vdash (\lnot B\cond\lnot A)\cond (A\cond B)}
\end{prooftree}
\end{array}\]
Der Kalkül muss ein klassischer sein, sonst wäre die Kontraposition
nicht rückgängig zu machen. Noch drastischere Einsicht diesbezüglich
schaffen die Bäume:
\[\begin{array}{l@{\qquad}l}
\begin{prooftree}
    \infer0{A\vdash A}
  \infer1[$\lnot$R]{\vdash A,\lnot A}
\infer1[$\lor$L]{\vdash A\lor\lnot A}
\end{prooftree}
&
\begin{prooftree}
    \infer0{A\vdash A}
  \infer1[$\lnot$L]{\vdash\lnot A, A}
\infer1[$\lnot$R]{\lnot\lnot A\vdash A}
\end{prooftree}
\end{array}\]
Es gibt auch Varianten des Sequenzenkalküls, die ausschließlich die
Ableitung von Theoremen der intuitionistischen Logik gestatten. Sie sind
allerdings umständlicher zu verwenden, da es sich um Einschränkungen des
klassischen Kalküls handelt. Bereits Gentzen beschrieb so einen
als LJ in \cite{Gentzen1935}. Eine sorgfältige Diskussion findet man
in \cite{Mimram}, \cite{von-Plato-Reasoning}.

Alternative Formen der Regeln zur Implikation sind
\[\dfrac{\Gamma,\lnot A\vdash\Delta\qquad\Gamma, B\vdash\Delta}
{\Gamma, A\cond B\vdash\Delta},\qquad
\dfrac{\Gamma\vdash \lnot A, B,\Delta}{\Gamma\vdash A\cond B,\Delta}.\]
Die Regeln der Allquantifizierung sind
\[\dfrac{\Gamma, A(t)\vdash\Delta}{\Gamma,\forall x\colon A(x)\vdash\Delta},\qquad
\dfrac{\Gamma\vdash A(u),\Delta}{\Gamma\vdash (\forall x\colon A(x)),\Delta}
(u\notin\mathrm{FV}(\Gamma,\Delta,A(x))),\]
die der Existenzquantifizierung sind
\[\dfrac{\Gamma, A(u)\vdash\Delta}{\Gamma,\exists x\colon A(x)\vdash\Delta}
(u\notin\mathrm{FV}(\Gamma,\Delta,A(x))),\qquad
\dfrac{\Gamma\vdash A(t),\Delta}{\Gamma\vdash (\exists x\colon A(x)),\Delta}.\]
Eine besondere Bedeutung besitzt die Schnittregel
\[\dfrac{\Gamma\vdash A,\Delta\qquad\Gamma',A\vdash\Delta'}
{\Gamma,\Gamma'\vdash\Delta,\Delta'}.\]
Laut Gentzens Hauptsatz findet sich zu jedem Beweis einer Sequenz,
in dem die Schnittregel zur Anwendung kommt, ein alternativer Beweis,
der auf sie verzichtet. Kurzum ist sie zulässig, aber redundant. Dieses
Resultat ist von großer Bedeutung für die Beweistheorie.

\subsection{Bezug zum Tableaukalkül}

Der \emph{Tableaukalkül}\index{Tableaukalkuel@Tableaukalkül} ist ein
systematisches Beweisverfahren, bei dem durch abermalige Zurückführung
einer Formel auf kleinere Formeln ein Baum entsteht. In einer
geläufigen Form des Verfahrens kommt der Beweis einer Aussage zustande,
indem ihre Verneinung widerlegt wird. Die Widerlegung stellt sich
dadurch her, dass jeder Pfad eine Formel enthält, deren
Verneinung bereits auf dem direkten Pfad zur Wurzel vorkam, was auch
als \emph{Schließung} des Pfades bezeichnet wird. Der Baum verzweigt
sich nicht bei jeder Formel. Man unterscheidet zwischen
Formeln vom Typ einer Konjunktion und Formeln vom Typ einer
Disjunktion. Lediglich bei den Formeln vom Typ einer Disjunktion
findet eine Verzweigung statt.

Es stellt sich im Fortgang heraus, dass der Tableaukalkül in der Logik
nicht abgeschieden steht. Ganz im Gegenteil lässt sich ein enger Bezug
zum Schließen von Sequenzen herstellen. Genauer gesagt gehört zu jeder
Regel des Tableaukalküls eine zulässige Regel des Schließens von
Sequenzen, womit sich dieser als ein Teilsystem des allgemeinen
Sequenzenkalküls erweist.

Laut der Reductio ad absurdum ist $\Gamma\vdash A$ auf
$\Gamma,\lnot A\vdash\bot$ zurückführbar. Im Weiteren wird $\Gamma\vdash$
als Abkürzung für $\Gamma\vdash\bot$ geschrieben. Man ruft sich nun
die Äquivalenz der Aussagen $A\cond B$ und $\lnot A\lor B$ in Erinnerung.
Weiterhin befindet man mit den de morganschen Gesetzen die Aussage
$\lnot (A\land B)$ zu $\lnot A\lor\lnot B$ äquivalent, sowie $\lnot (A\lor B)$
zu $\lnot A\land\lnot B$. Aus diesen Überlegungen heraus
gelangt man zu den unverzweigenden zulässigen Regeln%
\[
\dfrac{\Gamma,A,B\vdash}{\Gamma,A\land B\vdash},\qquad
\dfrac{\Gamma,\lnot A,\lnot B\vdash}{\Gamma,\lnot (A\lor B)\vdash},\qquad
\dfrac{\Gamma,A,\lnot B\vdash}{\Gamma,\lnot(A\cond B)\vdash},
\]
und den verzweigenden zulässigen Regeln
\[
\dfrac{\Gamma,A\vdash\quad\Gamma,B\vdash}{\Gamma,A\lor B\vdash},\qquad
\dfrac{\Gamma,\lnot A\vdash\quad\Gamma,\lnot B\vdash}{\Gamma,\lnot (A\land B)\vdash},\qquad
\dfrac{\Gamma,\lnot A\vdash\quad\Gamma,B\vdash}{\Gamma,(A\cond B)\vdash}.
\]
Schließlich wäre noch festzustellen, dass $\Gamma,A,\lnot A\vdash$
mit $\Gamma,A\vdash A$ gleichbedeutend ist, und somit die Rolle einer
Grundsequenz einnehmen darf.

Ein Beispiel. Mit den aufgestellten Regeln ergibt sich für die
Kontraposition der folgende Beweisbaum:
\[\begin{array}{@{}l@{\qquad}l}
\begin{prooftree}
        \infer0{\lnot A, \lnot B, \lnot\lnot A\vdash}
        \infer0{B, \lnot B, \lnot\lnot A\vdash}
      \infer2{A\cond B, \lnot B, \lnot\lnot A\vdash}
    \infer1{A\cond B, \lnot(\lnot B\cond\lnot A)\vdash}
  \infer1{\lnot ((A\cond B)\cond (\lnot B\cond\lnot A))\vdash}
\infer1{\vdash (A\cond B)\cond (\lnot B\cond\lnot A)}
\end{prooftree}
&
\begin{prooftree}[proof style = downwards, separation = 2em, rule margin = 0.7ex]
          \hypo{\lnot A\,\checkmark}
          \hypo{B\,\checkmark}
        \infer2{\lnot\lnot A}
      \infer[rule style = no rule]1{\lnot B}
    \infer[rule style = no rule]1{\lnot(\lnot B\cond\lnot A)}
  \infer[rule style = no rule]1{A\cond B}
\infer[rule style = no rule]1{\lnot ((A\cond B)\cond (\lnot B\cond\lnot A))}
\end{prooftree}
\end{array}\]
Wird dieser Baum nun auf den Kopf gestellt und die Notation dergestalt
verkürzt, dass jede der Formeln nur einmal aufgeschrieben werden
braucht, findet sich die übliche Notation des Tableaukalküls wieder. Die
rechte Darstellung zeigt das Resultat dieser Umgestaltung. Man mag das Schließen
von Sequenzen insofern als vielseitig bewerten. Die dazugewonnene Sichtweise
schafft überdies ein tiefergründiges Verständnis des Tableaukalküls.


\newpage
\section{Logik mit Gleichheit}

\subsection{Axiome der Gleichheit}

Im Folgenden Abschnitt geht es um allgemeingültige Erwägungen zur
Gleichheit. Das wären Gesetzmäßigkeiten, die die Gleichheit immer
erfüllen soll, ungeachtet, ob sie zwischen zwei Zahlen, zwei Mengen,
oder zwei wie auch immer gearteten Objekten besteht.

Moderne Formulierungen charakterisieren die Gleichheit durch die Axiome
\begin{align*}
&\vdash\forall x\colon x=x, &&\text{(Reflexivität)}\\
&\vdash\forall x\colon\forall y\colon x=y\cond A(x)\cond A(y).
&&\text{(Ersetzung)}
\end{align*}
Das zweite Axiom stellt eigentlich ein Schema dar, weil dieses für jede
Formel $A$ gilt. Mit $A(u)$ sei hierbei gemeint, dass in $A$ die ungenannte
Variable $u$ frei vorkommen darf, wobei $A(t)$ als $A(u)[u:=t]$ für einen
Term $t$, einschließlich $t=x$ und $t=y$, zu verstehen sein soll. Man gewinnt
aus dem Schema kurzerhand die Regel
\[\dfrac{\Gamma\vdash t=t'\qquad\Gamma'\vdash A(t)}
{\Gamma,\Gamma'\vdash A(t')}.\qquad\text{($t,t'$ sind beliebige Terme)}\]
Das erste Axiom charakterisiert insofern die Regel zur Einführung
der Gleichheit, das zweite die Regel zur Beseitigung. Die Symmetrie
der Gleichheit lässt sich aus den beiden Regeln ableiten. Sei hierzu
$A(u):\bicond (u=x)$. Nun ist $A(x)\bicond (x=x)$ und
$A(y)\bicond (y=x)$. Man setze $t:=x$ und $t':=y$. Es findet sich:
\[
\begin{prooftree}
  \infer0{x=y\vdash x=y}
  \infer0{\vdash x=x}
\infer2{x=y\vdash y=x}
\end{prooftree}
\]
Aus dieser Sequenz erhält man anschließend
\[\vdash\forall x\colon\forall y\colon x=y\cond y=x.\]
Bezüglich $A(u):\bicond (f(x)=f(u))$ führt die Ausübung der soeben
gemachten Vorgehensweise auf
\[\vdash\forall x\colon\forall y\colon x=y\cond f(x)=f(y).\]
Es induziert die Ersetzungsregel für Funktionen,
\[\dfrac{\Gamma\vdash t=t'}{\Gamma\vdash f(t)=f(t')}.\]
Zu beachten wäre allerdings, dass $f$ hierfür auf dem gesamten
Diskursuniversum definiert sein muss. Würde das Symbol $f$ mit einer
Funktion belegt, die für eine bestimmte Belegung von $x$ nicht definiert
ist, was soll $f(x)$ dann sein?

Mehrmalige Anwendung des Ersetzungsaxioms ermöglicht darüber hinaus
mehrstellige Ersetzungen wie
\begin{gather*}
\vdash\forall x,x',y,y'\colon x=x'\land y=y'\cond A(x,y)\cond A(x',y'),\\
\vdash\forall x,x',y,y'\colon x=x'\land y=y'\cond f(x,y)=f(x',y').
\end{gather*}
Ferner implizieren die Axiome das Transitivgesetz
\[\vdash\forall x,y,z\colon x=y\land y=z\cond x=z.\]
Es steht $\forall x,y,z\colon A$ als Abkürzung für
$\forall x\colon\forall y\colon\forall z\colon A$.

Ganz allgemein gilt
\[\vdash\forall x\colon\forall y\colon x=y\cond s(x)=s(y)\]
für jeden Term $s$. Dies bestätigt sich unschwer folgendermaßen.
Sei $h$ eine frische Hilfsvariable, die nicht
frei in $s$ vorkommt und
\[A:\bicond (s[u:=x]=s[u:=h]),\]
wobei $s(x)=s[u:=x]$ und $s(y)=s[u:=y]$ ist. Der Schluss
\[\dfrac{\Gamma\vdash x=y\quad\vdash A[h:=x]}{\Gamma\vdash A[h:=y]}\]
vereinfacht sich nun zu
\[\dfrac{\Gamma\vdash x=y\quad\;\overline{\vdash s[u:=x]=s[u:=x]}}{\Gamma\vdash s[u:=x]=s[u:=y]},
\;\;\text{kurz}\;\;\dfrac{\Gamma\vdash x=y\quad\;\overline{\vdash s(x)=s(x)}}{\Gamma\vdash s(x)=s(y)}.\]
Es genügt übrigens, das Ersetzungsaxiom für atomare Aussageformen zu fordern.
Seien hierzu $P,Q$ Prädikate. Sei $A(x)$ zum Beispiel die Formel $P(x)\land Q(x)$.
Dann ist die Regel
\[\dfrac{\Gamma\vdash x=y\qquad \Gamma'\vdash A(x)}{\Gamma,\Gamma'\vdash A(y)}\]
zulässig, denn:
\[\begin{prooftree}
    \hypo{\Gamma\vdash x=y}
      \hypo{\Gamma'\vdash P(x)\land Q(x)}
    \infer1{\Gamma'\vdash P(x)}
  \infer2{\Gamma,\Gamma'\vdash P(y)}
    \hypo{\Gamma\vdash x=y}
      \hypo{\Gamma'\vdash P(x)\land Q(x)}
    \infer1{\Gamma'\vdash Q(x)}
  \infer2{\Gamma,\Gamma'\vdash Q(y)}
\infer2{\Gamma,\Gamma'\vdash P(y)\land Q(y)}
\end{prooftree}\]
Für die anderen Junktoren klappt es analog. Per struktureller Induktion
über den Formelaufbau bestätigt sich die Regel daraufhin in allgemeiner
Weise als zulässig.

\subsection{Von der Identität des Ununterscheidbaren}

Dem Gleichheitsbegriff wohnt das \emph{Principium
identitatis indiscernibilium} inne, das \emph{Prinzip der Identität des
Ununterscheidbaren}, englisch \emph{Identity of indiscernibles}.
Es besagt, dass man keine zwei ungleichen Objekte finden kann,
die in allen ihren Eigenschaften übereinstimmen. Man nennt es auch
\emph{Gleichheit nach Leibniz}, weil Wilhelm Gottfried
Leibniz dieses im 27. Kaptiel von
\emph{Nouveaux Essais sur L'entendement humain II}
im Bezug zum Kosmos beschrieb. Am Ende findet man das Wesentliche
nochmals in fasslicher, bildhafter Form,

\begin{quote}
»Ich erinnere mich, dass eine große Prinzessin, die von erhabenem Geist
ist, einmal sagte, als sie in ihrem Garten spazieren ging, dass sie
nicht glaube, dass es zwei vollkommen ähnliche Blätter gebe. Ein
geistreicher Herr, der mit auf dem Spaziergang war, glaubte, dass es
leicht sein würde, solche zu finden; aber obwohl er viel suchte, wurde
er durch seine Augen davon überzeugt, dass man immer einen Unterschied
bemerken könne.«
\end{quote}

\noindent
Formalisierung erfährt das Prinzip durch die Aussage
\[(\forall P\colon P(x)\bicond P(y))\cond x=y.\]
Man sollte bedenken, dass diese Formulierung die Prädikatenlogik
zweiter Stufe erfordert, da hier über Prädikate quantifiziert wird.
Die Umkehrung
\[x=y\cond (\forall P\colon P(x)\bicond P(y)).\]
wird als unbedenklich angesehen, so dass man das Prinzip auch als
Äquivalenz formuliert. Die Umkehrung ist fast trivial aus den
Axiomen ableitbar, weil es sich bereits um eine gewisse Form des
Ersetzungsaxioms handelt. Umgekehrt können wir aus der Äquivalenz die
beiden Axiome zurückgewinnen. Reflexivität besteht offenkundig, weil
$P(x)$ immer äquivalent zu $P(x)$ ist. Und zum Ersetzungsaxiom wurde
bereits ausgeführt, dass es genügt, dieses für Prädikate zu fordern.

Es verbleibt zu untersuchen, ob das Prinzip aus den Axiomen
herleitbar ist. Hierzu wird $P(u):=(x=u)$ als Prädikat gewählt, womit
$x=x$ als äquivalent zu $x=y$ vorausgesetzt wird. Weil $x=x$ gemäß
Reflexivität vorliegt, erhält man wie gewünscht $x=y$.

\newpage
\subsection{Eindeutige Existenz}

Gelegentlich tritt in der Mathematik eine Aussage der eindeutigen
Existenz auf, dass heißt, eine Aussage, laut der das Objekt mit der
geforderten Eigenschaft existiert und zudem eindeutig festgelegt ist.
Beispielsweise existiert bei einer Funktion zu jedem Argument genau ein
Wert. Weiterhin tritt bei sogenannten universellen Eigenschaften die
Forderung eindeutiger Existenz auf. Sie führen in die Kategorientheorie,
und der Leser mag daraus schließen, auch wenn er noch nie von ihr gehört
hat, dass diese anscheinend die Prädikatenlogik mit Gleichheit als
logisches System enthalten muss. Zumindest brauchen wir sie für die
folgenden Ausführungen.

Formal ist die eindeutige Existenz fassbar als der Quantor
\[(\exists! x\colon A(x))\,:\bicond\,
(\exists x\colon A(x)\land\forall y\colon A(y)\cond x=y).\]
Diese Aussage ist in Existenz und Eindeutigkeit zerlegbar gemäß
\begin{Satz}\label{eindeutige-Existenz-separat}
Es gilt die Äquivalenz
\[(\exists! x\colon A(x))\,\bicond\,
(\exists x\colon A(x))\land(\forall x,y\colon A(x)\land A(y)\cond x=y).\]
\end{Satz}
\begin{Beweis}
Die linke Seite gelte. Dann liegt ein $u$ mit $A(u)$
und $\forall y\colon A(y)\cond u=y$ vor. Der Existenzaussage wird
mit $x:=u$ genügt. Die Eindeutigkeit geht via
\[(\forall y\colon A(y)\cond u=y), A(x), A(y)\vdash x=y.\]
Wir spezialisieren die gegebene Allaussage einmal mit $y:=x$ und einmal
mit $y:=y$. Mit $A(x)$ und $A(y)$ bekommt man daraufhin $u=x$ und $u=y$. Ergo
folgt $x=y$ per Symmetrie und Transitivität der Gleichheit.

Die rechte Seite gelte. Dann liegt ein $u$ mit $A(u)$ vor. Wir wählen
$x:=u$ für die Existenzaussage. Die Aussage $A(u)$ liegt bereits vor.
Verbleibt $\forall y\colon A(y)\cond u=y$ zu bestätigen. Wir nehmen
also $A(y)$ an. Aus der Spezialisierung der gegebenen Allaussage mit
$x:=u$ und $y:=y$ wird $u=y$ via $A(u)\land A(y)$ abgetrennt.\,\qedsymbol
\end{Beweis}

\noindent
Vorsorglich erwähnen möchte ich
\[(\forall x,y\colon A(x)\land A(y)\cond x=y)\bicond
(\forall x\colon A(x)\cond\forall y\colon A(y)\cond x = y).\]
Der Beweis sollte nicht viel Mühe bereiten.

\newpage
\section{Induktion}

\subsection{Einfache Induktion}

In der Philosophie bezeichnet man als \emph{Induktion} eine Art von
Schlussfolgerung, die da ist der Schluss vom Speziellen auf das
Allgemeine. Folgendes Beispiel verdeutlicht die Idee der Überlegung. Ein
Gegenstand wird einmal fallen gelassen, man beobachtet wie dieser zu
Boden fällt. Wiederholung des Experiments führt abermals zum selben
Resultat. Induktiv schließt man daraus, dass dieses Resultat \emph{immer}
eintreten wird. Jedoch kann Induktion zu Fehlschlüssen führen, weshalb
es nicht als mathematisches Beweisverfahren brauchbar ist. Nur weil
bereits dreimal eine Toastbrotscheibe auf die Marmeladenseite gefallen
ist, heißt das nicht, dass dieses Resultat immer eintreten müsse. In der
Mathematik bieten die Borwein"=Integrale ein prägnantes Beispiel, wo
leichtfertige Argumentation verfänglich wäre.

Mit der bedenklichen Induktion in der Philosophie teilt sich die
\emph{vollständige Induktion} den Namen. Sie ist allerdings unfehlbar.
Das Verfahren ist für die moderne Mathematik und Informatik von
wesentlicher Bedeutung.

Die vollständige Induktion\index{Induktion}%
\index{vollstaendige Induktion@vollständige Induktion}
wird vermittelt durch das Axiomenschema
\[\vdash A(0)\land (\forall n\in\N\colon A(n)\cond A(n+1))
\cond (\forall n\in\N\colon A(n)).\]
Es induziert die Regel
\[\dfrac{\Gamma\vdash A(0)\qquad\Gamma',n\in\N,A(n)\vdash A(n+1)}{
\Gamma,\Gamma'\vdash\forall n\in\N\colon A(n)}(n\notin\mathrm{FV}(\Gamma')).\]
Zur Veranschaulichung wird gerne eine endlose Dominoreihe herangezogen.
Fällt der erste Dominostein um, und ist sicher, dass mit einem
Dominostein ebenso dessen Nachfolger umfällt, muss \emph{jeder}
Dominostein irgendwann umfallen.

Man bezeichnet $A(0)$ als den \emph{Induktionsanfang}\index{Induktionsanfang}.
Die Ableitung von $A(n+1)$ aus $A(n)$ heißt \emph{Induktionsschritt}\index{Induktionsschritt},
wobei $A(n)$ darin die Bezeichnung \emph{Induktionsvoraussetzung}\index{Induktionsvoraussetzung} trägt.

Ein erstes Beispiel. Man definiert die Potenz einer Zahl $a$ rekursiv als
\[a^0 := 1,\qquad a^{n+1} := aa^n.\]
Zu beweisen sei das Potenzgesetz
\[A(n)\,:\bicond\, (ab)^n = a^n b^n.\]
Der Anfang $A(0)$ bestätigt sich via
\[(ab)^0 = 1 = 1\cdot 1 = a^0 b^0.\]
Den Induktionsschritt $(A(n)\cond A(n+1))$ bestätigt
die Rechnung
\[(ab)^{n+1}\stackrel{\text{(1)}}= (ab)(ab)^n
\stackrel{\text{IV}}= aba^n b^n = aa^n bb^n
\stackrel{\text{(2)}}= a^{n+1} b^{n+1}.\]
Die Stelle, wo $A(n)$ zur Anwendung kam, wurde mit IV annotiert,
was für \emph{Induktionsvoraussetzung} steht. Die Umformungen
(1), (2) gelten laut Definition.

Bislang trat die Induktion so auf, dass der Anfang in der
Zahl Null liegt. Aus der Regel folgt aber bereits, dass man den
Anfang in jede beliebige natürliche Zahl legen kann.
\begin{Satz}
Es gilt für $n,n_0\in\N$ das allgemeine Schema
\[A(n_0)\land (\forall n\ge n_0\colon A(n)\cond A(n+1))
\cond (\forall n\ge n_0\colon A(n)).\]
\end{Satz}
\begin{Beweis}
Die Prämisse wird spezialisiert mit $n:=u+n_0$, wobei $u$ beliebig sein
darf, da $u+n_0\ge n_0$ für jedes $u\in\N$
erfüllt ist. Es sei nun $B(u):\bicond A(u+n_0)$. Demnach ist
$A(n_0)$ äquivalent zu $B(0)$. Und es ist $A(u+n_0+1)$ äquivalent
zu $B(u+1)$. Insgesamt erhält man aus der Prämisse also
\[B(0)\land (\forall u\colon B(u)\cond B(u+1)).\]
Per herkömmlicher Induktion gilt also $B(u)$ bzw. $A(u+n_0)$ für
jedes natürliche $u$. Wird dies nun mit der Resubstitution $u:=n-n_0$
mit $n\ge n_0$ spezialisiert, erhält man $A(n)$, und somit schließlich die
gesuchte Konklusion $(\forall n\ge n_0\colon A(n))$.\,\qedsymbol
\end{Beweis}

\noindent
Spezialisieren ist hier formal zu verstehen. Das Resultat
der Substitution ist eigentlich nicht weniger allgemein als die
ursprüngliche Aussage.

Befasst man sich im Weiteren mit der Mengenlehre, wo gemeinhin mit
Mengen argumentiert wird, bietet sich an, auch die Induktion bezüglich
einer Menge zu formulieren. Sei hierzu $M\subseteq\N$ definiert als
die Aussonderung
\[M := \{n\in\N\mid A(n)\}.\]
Mit $A(n)\bicond n\in M$ nimmt das Schema der Induktion daraufhin die Form
\[\vdash 0\in M\land (\forall n\in\N\colon n\in M\cond n+1\in M)\cond M = \N\]
an. Man verwendet diese Variante vorwiegend, um die axiomatische
Charakterisierung der natürlichen Zahlen in Bezug zur Mengenlehre zu
bringen. Sie wurde Ende des 19. Jahrhunderts von Richard Dedekind und
Giuseppe Peano erdacht. Das Axiom der Induktion ist darin als letzte
und komplizierteste. Es lautet
\[\vdash \forall P\colon P(0)\land (\forall n\colon P(n)\cond P(S(n)))
\cond (\forall n\colon P(n)).\]
Hiermit wird allerdings die Prädikatenlogik zweiter Stufe vorausgesetzt,
weil über alle Prädikate $P$ quantifiziert wird. Ein Verzicht auf die
Logik zweiter Stufe ist aber möglich, indem das Axiom wie zuvor als
Schema formuliert wird. Das so in die Prädikatenlogik erster Stufe gebrachte
System nennt man die \emph{Peano"=Arithmetik}.

\subsection{Starke Induktion}

Die starke Induktion verstärkt die Induktionsvoraussetzung
dahingehend, dass sie nicht nur für den unmittelbaren Vorgänger, sondern
für sämtliche Vorgänger vorausgesetzt werden darf. Damit wird der
Induktionsbeweis unter Umständen erleichtert. Die starke muss nicht
extra axiomatisch gefordert werden, sie ist aus der herkömmlichen
ableitbar.
\begin{Satz}[Starke Induktion]\index{Induktion!starke}\index{starke Induktion}
Es gilt das Schema
\[A(0)\land (\forall n\in\N\colon (\forall k\le n\colon A(k))\cond A(n+1))
\cond (\forall n\in\N\colon A(n))\]
\end{Satz}
\begin{Beweis}
Sei hierzu
\[C(n) :\bicond (\forall k\le n\colon A(k)).\]
Die Prämissen seien angenommen. Es ist $A(0)$
gleichbedeutend mit $C(0)$. Wir überzeugen uns nun von der Folgerung
\[(\forall n\in\N\colon C(n)\cond A(n+1))\cond (\forall n\in \N\colon C(n)\cond C(n+1)).\]
Sei also $n\in\N$ und $C(n)$ angenommen, dann haben wir auch $A(n+1)$
und somit $C(n)\land A(n+1)$, was äquivalent zu $C(n+1)$ ist. Per
herkömmlicher Induktion gilt also $C(n)$ für jedes natürliche
$n$. Aus $C(n)$ folgt aber $A(n)$, womit erst recht $A(n)$ für jedes
natürliche $n$ gilt.\,\qedsymbol
\end{Beweis}

\noindent
Ein wenig eleganter formuliert sich das Schema auch in der Form
\[(\forall n\in\N\colon (\forall k < n\colon A(k))\cond A(n))
\cond (\forall n\in\N\colon A(n)).\]
Weil keine natürliche Zahl kleiner als null existiert, greift hier
das Prinzip der leeren Wahrheit, womit $A(0)$ trotzdem zu bestätigen
ist.

Aus der starken Induktion leitet sich ein Sachverhalt ab, der von
der Anschauung her evident erscheinen mag.

\begin{Satz}[Wohlordnungsprinzip]%
\label{Wohlordnungsprinzip}\index{Wohlordnungsprinzip}\newlinefirst
Jede nichtleere Teilmenge von $\N$ besitzt ein
kleinstes Element.
\end{Satz}
\begin{Beweis}
Gegenstand der Betrachtung sei die Menge $M\subseteq\N$. Mit
$A(n):\Leftrightarrow n\notin M$ ergibt sich aus dem Schema der starken
Induktion die Aussage
\[(\forall n\in\N\colon (\underbrace{\forall k<n\colon k\notin M)\cond n\notin M}
_{\lnot(\forall k<n\colon k\notin M)\lor n\notin M})
\cond (\underbrace{\forall n\in\N\colon n\notin M}_{M=\emptyset}).\]
Deren Kontraposition ist
\[M\ne\emptyset\cond (\exists n\in\N\colon (\forall k<n\colon k\notin M)\land n\in M).\]
Mit $M\subseteq\N$ ergibt sich in der Konklusion die Verkürzung
\[n\in\N\land n\in M\iff n\in\N\cap M = M.\]
Unternimmt man des Weiteren die Umformung
\begin{align*}
(\forall k<n\colon k\notin M)\iff (\forall k\in M\colon n\le k),
\end{align*}
gelangt man schließlich zur Behauptung
\[M\ne\emptyset\cond (\exists n\in M\colon\forall k\in M\colon n\le k).\,\qedsymbol\]
\end{Beweis}

\noindent
Insofern im Beweis ausschließlich Äquivalenzumformungen unternommen
wurden, stellt sich heraus, dass das Wohlordnungsprinzip
zur starken Induktion gleichwertig ist. Da in die Aussageform
$A(n)$ per se nur natürliche Zahlen $n$ eingesetzt werden, erhält man
nämlich mit der Festlegung
\[M := \{n\in\N\mid\lnot A(n)\}\]
das allgemeine Schema zurück.

Wird zum Wohlordnungsprinzip neuerlich die Kontraposition
gebildet, jedoch ohne weitergehende Umformungen vorzunehmen, gelangt man zu
\[(\forall n\in M\colon\exists k\in M\colon k<n)\cond M = \emptyset.\]
Dieses Argument kann alternativ zur starken Induktion verwendet
werden. In Worten betrachtet man die hypothetische Menge $M$
der Elemente, die die Aussageform $A(n)$ nicht erfüllen. Kann gezeigt
werden, dass zu jedem $n\in M$ ein $k\in M$ mit $k<n$ existieren würde,
muss $M$ leer sein, da sich andernfalls ein Widerspruch ergäbe, da $M$
als Teilmenge der natürlichen Zahlen ein kleinstes Element besäße.
Ist $M$ leer, gilt $A(n)$ für jede natürliche Zahl $n$.

Statt vom Wohlordnungsprinzip sprechen manche vom \emph{Prinzip des
kleinsten Elements}, um eine Verwechslung mit dem \emph{Wohlordnungssatz}
zu vermeiden, einem Satz der Mengenlehre, nach welchem jede
Menge wohlgeordnet werden kann.

\subsection{Strukturelle Induktion}\label{sec:Strukturelle-Induktion}

Die herkömmliche Induktion verläuft über die natürlichen Zahlen.
Sie beginnt ohne Beschränkung der Allgemeinheit in der Null, und setzt
sich dann schrittweise auf den Nachfolger fort. Es wird also $A(0)$
bestätigt, und $A(n+1)$ unter Annahme von $A(n)$.

Strukturelle Induktion verläuft allgemeiner über Knoten. Es gibt ein
oder mehrere Anfangsknoten $v_0$, für die jeweils der Induktionsanfang $A(v_0)$
zu bestätigen ist. Außerdem liegen Regeln vor, wie aus bereits vorhandenen
Knoten neue Knoten abzuleiten sind. Die neuen Knoten nehmen die Rolle der
Nachfolger ein. So entsteht ein Baum oder ein gerichteter Graph.
Leitet sich $v$ aus $v_1$ bis $v_n$ ab, so besteht der zugehörige
Induktionsschritt darin, dass $A(v)$ unter Annahme von $A(v_1)$
bis $A(v_n)$ gezeigt wird. Bei einem Baum verläuft die Induktion von
den Blättern aus zu einer Wurzel hin. Die gewöhnliche Induktion verlief
von der Null zu einer Zahl hin. Wurde alles gezeigt, ist die Induktion
also \emph{vollständig}, darf die Wurzel wie die Zahl beliebig sein.

Ich will versuchen, das Prinzip so mit Dominosteinen zu veranschaulichen,
wie die herkömmliche Induktion als Fallen einer Dominoreihe verständlich
wird. Man kann sich eine Regel als ein Plättchen bestimmter Art vorstellen.
Sofern alle notwendigen Dominosteine das Plättchen erreichen, fällt es
um, womit auch der auf das Plättchen folgende Dominostein umfällt.
Abermals erreichen notwendige Steine, worunter auch oder nur Nachfolger
zu finden sind, ein weiteres Plättchen, worauf auch dieses fällt.
Das läuft zumindest solange, bis man das gewünschte Fallen des
begehrten Wurzelsteins beobachtet. Als dynamischer Prozess gesehen,
hat man hier eine kausale Struktur, bei der Wirkungen nur unter einer
oder mehreren passenden Ursachen auftreten.

Die herkömmliche Induktion stellt einen Spezialfall der strukturellen
Induktion dar, bei der die Bäume endliche Listen sind. Bei
der strukturellen Induktion über den Formelaufbau verläuft die Induktion
über Formeln. Sie beginnt in den atomaren Formeln. Die Regeln zur
Ableitung sind die Produktionsregeln. Bei der strukturellen Induktion
über die Konstruktion eines Beweises verläuft die Induktion über
Sequenzen. Sie beginnt in den Grundsequenzen. Die Regeln zur Ableitung
sind die Schlussregeln.

Ein weiteres Beispiel bietet die Induktion über die Punkte des
Gitters $\Z_{\ge 0}\times Z_{\ge 0}$. Sagen wir, der einzige
Anfang ist $A(0,0)$. Die erste Schrittweise bestehe in $A(m+1,n)$
unter Annahme $A(m,n)$. Die zweite Schrittweise bestehe in $A(m,n+1)$
unter Annahme $A(m,n)$. Diese Art von struktureller Induktion ist
gleichwohl gegen die herkömmliche ersetzbar. Der Beweis untergliedert
sich dabei in zwei Induktionsbeweise. Es wird zunächst
$A(m,0)$ für jedes $m$ bestätigt und $A(m,0)$ anschließend als
Anfang für $A(m,n)$ benutzt. Der zweite Induktionsbeweis ist so gesehen
durch $m$ parametrisiert, was nichts Schlimmes ist, denn
parametrisierte Argumentation kennzeichnet ja das übliche Vorgehen
zur Bestätigung allquantifizierter Aussagen.

Das folgende Beispiel zeigt den typischen Charakter struktureller
Induktion besser. Es sei $A$ ein Alphabet von Symbolen. Die Menge $L$
der \emph{einfach verketteten Listen}
von Symbolen sei wie folgt als induktive Struktur definiert. Es sei
$\emptyset\in L$ und mit $a\in L$ und einem Symbol $x\in A$ auch
$f_x(a)\in L$ mit $f_x(a):=(x,a)$. Nichts anderes sei eine Liste. Bei
einer Liste der Form des geordneten Paars $(x,a)$ stellt $x$ das
anfängliche Element dar und $a$ die restliche Liste. Konstruktionen
dieser Art sind in den Grundzügen der Informatik geläufig. Zwar mögen sie
ohne Verfügbarkeit von weitgehenden Optimierungen ineffizient sein, dafür
stellen sie aber wichtige Hilfsmittel für die Theorie dar. In der
Programmiersprache Scheme schreibt man beispielsweise \verb|'()| statt
$\emptyset$ und \verb|(cons x a)| statt $f_x(a)$.

Die Länge $l(a)$ einer Symbolliste $a$ definieren wir nun rekursiv als
\[l(a) := \begin{cases}
0, & \text{wenn}\; a=\emptyset,\\
l(a_1)+1, & \text{wenn}\; a = (a_0,a_1).
\end{cases}\]
Genau genommen muss die induktive Struktur $L$ diesbezüglich, damit diese Funktion
unbestreitbar wohldefiniert ist, durch $\emptyset$ und die $f_x$ frei
erzeugt sein, siehe Def. \ref{def:frei-erzeugt} auf S. \pageref{def:frei-erzeugt},
was sich aber unschwer bestätigen lässt. Die Konkatenation zweier Listen $a,b$
definieren wir auf dieselbe Weise rekursiv als
\[a\circ b := \begin{cases}
b, & \text{wenn}\; a=\emptyset,\\
(a_0, (a_1\circ b)), & \text{wenn}\; a=(a_0,a_1).
\end{cases}\]
Es sollte klar sein, dass für je zwei Listen $a,b$ gilt
\[l(a\circ b) = l(a) + l(b).\]
Der strenge Beweis wird nun aber via struktureller Induktion über den
Aufbau von $a$ geführt. Im Induktionsanfang $a=\emptyset$ erhält man
\[l(\emptyset\circ b) = l(b) = 0 + l(b) = l(\emptyset) + l(b).\]
Zum jeweiligen Induktionsschritt bezüglich $x\in A$ findet sich die
Rechnung
\begin{align*}
l((x, a)\circ b) &= l((x, a\circ b)) = l(a\circ b) + 1
\stackrel{\text{IV}}= l(a) + l(b) + 1\\
&= l((x, a)) + l(b).
\end{align*}

\newpage
\section{Modallogik}\label{sec:Modallogik}

\subsection{Das System K}

Die Modallogik handelt von den Weisen, wie eine Aussage ausgeprägt sein
kann, was durch modalisierende Operatoren ausgedrückt wird. Es gibt
unterschiedliche Systeme der Modallogik. In vielen Systemen finden sich
zwei Modalitäten, die \emph{Notwendigkeit} der Aussage und die
\emph{Möglichkeit} der Aussage. Allerdings artikulieren diese Sprechweisen
nur die \emph{alethische} Deutung der Modalitäten. Je nach System und
Anwendung sind unterschiedliche Deutungen der Modalitäten zuträglich.

Ist $A$ die Aussage »Es regnet«, drückt $\lnec A$ die Aussage
»Es regnet notwendigerweise« und $\lpos A$ die Aussage
»Es regnet möglicherweise« aus.

Die Modallogik scheint wichtiger für Philosophen als für
Mathematiker. Indessen kamen mit der Zeit Anwendungen in der
Mathematik und der Informatik zum Vorschein. Dem Basiswissen besonders
dienlich ist meines Erachtens die \emph{dynamische Logik} mit
ihrer engen Beziehung zum Hoare"=Kalkül bzw. zum dijkstraschen wlp"=Kalkül.
Diese Kalküle geben uns die Mittel in die Hand, Algorithmen auf ihre Korrektheit
hin zu untersuchen. Die Funktion, die der Algorithmus darstellt, bekommt
hierzu eine \emph{Spezifikation}, bestehend aus einer \emph{Vorbedingung}
und einer \emph{Nachbedingung}. Man zeigt nun, dass der Algorithmus auch
das tut, was er soll, dergestalt dass der Wert der Funktion immer die
Nachbedingung erfüllt, sofern ihre Argumente die Vorbedingung erfüllen.

Der Modallogik kommt somit auch eine gewisse Bedeutung für die praktische
Arbeit zu, nicht nur für höhere mathematische respektive philosophische
Erwägungen und Betrachtungen.

Wir wollen uns zunächst mit dem \emph{System K} beschäftigen, dem
Grundsystem der \emph{normalen Modallogiken}. Spezifischere Systeme
entstehen durch Hinzunahme weiterer Axiome. Das System K enthält
sämtliche Regeln und Axiome der klassischen Aussagenlogik. Hinzu kommt
die Regel
\[\dfrac{\vdash A}{\vdash\lnec A}\qquad\text{(Nezessisierungsregel)}\]
und das Schema
\[\vdash\lnec(A\cond B)\cond (\lnec A\cond\lnec B).\qquad\text{(Axiomenschema K)}\]
Wie gehabt induziert das Schema sogleich eine zulässige Regel,
\[\dfrac{\Gamma\vdash\lnec(A\cond B)}{\Gamma\vdash\lnec A\cond\lnec B}.\qquad\text{(Regel K)}\]
Wichtig ist, dass nur Theoreme Nezessisierung erfahren dürfen.
Dagegen ist
\[\dfrac{a\vdash a}{a\vdash\lnec a}\qquad \text{(verboten)}\]
ein unzulässiger Schluss. So ist $a\cond\lnec a$ kein Theorem. Man wird
mit der Semantik für das System K leicht ein Gegenmodell dieser Formel
finden.

Man definiert $\lpos A$ als äquivalent zu $\lnot\lnec\lnot A$.

Statt der einfachen Nezessisierung und Schema K kann man auch eine
einzige allgemeine Nezessisierungsregel\index{Nezessisierungsregel}
voraussetzen. Mit $n\ge 0$ lautet sie
\[\dfrac{A_1,\ldots,A_n\vdash B}{\lnec A_1,\ldots,\lnec A_n\vdash\lnec B}.\]
Man beweist die Regel per Induktion über $n$ als zulässig. Im Anfang $n=0$
nimmt sie schlicht die Form der Nezessisierungsregel an. Der
Induktionsschritt wird durch den Beweisbaum
\[
\begin{prooftree}
        \hypo{A_1,\ldots,A_n,A_{n+1}\vdash B}
      \infer1{A_1,\ldots,A_n\vdash A_{n+1}\cond B}
    \infer1[IV]{\lnec A_1,\ldots,\lnec A_n\vdash\lnec (A_{n+1}\cond B)}
  \infer1[K]{\lnec A_1,\ldots,\lnec A_n\vdash\lnec A_{n+1}\cond\lnec B}
  \infer0{\lnec A_{n+1}\vdash\lnec A_{n+1}}
\infer2[Modus ponens]{\lnec A_1,\ldots,\lnec A_n,\lnec A_{n+1}\vdash\lnec B}
\end{prooftree}
\]
bestätigt.

Auch dem Fitch"=Style wurde eine Darstellung der Schlüsse der Modallogik
hinzugefügt. Sie ist meines Erachtens etwas schwieriger zu durchschauen
als das Schließen von Sequenzen. Ich will in diesem Buch nicht näher
darauf eingehen.

\subsection{Das System S4}

\begin{table}
\begin{center}
\caption{Übersicht über zusätzliche Schemata}
\label{tab:Modallogik-Schemata}
\begin{tabular}{clll}
\toprule
& \strong{Schema} & \strong{Relationen} & \strong{Formel}\\
\midrule[\heavyrulewidth]
K & $\lnec (A\cond B)\cond(\lnec A\cond\lnec B)$ & sämtliche & keine Einschränkung\\
T & $\lnec A\cond A$ & reflexive & $\forall x\colon R_{xx}$\\
B & $A\cond \lnec\lpos A$ & symmetrische & $\forall x,y\colon R_{xy}\cond R_{yx}$\\
D & $\lnec A\cond\lpos A$ & serielle & $\forall x\colon\exists y\colon R_{xy}$\\
4 & $\lnec A\cond\lnec\lnec A$ & transitive & $\forall x,y,z\colon R_{xy}\land R_{yz}\cond R_{xz}$\\
5 & $\lpos A\cond\lnec\lpos A$ & euklidische & $\forall x,y,z\colon R_{xy}\land R_{xz}\cond R_{yz}$\\
\bottomrule
\end{tabular}
\end{center}
\end{table}
Das System S4 formt sich aus den Schemata KT4, siehe Tabelle
\ref{tab:Modallogik-Schemata}.

Die Übersetzung nach Gödel-McKinsey-Tarski ist
\begin{align*}
P' &= \lnec P, & (A\land B)' &= A'\land B', & (A\cond B)' &= \lnec (A'\cond B'),\\
\bot' &= \bot, & (A\lor B)' &= A'\lor B', & (\lnot A)' &= \lnec\lnot A'.
\end{align*}
Man deutet $\lnec A$ als »A ist beweisbar«. Es ist $A$ genau
dann ein Theorem der intuitionistischen Logik, wenn $A'$ ein
Theorem im System S4 ist.

\section{Beweistheoretische Überlegungen}

\subsection{Ableitbarkeit}

Auf das Urteil $\Gamma\vdash A$ blickt man aus zwei Sichtweisen. Zum
einen stellt es als Sequenz zunächst ein syntaktisches Konstrukt dar,
das Gegenstand eines formalen Systems ist. Zum anderen kommt darin ja die
metasprachliche Aussage zum Ausdruck, dass $A$ aus $\Gamma$ ableitbar sei.
Zur Schaffung von Klarheit wollen wir näher zwischen den beiden Sichtweisen
unterscheiden. Dazu wird die metalogische Aussage mit einem Index versehen,
der darüber informiert, welches logische System betrachtet wird.
Wir notieren zum Beispiel $\Gamma\vdash_\mathrm{M} A$ sowie $\Gamma\vdash_\mathrm{I} A$
und $\Gamma\vdash_\mathrm{C} A$ für die jeweilige Aussage bezüglich der
minimalen, intuitionistischen und klassischen Logik. Insofern Sequenzen
allerdings lediglich für endliche Kontexte erklärt sind, wird die
folgende Erklärung vorgenommmen.

\begin{Definition}[Ableitbarkeit]\label{def:Ableitbarkeit}\newlinefirst
Man nennt $A$ im System $S$ aus der Formelmenge $\Gamma$
ableitbar, kurz $\Gamma\vdash_S A$, wenn eine endliche Menge
$\Gamma_0\subseteq\Gamma$ existiert, so dass die Sequenz $\Gamma_0'\vdash A$
ableitbar ist, wobei mit $\Gamma_0'$ die syntaktische Entsprechung von
$\Gamma_0$ gemeint sei.
\end{Definition}

\noindent
Sofern Kontexte Listen sind, stellt $\Gamma_0'$ die Umwandlung von
$\Gamma_0$ in eine Liste mit denselben Elementen dar. Im Folgenden
soll nicht weiter pedantisch zwischen $\Gamma_0$ und $\Gamma_0'$
unterschieden werden, da unzweideutig aus dem Zusammenhang hervorgeht,
was gemeint ist.

Zu einer endlichen Formelmenge $\Gamma_0$ gilt $\Gamma_0\vdash_S A$
demzufolge genau dann, wenn die Sequenz $\Gamma_0\vdash A$
ableitbar ist, symbolisch%
\[(\Gamma_0\vdash_S A)\,\bicond\, (\vdash_S \Gamma_0\vdash A).\]
Für die Ableitbarkeit besteht damit verbunden Kompaktheit, dergestalt
dass $\Gamma\vdash_S A$ genau dann gilt, wenn ein endliches
$\Gamma_0\subseteq\Gamma$ mit $\Gamma_0\vdash_S A$ existiert.

Weiterhin sind die Schlussregeln nun als metasprachliche Aussagen
fassbar. So ist die Beseitigung der Subjunktion beschrieben durch%
\[(\Gamma\vdash_{\mathrm C} A\cond B)\land (\Gamma'\vdash_{\mathrm C} A)
\cond (\Gamma\cup\Gamma'\vdash_{\mathrm C} B).\]
Anders als in der Regel dürfen $\Gamma,\Gamma'$ hierbei auch
unendlich sein. Eleganter finde ich hier aber, die Regel in der
exportierten bzw. geschönfinkelten Form
\[(\Gamma\vdash_{\mathrm C} A\cond B)\cond (\Gamma'\vdash_{\mathrm C} A)\cond
(\Gamma\cup\Gamma'\vdash_{\mathrm C} B)\]
zu formulieren. Zur Anwendung einer Regel genügt daraufhin nämlich
innerhalb des metalogischen Systems allein der Modus ponens. Außerdem
kann eine Regel nun auch teilweise angewendet werden. So bestätigt
der Schluss
\[\begin{prooftree}
  \infer0{(\emptyset\vdash_{\mathrm C} A\cond B)\cond (\Gamma'\vdash_{\mathrm C} A)
    \cond(\emptyset\cup\Gamma'\vdash_{\mathrm C} B)}
  \hypo{\emptyset\vdash_{\mathrm C} A\cond B}
\infer2[MP]{(\Gamma'\vdash_{\mathrm C} A)\cond (\Gamma'\vdash_{\mathrm C} B)}
\end{prooftree}\]
die Regel, laut der $\Gamma'\vdash B$
aus $\Gamma'\vdash A$ abgeleitet werden darf, als zulässig,
sofern denn die Sequenz $\vdash A\cond B$ ableitbar ist.
Spinnen wir diese Vorgehensweise weiter fort, hält uns nichts davon ab,
natürliches Schließen als metalogisches System selbst zu verwenden.
Die Subjunktionseinführung
\[\dfrac{(\Gamma\vdash_{\mathrm C} A)\vdash (\Gamma\vdash_{\mathrm C} B)}{
\vdash (\Gamma\vdash_{\mathrm C} A)\cond (\Gamma\vdash_{\mathrm C} B)}\]
präzisiert für endliches $\Gamma$ nun den Gedankengang, dass die Regel
\[\dfrac{\Gamma\vdash A}{\Gamma\vdash B}\]
als zulässig erkannt wird, indem $\Gamma\vdash_{\mathrm C} B$ unter
Annahme von $\Gamma\vdash_{\mathrm C} A$ abgeleitet wird. Damit fallen
die metalogischen Überlegungen selbst unter die Formalisierung. Dies
wird man spätestens dann nicht mehr leugnen können, wenn die
Metalogik durch einen Beweisassistenten bereitgestellt wird.

Vermittels der Einführung von Grundsequenzen, der Abschwächungsregel
und des Modus ponens gelangt man zu den drei Einsichten%
\begin{align*}
&A\in\Gamma\cond (\Gamma\vdash_{\mathrm C} A),
  &&\text{(Extensivität)}\\
&\Gamma\subseteq\Gamma'\land (\Gamma\vdash_{\mathrm C} A)\cond
  (\Gamma'\vdash_{\mathrm C} A), &&\text{(Monotonie)}\\
&(\Gamma\cup\{A\}\vdash_{\mathrm C} B)\land (\Gamma\vdash_{\mathrm C} A)\cond
  (\Gamma\vdash_{\mathrm C} B). &&\text{(Schnittregel)}
\end{align*}
Eine Darstellung aus der Sichtweise der Mengenlehre fördert die Einordnung
dieser Sachverhalte. Als Hilfsmittel dient hierbei der
\emph{tarskische Konsequenzoperator}%
\[\Cn_S(\Gamma) := \{A\mid \Gamma\vdash_S A\},\]
der $\Gamma$ die Menge aller Formeln zuordnet,
die aus $\Gamma$ ableitbar sind. Man spricht auch vom
\emph{Inferenzoperator} oder vom \emph{deduktiven Abschluss}.
Wir schreiben kurz $\Cn$ statt $\Cn_{\mathrm C}$. Dieser genügt den
drei Axiomen
\begin{align*}
&\Gamma\subseteq\Cn(\Gamma), &&\text{(Extensivität)}\\
&\Gamma\subseteq\Gamma'\cond \Cn(\Gamma)\subseteq\Cn(\Gamma'), &&\text{(Monotonie)}\\
&\Cn(\Cn(\Gamma)) \subseteq \Cn(\Gamma). &&\text{(Abgeschlossenheit)}
\end{align*}
Das heißt, es handelt sich bei $\Cn$ um einen Hüllenoperator. Aus der
Anfügung der Monotonie an die Extensivität folgt auch die Umkehrung
der Abgeschlossenheit. Somit gilt mehr noch die Idempotenz
\[\Cn(\Cn(\Gamma)) = \Cn(\Gamma).\]
Die vormals
vorhandene Schnittregel leitet sich aus den drei Axiomen ab.
Sie wird zunächst in die Form%
\[A\in\Cn(\Gamma)\cond\Cn(\Gamma\cup\{A\})\subseteq\Cn(\Gamma)\]
gebracht. Es gilt $\Gamma\cup\{A\}\subseteq\Cn(\Gamma)$, denn
$\Gamma\subseteq\Cn(\Gamma)$ gilt gemäß Extensivität, und
$\{A\}\subseteq\Cn(\Gamma)$ gilt gemäß der Prämisse.
Laut Monotonie und Idempotenz folgt%
\[\Cn(\Gamma\cup\{A\})\subseteq\Cn(\Cn(\Gamma)) = \Cn(\Gamma).\]
Obgleich die Abgeschlossenheit intuitiv klar erscheint, ist sie
umgekehrt aus der Schnittregel zu gewinnen. Prämisse sei also
$A\in\Cn(\Cn(\Gamma)$, womit $\Cn(\Gamma)\vdash_{\mathrm C} A$. Das heißt aber,
es existiert eine endliche Teilmenge
\[\{A_1,\ldots,A_n\}\subseteq\Cn(\Gamma)\;\text{mit}\;
\{A_1,\ldots,A_n\}\vdash_{\mathrm C} A.\]
Mithin gilt $\Gamma\vdash_{\mathrm C} A_k$ zu jedem $k$.
Nun unternimmt man der Reihe nach den Schnitt der Form
\[\dfrac{\Gamma\cup\{A_1,\ldots,A_k\}\vdash_{\mathrm C} A\qquad\Gamma\vdash_{\mathrm C} A_k}
{\Gamma\cup\{A_1,\ldots,A_{k-1}\}\vdash_{\mathrm C} A}\]
zu den $k$ von $k=n$ bis $k=1$. Man gelangt damit schließlich zu
$\Gamma\vdash_{\mathrm C} A$, also zu der gewünschten Feststellung $A\in\Cn(\Gamma)$.

\newpage
\subsection{Die zulässige Ersetzungsregel}

Mit der Äquivalenz zweier Aussagen verhält es sich in gewisser Weise
wie mit einer Gleichheit. Und zwar vermittelt die \emph{zulässige
Ersetzungsregel}, eine Teilformel gegen eine zu ihr äquivalente
Formel ersetzen zu dürfen, analog wie bei der Termumformung eines
Teilterms. Sie ermöglicht die bequeme Äquivalenzumformung von
Aussagen, womit man sie als besonders nützlich erachten darf.

\begin{Satz}[Zulässige Ersetzungsregel]%
\index{Ersetzungsregel}\newlinefirst
Es gelten die beiden gleichwertigen Regeln
\[\dfrac{\Gamma\vdash A\Leftrightarrow B}
{\Gamma\vdash C(A)\Leftrightarrow C(B)},\qquad
\dfrac{\Gamma\vdash A\Leftrightarrow B\qquad\Gamma'\vdash C(A)}
{\Gamma,\Gamma'\vdash C(B)}.\]
\end{Satz}
\begin{Beweis}
Zunächst zur Gleichwertigkeit der beiden Regeln:
\begin{small}
\[\begin{array}{@{}l@{\;\;}l}
\begin{prooftree}[separation = 1em, center = false]
      \hypo{\Gamma\vdash A\bicond B}
    \infer1{\Gamma\vdash C(A)\bicond C(B)}
  \infer1{\Gamma\vdash C(A)\cond C(B)}
  \hypo{\Gamma'\vdash C(A)}
\infer2{\Gamma,\Gamma'\vdash C(B)}
\end{prooftree}
&
\begin{prooftree}[separation = 1em, center = false]
      \hypo{\Gamma\vdash A\bicond B}
      \infer0{C(A)\vdash C(A)}
    \infer2{\Gamma, C(A)\vdash C(B)}
  \infer1{\Gamma\vdash C(A)\cond C(B)}
      \hypo{\Gamma\vdash A\bicond B}
      \infer0{C(B)\vdash C(B)}
    \infer2{\Gamma, C(B)\vdash C(A)}
  \infer1{\Gamma\vdash C(B)\cond C(A)}
\infer2{\Gamma\vdash C(A)\bicond C(B)}
\end{prooftree}
\end{array}\]
\end{small}%
Wir führen nun eine strukturelle Induktion über den Formelaufbau durch.
Die Behauptung wird hierbei in der Form
\[\dfrac{\Gamma\vdash F\Leftrightarrow F'}{
\Gamma,C(F)\vdash C(F')},\quad C(F) := C[P:=F],\quad C(F') := C[P:=F']\]
geschrieben. Weil $F,F'$ vertauscht werden dürfen, erhält man somit auch
die umgekehrte Folgerung, so dass die Äquivalenz von $C(F)$ und $C(F')$
hergestellt wird. Wir definieren die Abkürzungen
\[A := C(F),\quad A' := C(F'),\quad B := D(F),\quad B' := D(F').\]
Zunächst die Basisfälle. Die Formeln $C:=\bot$,
$C:=\top$ und $C:=Q$ mit atomarer Variable $Q\ne P$
bleiben von der Substitution unbetroffen und sind offenkundig zu sich
selbst äquivalent. Für $C=P$ erhält man schlicht die Prämisse.

Zum Induktionsschritt. Man hat nun
\[(C\land D)[P:=F]
\iff C[P:=F]\land D[P:=F]\iff A\land B\]
usw. Induktionsvoraussetzung sei also $\Gamma\vdash A\Leftrightarrow A'$
und $\Gamma\vdash B\Leftrightarrow B'$. Zu zeigen ist
\begin{align*}
& \Gamma,A\land B\vdash A'\land B',
&& \Gamma,A\cond B\vdash A'\cond B',
&& \Gamma,\forall x\colon A\vdash\forall x\colon A',\\
& \Gamma,A\lor B\vdash A'\lor B',
&& \Gamma,A\bicond B\vdash A'\bicond B',
&& \Gamma,\exists x\colon A\vdash\exists x\colon A'
\end{align*}
und $\Gamma,\lnot A\vdash\lnot A'$. Es findet sich:
\begin{small}
\[\begin{array}{@{}l@{\quad\;\;}l}
\begin{prooftree}[separation = 1em, center = false]
      \hypo{\Gamma\vdash A\bicond A'}
    \infer1{\Gamma,A'\vdash A}
    \infer0{\lnot A\vdash\lnot A}
  \infer2{\Gamma,\lnot A,A'\vdash\bot}
\infer1{\Gamma,\lnot A\vdash\lnot A'}
\end{prooftree}
&
\begin{prooftree}[separation = 1em, center = false]
      \hypo{\Gamma\vdash A\bicond A'}
    \infer1{\Gamma\vdash A\cond A'}
      \infer0{A\land B\vdash A\land B}
    \infer1{A\land B\vdash A}
  \infer2{\Gamma,A\land B\vdash A'}
      \hypo{\Gamma\vdash B\bicond B'}
    \infer1{\Gamma\vdash B\cond B'}
      \infer0{A\land B\vdash A\land B}
    \infer1{A\land B\vdash B}
  \infer2{\Gamma,A\land B\vdash B'}
\infer2{\Gamma,A\land B\vdash A'\land B'}
\end{prooftree}
\end{array}\]
\end{small}%
Die Erstellung der restlichen Bäume sei dem Leser überlassen.

Für die Quantoren findet sich:
\[
\begin{array}{@{}l@{\quad\;\;}l@{}}
\begin{prooftree}[separation = 1em, center = false]
      \hypo{\Gamma\vdash A\bicond A'}
    \infer1{\Gamma\vdash A\cond A'}
      \infer0{\forall x\colon A\vdash\forall x\colon A}
    \infer1{\forall x\colon A\vdash A}
  \infer2{\Gamma,\forall x\colon A\vdash A'}
\infer1[$x\notin\mathrm{FV}(\Gamma)$]{\Gamma,\forall x\colon A\vdash\forall x\colon A'}
\end{prooftree}
&
\begin{prooftree}[separation = 1em, center = false]
  \infer0{\exists x\colon A\vdash\exists x\colon A}
        \hypo{\Gamma\vdash A\bicond A'}
      \infer1{\Gamma\vdash A\cond A'}
      \infer0{A\vdash A}
    \infer2{\Gamma, A\vdash A'}
  \infer1{\Gamma, A\vdash\exists x\colon A'}
\infer2[$x\notin\mathrm{FV}(\Gamma)$]{\Gamma,\exists x\colon A\vdash\exists x\colon A'}
\end{prooftree}
\end{array}
\]
Es darf hierbei $x\notin\mathrm{FV}(\Gamma)$ vorausgesetzt werden, weil
die gebundene Variable andernfalls ja vor der Betrachtung in eine
frische umbenannt werden kann.\,\qedsymbol
\end{Beweis}

\noindent
Es wäre noch zu erwähnen, dass die Regel bei den modalisierenden
Operatoren für gewöhnlich unzulässig ist. So ist in den modallogischen
Systemen K, T, B, D, S4, S5 keine der Formeln
\begin{gather*}
(a\bicond b)\cond (\lnec a\bicond\lnec b),\\
(a\bicond b)\cond (\lpos a\bicond\lpos b)
\end{gather*}
ein Theorem. Ein Gegenmodell ist jeweils schnell gefunden, dafür
bedarf es nicht mehr als zwei Welten.

Es besteht hier aber ein wesentlicher Unterschied zwischen relativer
Äquivalenz $\Gamma\vdash A\bicond B$ und absoluter Äquivalenz
$\vdash A\bicond B$. Zu einer absoluten Äquivalenz gilt die Ersetzungsregel
selbst in der Modallogik. Der Induktionsbeweis wird hierzu für leeres
$\Gamma$ erweitert um:
\[
\begin{array}{@{}l@{\qquad}l}
\begin{prooftree}
        \hypo{\vdash A\bicond A'}
      \infer1{\vdash A\cond A'}
    \infer1{\vdash\lnec(A\cond A')}
  \infer1{\vdash\lnec A\cond\lnec A'}
        \hypo{\vdash A\bicond A'}
      \infer1{\vdash A'\cond A}
    \infer1{\vdash\lnec(A'\cond A)}
  \infer1{\vdash\lnec A'\cond\lnec A}
\infer2{\vdash\lnec A\bicond\lnec A'}
\end{prooftree}
&
\begin{prooftree}
        \hypo{\vdash A\bicond A'}
      \infer1{\vdash\lnot A\bicond\lnot A'}
    \infer1{\vdash\lnec\lnot A\bicond\lnec\lnot A'}
  \infer1{\vdash\lnot\lnec\lnot A\bicond\lnot\lnec\lnot A'}
\infer1{\vdash\lpos A\bicond\lpos A'}
\end{prooftree}
\end{array}
\]
Der Leser möge diesbezüglich zunächst Abschnitt \ref{sec:Modallogik} studieren.

\section{Zur Beweisführung}

\subsection{Widerspruchsbeweise}

Beim \emph{Beweis durch Widerspruch} widerlegt man eine Aussage, indem
gezeigt wird, dass die Annahme der Aussage zu einem logischen
Widerspruch\index{Widerspruch} führt. In manchen Situationen bietet
diese Art der Argumentation eine große Hilfe. So schreibt der britische
Mathematiker Godfrey Harold Hardy in seinem Essay
\emph{A Mathematician's Apology} die Worte
\begin{quote}
»The proof is by \emph{reductio ad absurdum}, and \emph{reductio ad
absurdum}, which Euclid loved so much, is one of a mathematician's
finest weapons. It is a far finer gambit than any chess gambit: a chess
player may offer the sacrifice of a pawn or even a piece, but a
mathematician offers \emph{the game}.«
\end{quote}
Zur Schaffung von Klarheit muss man zunächst zwei inhaltlich
verschiedene Arten des Widerspruchsbeweises unterscheiden.
Präzisieren lässt sich diese Unterscheidung anhand
der Regeln
\[\dfrac{\Gamma,A\vdash\bot}{\Gamma\vdash\lnot A},\qquad
\dfrac{\Gamma,\lnot A\vdash\bot}{\Gamma\vdash A}.\]
Die linke Regel stellt die stets verfügbare Negationseinführung dar,
die man auch als \emph{Widerlegung durch Widerspruch} bezeichnen kann.
In der rechten Regel, der klassischen \emph{Reductio ad absurdum},
gelangt man zunächst per Negationseinführung von
$\Gamma,\lnot A\vdash\bot$ zu $\Gamma\vdash\lnot\lnot A$, und daraufhin
zu $\Gamma\vdash A$. Die Beseitigung der Doppelnegation%
\index{Doppelnegation} ist allerdings lediglich in der klassischen
Logik verfügbar, in der intuitionistischen gilt sie dagegen als
unzulässig.

Manche stellen die Regeln in einer Form dar, in der die Kontradiktion
nicht explizit auftaucht. Wir erhalten sie als zulässige Regeln, indem
der Einführung der Kontradiktion direkt ihre Beseitigung
angeschlossen wird. Es findet sich
\[\dfrac{\Gamma,A\vdash\lnot B\qquad\Gamma'\vdash B}
{\Gamma,\Gamma'\vdash\lnot A},\qquad
\dfrac{\Gamma,\lnot A\vdash\lnot B\qquad\Gamma'\vdash B}
{\Gamma,\Gamma'\vdash A}.\]
Wie zuvor ist die linke Form allgemein verfügbar, die rechte dagegen
nur bei Vorhandensein der Beseitigung der Doppelnegation.

\subsection{Klassische Kontraposition}

Eine weitere Regel ist die umgekehrte Kontraposition
\[\dfrac{\Gamma\vdash\lnot A\cond\lnot B}{\Gamma\vdash B\cond A}.\]
Sie verlangt ebenfalls die klassische Logik. Mit ihr lässt sich
nämlich die klassische Reductio ad absurdum herleiten:
\[
\begin{prooftree}
            \hypo{\Gamma,\lnot A\vdash\bot}
          \infer1[Abschwächung]{\Gamma,\lnot A,\top\vdash\bot}
        \infer1[Neg-Einf.]{\Gamma,\lnot A\vdash\lnot\top}
      \infer1[Subj-Einf.]{\Gamma\vdash\lnot A\cond\lnot\top}
    \infer1[bedenklich]{\Gamma\vdash\top\cond A}
    \infer0[Axiom]{\top\vdash\top}
  \infer2[Modus ponens]{\Gamma,\top\vdash A}
\infer1[Kürzung der Tautologie]{\Gamma\vdash A}
\end{prooftree}
\]
Weil alle anderen Schlüsse unbedenklich sind, kann die umgekehrte
Kontraposition als der bedenkliche Schritt identifiziert werden.
In der klassischen Logik ist sie allerdings zulässig. Die Herleitung
unter Verwendung der Beseitigung der Doppelnegation sei dem Leser
überlassen. Als kleiner Tipp sei aber gegeben, dass die Einführung
der Doppelnegation unter allen Umständen zulässig ist, wie man sich
unschwer überzeugt.

Wir schreiben die Abkürzungen DNE für die Beseitigung der
Doppelnegation, LEM für den Satz vom ausgeschlossenen Dritten
und EFQ für ex falso quodlibet. Eine Alternative zu DNE bietet
LEM zuzüglich EFQ. Manche Beweise verkürzen sich damit, andere
verlängern sich. Tatsächlich lässt sich DNE aus LEM zuzüglich EFQ
herleiten. Umgekehrt lässt sich sowohl LEM als auch EFQ aus DNE
herleiten. Es tut sich die Frage auf, ob sich EFQ aus LEM herleiten
lässt. Die Antwort darauf lautet nein. Eine ausführliche Untersuchung
findet man in \cite{Diener}.

\subsection{Notwendige und hinreichende Bedingungen}

Manchmal trifft man auf die Ausdrucksweise, eine Bedingung $B$ sei für
eine Aussage $A$ notwendig. Sie sagt aus, dass $\lnot B$ zu $\lnot A$
führt. Falls die Bedingung verletzt ist, kann die Aussage unmöglich
gelten. Es liegt demnach die Implikation $\lnot B\cond\lnot A$ vor.
Sie wird im Sinne der klassischen Logik verstanden. Man hat also
\[(A\cond B)\,\bicond\, (\text{$B$ ist notwendig für $A$}).\]
Die Ausdrucksweise, eine Bedingung $B$ sei für $A$ hinreichend,
sagt aus, dass $B$ die Aussage $A$ bereits nach sich zieht. Es liegt
demnach die Implikation $B\cond A$ vor. Man hat also
\[(B\cond A)\,\bicond\, (\text{$B$ ist hinreichend für $A$}).\]
Ist eine Bedingung $B$ sowohl notwendig als auch hinreichend für $A$,
liegt eine Äquivalenz vor. Man hat also
\[(A\bicond B)\,\bicond\, (\text{$B$ ist notwendig und hinreichend für $A$}).\]

\subsection{Verneinung von Aussagen}

\begin{table}
\begin{center}
\caption{Verneinung von Aussagen}%
\label{tab:Verneinung-von-Aussagen}
\begin{tabular}{l}
\toprule
$\begin{array}{@{}r@{\;}l@{\quad\;\;}r@{\;}lr@{\;}l@{}}
\lnot(A\land B) &\bicond\lnot A\lor\lnot B
& \lnot(\forall x\colon A(x)) &\bicond (\exists x\colon\lnot A(x))
& \lnot\bot &\bicond\top\\
\lnot(A\lor B) &\bicond\lnot A\land\lnot B
& \lnot(\exists x\colon A(x)) &\bicond (\forall x\colon\lnot A(x))
& \lnot\top &\bicond\bot\\
\lnot (A\cond B) &\bicond A\land \lnot B
& \lnot (A\bicond B) &\bicond A\oplus B
& \lnot\lnot A &\bicond A
\end{array}$\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}
\begin{center}
\caption{Zur Verneinung von All- und Existenzaussagen}%
\label{tab:Negation-All-Ex}
\begin{tabular}{@{}cc@{}}
\toprule
$\begin{prooftree}[center = false, separation = 1em]
      \infer0{1\equiv\exists x\colon A(x)}
          \infer0{2\equiv\forall x\colon\lnot A(x)}
        \infer1{2\vdash\lnot A(u)}
        \infer0{3\equiv A(u)}
      \infer2{2,3\vdash \bot}
    \infer2{1,2\vdash\bot}
  \infer1{2\vdash\lnot\exists x\colon A(x)}
\infer1{\vdash (\forall x\colon\lnot A(x))\cond \lnot (\exists x\colon A(x))}
\end{prooftree}$
&
$\begin{prooftree}[center = false, separation = 1em]
        \infer0{1\equiv\lnot\exists x\colon A(x)}
          \infer0{2\equiv A(x)}
        \infer1{2\vdash\exists x\colon A(x)}
      \infer2{1, 2\vdash\bot}
    \infer1{1\vdash\lnot A(x)}
  \infer1{1\vdash\forall x\colon\lnot A(x)}
\infer1{\vdash \lnot (\exists x\colon A(x))\cond (\forall x\colon\lnot A(x))}
\end{prooftree}$\\[10pt]
$\begin{prooftree}[center = false, separation = 1em]
      \infer0{1\equiv\exists x\colon \lnot A(x)}
        \infer0{2\equiv\lnot A(u)}
          \infer0{3\equiv\forall x\colon A(x)}
        \infer1{3\vdash A(u)}
      \infer2{2, 3\vdash\bot}
    \infer2{1, 3\vdash\bot}
  \infer1{1\vdash\lnot\forall x\colon A(x)}
\infer1{\vdash (\exists x\colon\lnot A(x))\cond \lnot (\forall x\colon A(x))}
\end{prooftree}$
&
$\begin{prooftree}[center = false, separation = 1em]
        \infer0{1\equiv\lnot\forall x\colon A(x)}
                \infer0{2\equiv\lnot\exists x\colon\lnot A(x)}
              \infer1{2\vdash\forall x\colon\lnot\lnot A(x)}
            \infer1{2\vdash\lnot\lnot A(x)}
          \infer1{2\vdash A(x)}
        \infer1{2 \vdash\forall x\colon A(x)}
      \infer2{1,2\vdash\bot}
    \infer1{1\vdash\lnot\lnot\exists x\colon\lnot A(x)}
  \infer1{1\vdash\exists x\colon\lnot A(x)}
\infer1{\vdash \lnot (\forall x\colon A(x))\cond (\exists x\colon\lnot A(x))}
\end{prooftree}$\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Die Verneinung von All- und Existenzaussagen besitzt Umformungen, die
analog zu den de morganschen Gesetzen vonstatten gehen. In Tabelle
\ref{tab:Verneinung-von-Aussagen} ist die jeweilige Gesetzmäßigkeit
hinter das jeweilige de morgansche Gesetz gestellt. Die Beweise sind in
Tabelle \ref{tab:Negation-All-Ex} ausgeführt.

In Worten gilt eine Aussageform $A(x)$ genau dann nicht für jedes $x$,
wenn sich mindestens ein $x$ findet, für das $A(x)$ nicht gilt.
Und es existiert genau dann kein $A(x)$ erfüllendes $x$, wenn
$A(x)$ für jedes $x$ nicht gilt.

In Verbindung mit der Ersetzungsregel ermöglichen die in Tabelle
\ref{tab:Verneinung-von-Aussagen} aufgeführten Gesetze die
äquivalente Umformung verneinter Aussagen, die solange fortgeführt
werden kann, bis die Verneinung durch alle Junktoren und Quantoren
der Aussage gedrungen ist. Mit der Entfaltung von Def.
\ref{def:Beschr-Quant}, die später auf S. \pageref{def:Beschr-Quant}
eingeführt wird, findet sich zum Beispiel die Umformung
\begin{align*}
\lnot(\forall x\in M\colon A(x))
&\iff \lnot(\forall x\colon x\in M\cond A(x))\\
&\iff (\exists x\colon\lnot (x\in M\cond A(x)))\\
&\iff (\exists x\colon x\in M\land\lnot A(x))\\
&\iff (\exists x\in M\colon A(x)).
\end{align*}
Es stellt sich somit heraus, dass die Verneinung der beschränkten
Quantifizierung ebenfalls analog zu den de morganschen Gesetzen
umgeformt wird. Es gilt
\begin{align*}
\lnot(\forall x\in M\colon A(x)) &\bicond \exists x\in M\colon \lnot A(x),\\
\lnot(\exists x\in M\colon A(x)) &\bicond \forall x\in M\colon \lnot A(x).
\end{align*}

