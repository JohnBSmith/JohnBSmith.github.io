
\chapter{Analysis}
\section{Folgen}
\subsection{Konvergenz}

\begin{Definition}[open-ep-ball: offene Epsilon-Umgebung]%
\index{Epsilon-Umgebung}\index{offene Epsilon-Umgebung}
Sei $(M,d)$ ein metrischer Raum. Unter der offenen Epsilon-Umgebung
von $a\in M$ versteht man:%
\[U_\varepsilon(a) := \{x\mid d(x,a)<\varepsilon\}\]
Setze zunächst speziell $d(x,a):=|x-a|$ bzw. $d(x,a):=\|x-a\|$.
\end{Definition}

\begin{Definition}[lim: konvergente Folge, Grenzwert]%
\label{def:lim}\index{konvergente Folge}\index{Grenzwert}
\[\lim_{n\to\infty} a_n = a
\defiff \forall\varepsilon{>}0\;\exists n_0\;\forall n{\ge}n_0\;(a_n\in U_\varepsilon(a))\]
bzw.
\[\lim_{n\to\infty} a_n = a
\defiff \forall\varepsilon{>}0\;\exists n_0\;\forall n{\ge}n_0\;(\|a_n-a\|<\varepsilon).\]
\end{Definition}

\begin{Definition}[bseq: beschränkte Folge]%
\label{def:bseq}\index{beschreankte Folge@beschränkte Folge}
Eine Folge $(a_n)$ mit $a_n\in\R$ heißt genau dann beschränkt,
wenn es eine reelle Zahl $S$ gibt mit $|a_n|<S$ für alle $n$.

Eine Folge $(a_n)$ von Punkten eines normierten Raums heißt genau
dann beschränkt, wenn es eine reelle Zahl $S$ gibt mit $\|a_n\|<S$
für alle $n$.
\end{Definition}

\begin{Satz}[Grenzwert bei Konvergenz eindeutig bestimmt]\mbox{}\\
Eine konvergente Folge von Elementen eines metrischen Raumes
besitzt genau einen Grenzwert.
\end{Satz}

\begin{Beweis}
Sei $(a_n)$ eine konvergente Folge mit $a_n\to g_1$. Sei weiterhin
$g_1\ne g_2$. Es wird nun gezeigt, dass $g_2$ kein Grenzwert von $a_n$
sein kann. Wir müssen also zeigen:
\[\neg\lim_{n\to\infty} a_n=g_2 \iff
\exists\varepsilon{>}0\;\forall n_0\;\exists n{\ge}n_0\;
(a_n\notin U_\varepsilon(g_2))\]
mit $a_n\notin U_\varepsilon(g_2)\iff d(a_n,g_2)\ge\varepsilon$.

Um dem Existenzquantor zu genügen, wählt man nun
$\varepsilon = \frac{1}{2}d(g_1,g_2)$.
Nach Def. \ref{metric-space} (metric-space) gilt 
$d(g_1,g_2)>0$, daher ist auch $\varepsilon>0$. Nach Satz
\ref{construction-disjoint-ep-balls} sind die Umgebungen
$U_\varepsilon(g_1)$ und $U_\varepsilon(g_2)$ disjunkt.
Wegen $a_n\to g_1$ gibt es ein $n_0$ mit $a_n\in U_\varepsilon(g_1)$ für alle
$n\ge n_0$. Dann gibt es für jedes beliebig große $n_0$ aber auch
$n\ge n_0$ mit $a_n\notin U_\varepsilon(g_2)$.\,\qedsymbol
\end{Beweis}

\begin{Satz}[lim-scaled-ep: skaliertes Epsilon]\label{lim-scaled-ep}
Es gilt:
\[\lim_{n\to\infty} a_n=a \iff
\forall\varepsilon{>}0\;\exists n_0\;\forall n{\ge}n_0\;(\|a_n-a\|<R\varepsilon),\]
wobei $R>0$ ein fester aber beliebieger Skalierungsfaktor ist.
\end{Satz}

\begin{Beweis}
Betrachte $\varepsilon>0$ und multipliziere auf beiden Seiten
mit $R$. Dabei handelt es sich um eine Äquivalenzumformung.
Setze $\varepsilon':=R\varepsilon$. Demnach gilt:
\[\varepsilon>0 \iff \varepsilon'>0.\]
Nach der Ersetzungsregel düfen wir die Teilformel $\varepsilon>0$
nun ersetzen. Es ergibt sich die äquivalente Formel
\[\lim_{n\to\infty} a_n=a \iff
\forall\varepsilon'{>}0\;\exists n_0\;\forall n{\ge}n_0\;
(\|a_n-a\|<\varepsilon').\]
Das ist aber genau Def. \ref{def:lim} (lim).\,\qedsymbol
\end{Beweis}

\begin{Satz}
Es gilt:
\[\lim_{n\to\infty} a_n = a\implies \lim_{n\to\infty} \|a_n\| = \|a\|.\]
\end{Satz}

\begin{Beweis}
Nach Satz \ref{rev-tineq} (umgekehrte Dreiecksungleichung) gilt:
\[|\|a_n\|-\|a\|| \le \|a_n-a\| < \varepsilon.\]
Dann ist aber rest recht $|\|a_n\|-\|a\||<\varepsilon$.\,\qedsymbol
\end{Beweis}

\begin{Satz}\label{zero-seq-bounded}
Ist $(a_n)$ eine Nullfolge und $(b_n)$ eine beschränkte Folge,
dann ist auch $(a_n b_n)$ eine Nullfolge.
\end{Satz}

\begin{Beweis}
Wenn $(b_n)$ beschränkt ist, dann existiert nach
Def. \ref{def:bseq} (bseq) eine Schranke $S$ mit
$|b_n|<S$ für alle $n$. Man multipliziert nun auf beiden Seiten
mit $|a_n|$ und erhält
\[|a_n b_n| = |a_n| |b_n| < |a_n| S.\]
Wenn $a_n\to 0$, dann muss für jedes $\varepsilon$
ein $n_0$ existieren mit $|a_n|<\varepsilon$ für $n\ge n_0$.
Multipliziert man auf beiden Seiten mit $S$, und ergibt sich
\[|a_n b_n-0| = |a_n b_n| < |a_n| S < S\varepsilon.\]
Nach Satz \ref{lim-scaled-ep} (lim-scaled-ep) gilt dann
aber $a_n b_n\to 0$.\,\qedsymbol
\end{Beweis}

\begin{Satz}
Sind $(a_n)$ und $(b_n)$ Nullfolgen,
dann ist auch $(a_n b_n)$ eine Nullfolge.
\end{Satz}

\begin{Beweis}[Beweis 1]
Wenn $(b_n)$ eine Nullfolge ist, dann ist $(b_n)$ auch beschränkt.
Nach Satz \ref{zero-seq-bounded} gilt dann die Behauptung.
\end{Beweis}

\begin{Beweis}[Beweis 2]
Sei $\varepsilon>0$ beliebig.
Es gibt ein $n_0$, so dass
$|a_n|<\varepsilon$ und $|b_n|<\varepsilon$ für $n\ge n_0$.
Demnach ist
\[|a_n b_n| = |a_n| |b_n|< |a_n|\varepsilon <\varepsilon^2.\]
Wegen $\varepsilon>0\iff\varepsilon'>0$ mit
$\varepsilon'=\varepsilon^2$ gilt
\[\forall\varepsilon'{>}0\;\exists n_0\;\forall n{\ge}n_0\;
(|a_n b_n|<\varepsilon').\]
Nach Def. \ref{def:lim} (lim) gilt somit die Behauptung.\,\qedsymbol
\end{Beweis}

\begin{Satz}[Grenzwertsatz zur Addition]%
\label{lim-add}\index{Grenzwertsaetze@Grenzwertsätze}
Seien $(a_n)$, $(b_n)$ Folgen von Vektoren eines normierten Raumes.
Es gilt:
\[\lim_{n\to\infty} a_n = a\land \lim_{n\to\infty} b_n
= b \implies \lim_{n\to\infty} a_n+b_n = a+b.\]
\end{Satz}

\begin{Beweis}
Dann gibt es ein $n_0$, so dass für $n\ge n_0$ sowohl
$\|a_n-a\|<\varepsilon$ als auch $\|b_n-b\|<\varepsilon$.
Addition der beiden Ungleichungen ergibt
\[\|a_n-a\| + \|b_n-b\| < 2\varepsilon.\]
Nach der Dreiecksungleichung, das ist Axiom (N3) in Def.
\ref{def:normed-space} (normed-space), gilt nun aber die Abschätzung
\[\|(a_n+b_n)-(a+b)\| = \|(a_n-a)+(b_n-b)\| \le \|a_n-a\|+\|b_n-b\|.\]
Somit gilt erst recht
\[\|(a_n+b_n)-(a+b)\| < 2\varepsilon.\]
Nach Satz \ref{lim-scaled-ep} (lim-scaled-ep)
folgt die Behauptung.\,\qedsymbol
\end{Beweis}

\begin{Satz}[Grenzwertsatz zur Skalarmultiplikation]\label{lim-smult}
Sei $(a_n)$ eine Folge von Vektoren eines normierten Raumes
und sei $r\in\R$ oder $r\in\C$. Es gilt:
\[\lim_{n\to\infty} a_n = a\implies \lim_{n\to\infty} ra_n\to ra.\]
\end{Satz}

\begin{Beweis}
Sei $\varepsilon>0$ fest aber beliebig. Es gibt nun ein $n_0$, so
dass $\|a_n-a\|<\varepsilon$ für $n\ge n_0$.
Multipliziert man auf beiden Seiten
mit $|r|$ und zieht Def. \ref{def:normed-space} (normed-space)
Axiom (N2) heran, dann ergibt sich
\[\|ra_n-ra\| = |r|\,\|a_n-a\|<|r|\varepsilon.\]
Nach Satz \ref{lim-scaled-ep} (lim-scaled-ep)
folgt die Behauptung.\,\qedsymbol
\end{Beweis}

\begin{Satz}[Grenzwertsatz zum Produkt]\mbox{}\\
Seien $(a_n)$ und $(b_n)$ Folgen
reeller Zahlen. Es gilt:
\[\lim_{n\to\infty} a_n=a\land\lim_{n\to\infty} b_n=b\implies
\lim_{n\to\infty} a_n b_n = ab.\]
\end{Satz}

\begin{Beweis}
Nach Voraussetzung sind $a_n-a$ und $b_n-b$ Nullfolgen.
Da das Produkt von Nullfolgen wieder eine Nullfolge ist, gilt
\[(a_n-a)(b_n-b) = a_n b_n-a_n b-ab_n+ab\to 0.\]
Da nach Satz \ref{lim-smult} aber $a_n b\to ab$ und $ab_n\to ab$,
ergibt sich nach Satz \ref{lim-add} nun
\[(a_n-a)(b_n-b)+a_n b+ab_n = a_n b_n+ab\to 2ab.\]
Addiert man nun noch die konstante Folge $-2ab$
und wendet nochmals Satz \ref{lim-add} an, dann ergibt sich
die Behauptung
\[a_n b_n\to ab.\,\qedsymbol\]
\end{Beweis}

\newpage
\begin{Satz}\label{cont-seqcont}%
\index{folgenstetig}\index{stetig!folgenstetig}
Sei $M$ ein metrischer Raum und $X$ ein topologischer Raum.
Eine Abbildung $f\colon M\to X$ ist genau dann stetig, wenn
sie folgenstetig ist.
\end{Satz}

\begin{Satz}[Satz zur Fixpunktgleichung]\index{Fixpunktgleichung}
Sei $M$ ein metrischer Raum und sei $f\colon M\to M$.
Sei $x_{n+1}:=f(x_n)$ eine Fixpunktiteration. Wenn die Folge
$(x_n)$ zu einem Startwert $x_0$ konvergiert mit $x_n\to x$, und
wenn $f$ eine stetige Abbildung ist, dann muss der Grenzwert $x$ die
Fixpunktgleichung $x=f(x)$ erfüllen.
\end{Satz}

\begin{Beweis}
Wenn $x_n\to x$, dann gilt trivialerweise auch $x_{n+1}\to x$.
Weil $f$ stetig ist, ist $f$ nach Satz \ref{cont-seqcont}
auch folgenstetig. Daher gilt $\lim f(a_n) = f(\lim a_n)$ für jede
konvergente Folge $(a_n)$. Somit gilt:
\[x=\lim_{n\to\infty} x_{n+1} = \lim_{n\to\infty} f(x_n)
= f(\lim_{n\to\infty} x_n) = f(x).\;\qedsymbol\]
\end{Beweis}

\subsection{Wachstum und Landau-Symbole}
\begin{Definition}\label{Landau-O}
Seien $f,g\colon D\to\R$ mit $D=\N$ oder $D=\R$. Man sagt, die
Funktion $f$ wächst nicht wesentlich schneller als $g$, kurz
$f\in\mathcal O(g)$, genau dann, wenn
\[\exists(c>0)\exists(x_0>0)\forall(x>x_0)(|f(x)|\le c|g(x)|).\]
\end{Definition}

\begin{Korollar}
Ist $r\in\R$ mit $r\ne 0$ eine Konstante, dann gilt
$\mathcal O(rg)=\mathcal O(g)$.
\end{Korollar}
\begin{Beweis}
Nach Def. \ref{Landau-O} ist
\[f\in\mathcal O(rg) \iff 
\exists(c>0)\exists(x_0>0)\forall(x>x_0)(|f(x)|\le c|rg(x)|).\]
Man hat nun
\[|f(x)|\le c|rg(x)| = c\cdot |r|\cdot |g(x)|.\]
\end{Beweis}
Wegen $r\ne 0$ ist $|r|>0$ und daher auch $c>0\iff c|r|>0$. Sei
$c':=r|c|$. Also gilt $c>0\iff c'>0$. Nach der Ersetzungsregel
darf $c>0$ gegen $c'>0$ ersetzt werden und man erhält die
äquivalente Bedingung
\[\exists(c'>0)\exists(x_0>0)\forall(x>x_0)(|f(x)|\le c'|g(x)|).\]
Nach Def. \ref{Landau-O} ist das gerade $f\in\mathcal O(g)$.\;\qedsymbol

\section{Stetige Funktionen}

\begin{Definition}[Grenzwert einer Funktion]\label{fn-lim}
Sei $f\colon D\to\R$ mit $D\subseteq\R$ und sei $p$ ein
Häufungspunkt von $D$. Die Funktion $f$ heißt konvergent
gegen $L$ für $x\to p$, wenn%
\[\forall(\varepsilon>0)\exists(\delta>0)\forall(x\in D)(
  0<|x-x_0|<\delta\implies |f(x)-L|<\varepsilon).\]
Bei Konvergenz schreibt man $L=\lim\limits_{x\to p} f(x)$ und nennt $L$ den Grenzwert.
\end{Definition}

\begin{Definition}[cont: stetig]\label{cont}
Eine Funktion $f\colon D\to\R$ mit $D\subseteq\R$ heißt stetig an der
Stelle $x_0\in D$, wenn
\[\forall(\varepsilon>0)\exists(\delta>0)\forall(x\in D)(
  |x-x_0|<\delta\implies |f(x)-f(x_0)|<\varepsilon).\]
\end{Definition}

\begin{Definition}[Lipschitz-stetig]\mbox{}\\
Eine Funktion $f\colon D\to\R$ mit $D\subseteq\R$ heißt
Lipschitz"=stetig, wenn eine Konstante $L$ existiert, so dass
\[|f(b)-f(a)|\le L|b-a|\]
für alle $a,b\in D$.
\end{Definition}

\begin{Definition}[Lipschitz-stetig an einer Stelle]%
\label{Lipschitz-cont-at}\mbox{}\\
Eine Funktion $f\colon D\to\R$ mit $D\subseteq\R$ heißt
Lipschitz"=stetig an der Stelle $x_0\in D$, wenn eine Konstante $L$
existiert, so dass
\[|f(x_0)-(a)|\le L|x_0-a|\]
für alle $a\in D$.
\end{Definition}

\begin{Korollar}
Eine Funktion ist genau dann Lipschitz"=stetig, wenn sie an jeder
Stelle Lipschitz"=stetig ist und die Menge der optimalen
Lipschitz"=Konstanten dabei beschränkt.
\end{Korollar}
\begin{Beweis}
Eine Lipschitz"=stetige Funktion ist trivialerweise an jeder Stelle
Lipschitz"=stetig. Ist $f\colon D\to\R$ an der Stelle $b$ Lipschitz"=stetig,
dann existiert eine Lipschitz"=Konstante $L_b$ mit%
\[\forall(a\in D)(|f(b)-f(a)|\le L_b |b-a|).\]
Nach Voraussetzung ist $L=\sup_{b\in D} L_b$ endlich. Alle $L_b$ können
nun zu $L$ abgeschwächt werden und es ergibt sich%
\[\forall(b\in D)\forall(a\in D)(|f(b)-f(a)|\le L|b-a|).\;\qedsymbol\]
\end{Beweis}


\begin{Definition}[lokal Lipschitz-stetig]\mbox{}\\
Eine Funktion $f\colon D\to\R$ mit $D\subseteq\R$ heißt lokal
Lipschitz"=stetig in der Nähe einer Stelle $x_0\in D$, wenn es eine
Epsilon"=Umgebung $U_\varepsilon(x_0)$ gibt, so dass die Einschränkung
von $f$ auf diese Umgebung Lipschitz"=stetig ist. Die Funktion heißt
lokal Lipschitz"=stetig, wenn sie in der Nähe jeder Stelle
Lipschitz"=stetig ist.
\end{Definition}

\begin{Satz}\label{diff-nh-Lipschitz-cont-at}
Ist die Funktion $f\colon D\to\R$ an der Stelle $x_0$ differenzierbar,
dann gibt es ein $\delta>0$, so dass die Einschränkung von $f$
auf $U_\delta(x_0)$ an der Stelle $x_0$ Lipschitz"=stetig ist.
\end{Satz}

\begin{Beweis}
Def. \ref{fn-lim} wird in Def. \ref{diff} (diff) eingesetzt.
Es ergibt sich:
\[0<|x-x_0|<\delta\implies
\left|\frac{f(x)-f(x_0)}{x-x_0}-f'(x_0)\right|<\varepsilon.\]
Nach der umgekehrten Dreiecksungleichung \ref{rev-tineq} gilt
\[\left|\frac{f(x)-f(x_0)}{x-x_0}\right|-|f'(x_0)| \le
\left|\frac{f(x)-f(x_0)}{x-x_0}-f'(x_0)\right|
< \varepsilon.\]
Daraus ergibt sich
\[|f(x)-f(x_0)| < (|f'(x_0)|+\varepsilon)\cdot |x-x_0|\]
und somit erst recht
\[|f(x)-f(x_0)| \le (|f'(x_0)|+\varepsilon)\cdot |x-x_0|,\]
wobei jetzt auch $x=x_0$ erlaubt ist. Demnach wird Def.
\ref{Lipschitz-cont-at} erfüllt:
\[\exists(\delta>0)\forall(x\in U_\delta(x_0))(
|f(x)-f(x_0)| \le (|f'(x_0)|+\varepsilon)\cdot |x-x_0|).\;\qedsymbol\]
\end{Beweis}

\begin{Satz}\label{diff-bounded-Lipschitz-cont}
Eine differenzierbare Funktion ist genau dann Lipschitz"=stetig,
wenn ihre Ableitung beschränkt ist.
\end{Satz}
\begin{Beweis}
Wenn $f\colon I\to\R$ Lipschitz"=stetig ist, dann gibt es $L$ mit
\[\left|\frac{f(b)-f(a)}{b-a}\right|\le L\]
für alle $a,b\in D$ mit $a\ne b$. Daraus folgt
\[|f'(a)| = \left|\lim_{b\to a} \frac{f(b)-f(a)}{b-a}\right|
= \lim_{b\to a} \left|\frac{f(b)-f(a)}{b-a}\right|
\le L.\]
Demnach ist die Ableitung beschränkt.

Sei nun umgekehrt die Ableitung beschränkt. Für $a,b\in I$ mit $a\ne b$
gibt es nach dem Mittelwertsatz ein $x_0\in(a,b)$, so dass
\[|f'(x_0)| = \left|\frac{f(b)-f(a)}{b-a}\right|.\]
Da die Ableitung beschränkt ist gibt es ein Supremum
$L = \sup_{x\in I} |f'(x)|$. Demnach ist $|f'(x)|\le L$ für alle $x$.
Es ergibt sich
\[\left|\frac{f(b)-f(a)}{b-a}\right|\le L|b-a| \implies |f(b)-f(a)|\le L|b-a|.\]
Nun darf auch $a=b$ gewählt werden.\;\qedsymbol
\end{Beweis}

\begin{Satz}\label{diff-compact-Lipschitz-cont}
Eine auf einem kompakten Intervall $[a,b]$ definierte stetig
differenzierbare Funktion ist Lipschitz"=stetig.
\end{Satz}
\begin{Beweis}
Sei $f\colon [a,b]\to\R$ stetig differenzierbar. Dann ist $f'(x)$ stetig.
Nach dem Satz vom Minimum und Maximum ist $|f'(x)|$ beschränkt. Nach
Satz \ref{diff-bounded-Lipschitz-cont} muss $f$ Lipschitz"=stetig
sein.\;\qedsymbol
\end{Beweis}

\begin{Korollar}
Eine stetig differenzierbare Funktion ist lokal Lipschitz"=stetig.
\end{Korollar}
\begin{Beweis}
Sei $f\colon D\to\R$ stetig differenzierbar. Sei $[a,b]\in D$. Sei
$x_0\in [a,b]$. Die Einschränkung von $f$ auf $[a,b]$ ist
Lipschitz"=stetig nach Satz \ref{diff-compact-Lipschitz-cont}.
Dann ist auch die Einschränkung von $f$ auf
$U_\varepsilon(x_0)\subseteq [a,b]$ Lipschitz"=stetig.\;\qedsymbol
\end{Beweis}

\begin{Satz}
Es gibt differenzierbare Funktionen, die nicht überall lokal
Lipschitz"=stetig sind.
\end{Satz}
\begin{Beweis}
Aus Satz \ref{diff-bounded-Lipschitz-cont} ergibt sich also
Kontraposition, dass eine Funktion mit unbeschränkter Ableitung
nicht Lipschitz"=stetig sein kann.

Ist $f\colon D\to\R$ an jeder Stelle differenzierbar und ist $f'$
in jeder noch so kleinen Umgebung der Stelle $x_0$ unbeschränkt, dann
kann $f$ also in der Nähe dieser Stelle auch nicht lokal
Lipschitz"=stetig sein.

Ein Beispiel für eine solche Funktion ist $f\colon{}[0,\infty)\to\R$
mit
\[f(0):=0\quad \text{und}\quad f(x):=x^{3/2}\cos\Big(\tfrac{1}{x}\Big).\]
Einerseits gilt
\[f'(0) = \lim_{h\to 0}\frac{f(0+h)-f(0)}{h} = \lim_{h\to 0}\frac{f(h)}{h}
= \lim_{h\to 0} (h^{1/2}\cos\Big(\tfrac{1}{h}\Big)) = 0.\]
Die Funktion ist also an der Stelle $x=0$ differenzierbar.
Andererseits gilt nach den Ableitungsregeln
\[f'(x) = \frac{3}{2}\sqrt{x}\cos\Big(\tfrac{1}{x}\Big)+\frac{1}{\sqrt{x}}\sin\Big(\tfrac{1}{x}\Big).\]
für $x>0$. Der Term $\tfrac{1}{\sqrt{x}}$ erwirkt für $x\to 0$ immer
größere Maxima von $|f'(x)|$. Daher kann $f$ in der Nähe von $x=0$ nicht
lokal Lipschitz"=stetig sein.\;\qedsymbol
\end{Beweis}

\section{Differentialrechnung}
\begin{Definition}[diff: differenzierbar, Ableitung]\label{diff}
Eine Funktion $f\colon D\to\R$ heißt differenzieraber an der Stelle
$x_0\in D$, wenn der Grenzwert
\[f'(x_0) = \lim_{x\to x_0}\frac{f(x)-f(x_0)}{x-x_0}
= \lim_{h\to 0}\frac{f(x_0+h)-f(x_0)}{h}\]
existiert. Man nennt $f'(x_0)$ die Ableitung von $f$ an der Stelle
$x_0$.
\end{Definition}

\begin{Satz}
Sei $f\colon\R\to\R$ eine Funktion mit der Eigenschaft
$f(x)=0$ für $x\le 0$ und $f(x)>0$ für $x>0$. Es gibt glatte Funktionen
mit dieser Eigenschaft, jedoch keine analytische.
\end{Satz}

\begin{Beweis}
Wegen $f(x)=0$ für $x\le 0$ muss die linksseitige $n$-te Ableitung
an der Stelle $x=0$ immer verschwinden. Wenn die $n$-te Ableitung
stetig sein soll, muss auch die rechtsseitige Ableitung bei $x=0$
verschwinden. Da die Funktion glatt sein soll, muss das für jede
Ableitung gelten. Daher verschwindet die Taylorreihe an der Stelle
$x=0$. Da aber $f(x)>0$ für $x>0$, gibt es keine noch so kleine
Umgebung mit Übereinstimmung von $f$ und ihrer Taylorreihe.
Daher kann $f$ an der Stelle $x=0$ nicht analytisch sein.

Eine glatte Funktion lässt sich jedoch konstruieren:
\[f(x):=\begin{cases}
\ee^{-1/x}&\text{wenn}\;x>0,\\
0&\text{wenn}\;x\le 0.
\end{cases}\]
Ist nämlich $g(x)$ an einer Stelle glatt, dann ist
es nach Kettenregel, Produktregel und Summenregel auch $\ee^{g(x)}$.
Die $n$-te Ableitung lässt sich immer in der Form
\[\sum\nolimits_k e^{g(x)}{r_k(x)}
= e^{g(x)}\sum\nolimits_k r_k(x) = e^{g(x)}r(x)\]
darstellen, wobei die $r_k(x)$ bzw. $r(x)$ in diesem Fall rationale
Funktionen mit Polstelle bei $x=0$ sind. Da aber $e^{-1/x}$ für
$x\to 0$ schneller fällt als jede rationale Funktion steigen kann,
muss die rechtsseitige Ableitung an der Stelle $x=0$ immer
verschwinden.\;\qedsymbol
\end{Beweis}

