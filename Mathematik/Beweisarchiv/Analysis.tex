
\chapter{Analysis}
\section{Folgen}
\subsection{Konvergenz}

\begin{Definition}[open-ep-ball: offene Epsilon-Umgebung]%
\index{Epsilon-Umgebung}\index{offene Epsilon-Umgebung}
Sei $(M,d)$ ein metrischer Raum. Unter der offenen Epsilon-Umgebung
von $a\in M$ versteht man:%
\[U_\varepsilon(a) := \{x\mid d(x,a)<\varepsilon\}\]
Setze zunächst speziell $d(x,a):=|x-a|$ bzw. $d(x,a):=\|x-a\|$.
\end{Definition}

\begin{Definition}[lim: konvergente Folge, Grenzwert]%
\label{def:lim}\index{konvergente Folge}\index{Grenzwert}
\[\lim_{n\to\infty} a_n = a
\defiff \forall\varepsilon{>}0\;\exists n_0\;\forall n{\ge}n_0\;(a_n\in U_\varepsilon(a))\]
bzw.
\[\lim_{n\to\infty} a_n = a
\defiff \forall\varepsilon{>}0\;\exists n_0\;\forall n{\ge}n_0\;(\|a_n-a\|<\varepsilon).\]
\end{Definition}

\begin{Definition}[bseq: beschränkte Folge]%
\label{def:bseq}\index{beschreankte Folge@beschränkte Folge}
Eine Folge $(a_n)$ mit $a_n\in\R$ heißt genau dann beschränkt,
wenn es eine reelle Zahl $S$ gibt mit $|a_n|<S$ für alle $n$.

Eine Folge $(a_n)$ von Punkten eines normierten Raums heißt genau
dann beschränkt, wenn es eine reelle Zahl $S$ gibt mit $\|a_n\|<S$
für alle $n$.
\end{Definition}

\begin{Satz}[Grenzwert bei Konvergenz eindeutig bestimmt]\mbox{}\\
Eine konvergente Folge von Elementen eines metrischen Raumes
besitzt genau einen Grenzwert.
\end{Satz}

\begin{Beweis}
Sei $(a_n)$ eine konvergente Folge mit $a_n\to g_1$. Sei weiterhin
$g_1\ne g_2$. Es wird nun gezeigt, dass $g_2$ kein Grenzwert von $a_n$
sein kann. Wir müssen also zeigen:
\[\neg\lim_{n\to\infty} a_n=g_2 \iff
\exists\varepsilon{>}0\;\forall n_0\;\exists n{\ge}n_0\;
(a_n\notin U_\varepsilon(g_2))\]
mit $a_n\notin U_\varepsilon(g_2)\iff d(a_n,g_2)\ge\varepsilon$.

Um dem Existenzquantor zu genügen, wählt man nun
$\varepsilon = \frac{1}{2}d(g_1,g_2)$.
Nach Def. \ref{metric-space} (metric-space) gilt 
$d(g_1,g_2)>0$, daher ist auch $\varepsilon>0$. Nach Satz
\ref{construction-disjoint-ep-balls} sind die Umgebungen
$U_\varepsilon(g_1)$ und $U_\varepsilon(g_2)$ disjunkt.
Wegen $a_n\to g_1$ gibt es ein $n_0$ mit $a_n\in U_\varepsilon(g_1)$ für alle
$n\ge n_0$. Dann gibt es für jedes beliebig große $n_0$ aber auch
$n\ge n_0$ mit $a_n\notin U_\varepsilon(g_2)$.\,\qedsymbol
\end{Beweis}

\begin{Satz}[lim-scaled-ep: skaliertes Epsilon]\label{lim-scaled-ep}
Es gilt:
\[\lim_{n\to\infty} a_n=a \iff
\forall\varepsilon{>}0\;\exists n_0\;\forall n{\ge}n_0\;(\|a_n-a\|<R\varepsilon),\]
wobei $R>0$ ein fester aber beliebieger Skalierungsfaktor ist.
\end{Satz}

\begin{Beweis}
Betrachte $\varepsilon>0$ und multipliziere auf beiden Seiten
mit $R$. Dabei handelt es sich um eine Äquivalenzumformung.
Setze $\varepsilon':=R\varepsilon$. Demnach gilt:
\[\varepsilon>0 \iff \varepsilon'>0.\]
Nach der Ersetzungsregel düfen wir die Teilformel $\varepsilon>0$
nun ersetzen. Es ergibt sich die äquivalente Formel
\[\lim_{n\to\infty} a_n=a \iff
\forall\varepsilon'{>}0\;\exists n_0\;\forall n{\ge}n_0\;
(\|a_n-a\|<\varepsilon').\]
Das ist aber genau Def. \ref{def:lim} (lim).\,\qedsymbol
\end{Beweis}

\begin{Satz}
Es gilt:
\[\lim_{n\to\infty} a_n = a\implies \lim_{n\to\infty} \|a_n\| = \|a\|.\]
\end{Satz}

\begin{Beweis}
Nach Satz \ref{rev-tineq} (umgekehrte Dreiecksungleichung) gilt:
\[|\|a_n\|-\|a\|| \le \|a_n-a\| < \varepsilon.\]
Dann ist aber erst recht $|\|a_n\|-\|a\||<\varepsilon$.\,\qedsymbol
\end{Beweis}

\begin{Satz}\label{zero-seq-bounded}
Ist $(a_n)$ eine Nullfolge und $(b_n)$ eine beschränkte Folge,
dann ist auch $(a_n b_n)$ eine Nullfolge.
\end{Satz}

\begin{Beweis}
Wenn $(b_n)$ beschränkt ist, dann existiert nach
Def. \ref{def:bseq} (bseq) eine Schranke $S$ mit
$|b_n|<S$ für alle $n$. Man multipliziert nun auf beiden Seiten
mit $|a_n|$ und erhält
\[|a_n b_n| = |a_n| |b_n| < |a_n| S.\]
Wenn $a_n\to 0$, dann muss für jedes $\varepsilon$
ein $n_0$ existieren mit $|a_n|<\varepsilon$ für $n\ge n_0$.
Multipliziert man auf beiden Seiten mit $S$, und ergibt sich
\[|a_n b_n-0| = |a_n b_n| < |a_n| S < S\varepsilon.\]
Nach Satz \ref{lim-scaled-ep} (lim-scaled-ep) gilt dann
aber $a_n b_n\to 0$.\,\qedsymbol
\end{Beweis}

\begin{Satz}
Sind $(a_n)$ und $(b_n)$ Nullfolgen,
dann ist auch $(a_n b_n)$ eine Nullfolge.
\end{Satz}

\begin{Beweis}[Beweis 1]
Wenn $(b_n)$ eine Nullfolge ist, dann ist $(b_n)$ auch beschränkt.
Nach Satz \ref{zero-seq-bounded} gilt dann die Behauptung.
\end{Beweis}

\begin{Beweis}[Beweis 2]
Sei $\varepsilon>0$ beliebig.
Es gibt ein $n_0$, so dass
$|a_n|<\varepsilon$ und $|b_n|<\varepsilon$ für $n\ge n_0$.
Demnach ist
\[|a_n b_n| = |a_n| |b_n|< |a_n|\varepsilon <\varepsilon^2.\]
Wegen $\varepsilon>0\iff\varepsilon'>0$ mit
$\varepsilon'=\varepsilon^2$ gilt
\[\forall\varepsilon'{>}0\;\exists n_0\;\forall n{\ge}n_0\;
(|a_n b_n|<\varepsilon').\]
Nach Def. \ref{def:lim} (lim) gilt somit die Behauptung.\,\qedsymbol
\end{Beweis}

\newpage
\begin{Satz}[Grenzwertsatz zur Addition]%
\label{lim-add}\index{Grenzwertsaetze@Grenzwertsätze}
Seien $(a_n)$, $(b_n)$ Folgen von Vektoren eines normierten Raumes.
Es gilt:
\[\lim_{n\to\infty} a_n = a\land \lim_{n\to\infty} b_n
= b \implies \lim_{n\to\infty} a_n+b_n = a+b.\]
\end{Satz}

\begin{Beweis}
Dann gibt es ein $n_0$, so dass für $n\ge n_0$ sowohl
$\|a_n-a\|<\varepsilon$ als auch $\|b_n-b\|<\varepsilon$.
Addition der beiden Ungleichungen ergibt
\[\|a_n-a\| + \|b_n-b\| < 2\varepsilon.\]
Nach der Dreiecksungleichung, das ist Axiom (N3) in Def.
\ref{def:normed-space} (normed-space), gilt nun aber die Abschätzung
\[\|(a_n+b_n)-(a+b)\| = \|(a_n-a)+(b_n-b)\| \le \|a_n-a\|+\|b_n-b\|.\]
Somit gilt erst recht
\[\|(a_n+b_n)-(a+b)\| < 2\varepsilon.\]
Nach Satz \ref{lim-scaled-ep} (lim-scaled-ep)
folgt die Behauptung.\,\qedsymbol
\end{Beweis}

\begin{Satz}[Grenzwertsatz zur Skalarmultiplikation]\label{lim-smult}
Sei $(a_n)$ eine Folge von Vektoren eines normierten Raumes
und sei $r\in\R$ oder $r\in\C$. Es gilt:
\[\lim_{n\to\infty} a_n = a\implies \lim_{n\to\infty} ra_n\to ra.\]
\end{Satz}

\begin{Beweis}
Sei $\varepsilon>0$ fest aber beliebig. Es gibt nun ein $n_0$, so
dass $\|a_n-a\|<\varepsilon$ für $n\ge n_0$.
Multipliziert man auf beiden Seiten
mit $|r|$ und zieht Def. \ref{def:normed-space} (normed-space)
Axiom (N2) heran, dann ergibt sich
\[\|ra_n-ra\| = |r|\,\|a_n-a\|<|r|\varepsilon.\]
Nach Satz \ref{lim-scaled-ep} (lim-scaled-ep)
folgt die Behauptung.\,\qedsymbol
\end{Beweis}

\begin{Satz}[Grenzwertsatz zum Produkt]\mbox{}\\
Seien $(a_n)$ und $(b_n)$ Folgen
reeller Zahlen. Es gilt:
\[\lim_{n\to\infty} a_n=a\land\lim_{n\to\infty} b_n=b\implies
\lim_{n\to\infty} a_n b_n = ab.\]
\end{Satz}

\begin{Beweis}
Nach Voraussetzung sind $a_n-a$ und $b_n-b$ Nullfolgen.
Da das Produkt von Nullfolgen wieder eine Nullfolge ist, gilt
\[(a_n-a)(b_n-b) = a_n b_n-a_n b-ab_n+ab\to 0.\]
Da nach Satz \ref{lim-smult} aber $a_n b\to ab$ und $ab_n\to ab$,
ergibt sich nach Satz \ref{lim-add} nun
\[(a_n-a)(b_n-b)+a_n b+ab_n = a_n b_n+ab\to 2ab.\]
Addiert man nun noch die konstante Folge $-2ab$
und wendet nochmals Satz \ref{lim-add} an, dann ergibt sich
die Behauptung
\[a_n b_n\to ab.\,\qedsymbol\]
\end{Beweis}

\newpage
\begin{Satz}\label{cont-seqcont}%
\index{folgenstetig}\index{stetig!folgenstetig}
Sei $M$ ein metrischer Raum und $X$ ein topologischer Raum.
Eine Abbildung $f\colon M\to X$ ist genau dann stetig, wenn
sie folgenstetig ist.
\end{Satz}

\begin{Satz}[Satz zur Fixpunktgleichung]\index{Fixpunktgleichung}
Sei $M$ ein metrischer Raum und sei $f\colon M\to M$.
Sei $x_{n+1}:=f(x_n)$ eine Fixpunktiteration. Wenn die Folge
$(x_n)$ zu einem Startwert $x_0$ konvergiert mit $x_n\to x$, und
wenn $f$ eine stetige Abbildung ist, dann muss der Grenzwert $x$ die
Fixpunktgleichung $x=f(x)$ erfüllen.
\end{Satz}

\begin{Beweis}
Wenn $x_n\to x$, dann gilt trivialerweise auch $x_{n+1}\to x$.
Weil $f$ stetig ist, ist $f$ nach Satz \ref{cont-seqcont}
auch folgenstetig. Daher gilt $\lim f(a_n) = f(\lim a_n)$ für jede
konvergente Folge $(a_n)$. Somit gilt:
\[x=\lim_{n\to\infty} x_{n+1} = \lim_{n\to\infty} f(x_n)
= f(\lim_{n\to\infty} x_n) = f(x).\;\qedsymbol\]
\end{Beweis}

\subsection{Wachstum und Landau-Symbole}
\begin{Definition}\label{Landau-O}
Seien $f,g\colon D\to\R$ mit $D=\N$ oder $D=\R$. Man sagt, die
Funktion $f$ wächst nicht wesentlich schneller als $g$, kurz
$f\in\mathcal O(g)$, genau dann, wenn
\[\exists(c>0)\exists(x_0>0)\forall(x>x_0)(|f(x)|\le c|g(x)|).\]
\end{Definition}

\begin{Korollar}
Ist $r\in\R$ mit $r\ne 0$ eine Konstante, dann gilt
$\mathcal O(rg)=\mathcal O(g)$.
\end{Korollar}
\begin{Beweis}
Nach Def. \ref{Landau-O} ist
\[f\in\mathcal O(rg) \iff 
\exists(c>0)\exists(x_0>0)\forall(x>x_0)(|f(x)|\le c|rg(x)|).\]
Man hat nun
\[|f(x)|\le c|rg(x)| = c\cdot |r|\cdot |g(x)|.\]
\end{Beweis}
Wegen $r\ne 0$ ist $|r|>0$ und daher auch $c>0\iff c|r|>0$. Sei
$c':=r|c|$. Also gilt $c>0\iff c'>0$. Nach der Ersetzungsregel
darf $c>0$ gegen $c'>0$ ersetzt werden und man erhält die
äquivalente Bedingung
\[\exists(c'>0)\exists(x_0>0)\forall(x>x_0)(|f(x)|\le c'|g(x)|).\]
Nach Def. \ref{Landau-O} ist das gerade $f\in\mathcal O(g)$.\;\qedsymbol

\section{Stetige Funktionen}

\begin{Definition}[Grenzwert einer Funktion]\label{fn-lim}
Sei $f\colon D\to\R$ mit $D\subseteq\R$ und sei $p$ ein
Häufungspunkt von $D$. Die Funktion $f$ heißt konvergent
gegen $L$ für $x\to p$, wenn%
\[\forall(\varepsilon>0)\exists(\delta>0)\forall(x\in D)(
  0<|x-x_0|<\delta\implies |f(x)-L|<\varepsilon).\]
Bei Konvergenz schreibt man $L=\lim\limits_{x\to p} f(x)$ und nennt $L$ den Grenzwert.
\end{Definition}

\begin{Definition}[cont: stetig]\label{cont}
Eine Funktion $f\colon D\to\R$ mit $D\subseteq\R$ heißt stetig an der
Stelle $x_0\in D$, wenn
\[\forall(\varepsilon>0)\exists(\delta>0)\forall(x\in D)(
  |x-x_0|<\delta\implies |f(x)-f(x_0)|<\varepsilon).\]
\end{Definition}

\begin{Definition}[Lipschitz-stetig]\mbox{}\\
Eine Funktion $f\colon D\to\R$ mit $D\subseteq\R$ heißt
Lipschitz"=stetig, wenn eine Konstante $L$ existiert, so dass
\[|f(b)-f(a)|\le L|b-a|\]
für alle $a,b\in D$.
\end{Definition}

\begin{Definition}[Lipschitz-stetig an einer Stelle]%
\label{Lipschitz-cont-at}\mbox{}\\
Eine Funktion $f\colon D\to\R$ mit $D\subseteq\R$ heißt
Lipschitz"=stetig an der Stelle $x_0\in D$, wenn eine Konstante $L$
existiert, so dass
\[|f(x_0)-(a)|\le L|x_0-a|\]
für alle $a\in D$.
\end{Definition}

\begin{Korollar}
Eine Funktion ist genau dann Lipschitz"=stetig, wenn sie an jeder
Stelle Lipschitz"=stetig ist und die Menge der optimalen
Lipschitz"=Konstanten dabei beschränkt.
\end{Korollar}
\begin{Beweis}
Eine Lipschitz"=stetige Funktion ist trivialerweise an jeder Stelle
Lipschitz"=stetig. Ist $f\colon D\to\R$ an der Stelle $b$ Lipschitz"=stetig,
dann existiert eine Lipschitz"=Konstante $L_b$ mit%
\[\forall(a\in D)(|f(b)-f(a)|\le L_b |b-a|).\]
Nach Voraussetzung ist $L=\sup_{b\in D} L_b$ endlich. Alle $L_b$ können
nun zu $L$ abgeschwächt werden und es ergibt sich%
\[\forall(b\in D)\forall(a\in D)(|f(b)-f(a)|\le L|b-a|).\;\qedsymbol\]
\end{Beweis}


\begin{Definition}[lokal Lipschitz-stetig]\mbox{}\\
Eine Funktion $f\colon D\to\R$ mit $D\subseteq\R$ heißt lokal
Lipschitz"=stetig in der Nähe einer Stelle $x_0\in D$, wenn es eine
Epsilon"=Umgebung $U_\varepsilon(x_0)$ gibt, so dass die Einschränkung
von $f$ auf diese Umgebung Lipschitz"=stetig ist. Die Funktion heißt
lokal Lipschitz"=stetig, wenn sie in der Nähe jeder Stelle
Lipschitz"=stetig ist.
\end{Definition}

\begin{Satz}\label{diff-nh-Lipschitz-cont-at}
Ist die Funktion $f\colon D\to\R$ an der Stelle $x_0$ differenzierbar,
dann gibt es ein $\delta>0$, so dass die Einschränkung von $f$
auf $U_\delta(x_0)$ an der Stelle $x_0$ Lipschitz"=stetig ist.
\end{Satz}

\begin{Beweis}
Def. \ref{fn-lim} wird in Def. \ref{diff} (diff) eingesetzt.
Es ergibt sich:
\[0<|x-x_0|<\delta\implies
\left|\frac{f(x)-f(x_0)}{x-x_0}-f'(x_0)\right|<\varepsilon.\]
Nach der umgekehrten Dreiecksungleichung \ref{rev-tineq} gilt
\[\left|\frac{f(x)-f(x_0)}{x-x_0}\right|-|f'(x_0)| \le
\left|\frac{f(x)-f(x_0)}{x-x_0}-f'(x_0)\right|
< \varepsilon.\]
Daraus ergibt sich
\[|f(x)-f(x_0)| < (|f'(x_0)|+\varepsilon)\cdot |x-x_0|\]
und somit erst recht
\[|f(x)-f(x_0)| \le (|f'(x_0)|+\varepsilon)\cdot |x-x_0|,\]
wobei jetzt auch $x=x_0$ erlaubt ist. Demnach wird Def.
\ref{Lipschitz-cont-at} erfüllt:
\[\exists(\delta>0)\forall(x\in U_\delta(x_0))(
|f(x)-f(x_0)| \le (|f'(x_0)|+\varepsilon)\cdot |x-x_0|).\;\qedsymbol\]
\end{Beweis}

\begin{Satz}\label{diff-bounded-Lipschitz-cont}
Eine differenzierbare Funktion ist genau dann Lipschitz"=stetig,
wenn ihre Ableitung beschränkt ist.
\end{Satz}
\begin{Beweis}
Wenn $f\colon I\to\R$ Lipschitz"=stetig ist, dann gibt es $L$ mit
\[\left|\frac{f(b)-f(a)}{b-a}\right|\le L\]
für alle $a,b\in D$ mit $a\ne b$. Daraus folgt
\[|f'(a)| = \left|\lim_{b\to a} \frac{f(b)-f(a)}{b-a}\right|
= \lim_{b\to a} \left|\frac{f(b)-f(a)}{b-a}\right|
\le L.\]
Demnach ist die Ableitung beschränkt.

Sei nun umgekehrt die Ableitung beschränkt. Für $a,b\in I$ mit $a\ne b$
gibt es nach dem Mittelwertsatz ein $x_0\in(a,b)$, so dass
\[|f'(x_0)| = \left|\frac{f(b)-f(a)}{b-a}\right|.\]
Da die Ableitung beschränkt ist gibt es ein Supremum
$L = \sup_{x\in I} |f'(x)|$. Demnach ist $|f'(x)|\le L$ für alle $x$.
Es ergibt sich
\[\left|\frac{f(b)-f(a)}{b-a}\right|\le L|b-a| \implies |f(b)-f(a)|\le L|b-a|.\]
Nun darf auch $a=b$ gewählt werden.\;\qedsymbol
\end{Beweis}

\begin{Satz}\label{diff-compact-Lipschitz-cont}
Eine auf einem kompakten Intervall $[a,b]$ definierte stetig
differenzierbare Funktion ist Lipschitz"=stetig.
\end{Satz}
\begin{Beweis}
Sei $f\colon [a,b]\to\R$ stetig differenzierbar. Dann ist $f'(x)$ stetig.
Nach dem Satz vom Minimum und Maximum ist $|f'(x)|$ beschränkt. Nach
Satz \ref{diff-bounded-Lipschitz-cont} muss $f$ Lipschitz"=stetig
sein.\;\qedsymbol
\end{Beweis}

\begin{Korollar}
Eine stetig differenzierbare Funktion ist lokal Lipschitz"=stetig.
\end{Korollar}
\begin{Beweis}
Sei $f\colon D\to\R$ stetig differenzierbar. Sei $[a,b]\in D$. Sei
$x_0\in [a,b]$. Die Einschränkung von $f$ auf $[a,b]$ ist
Lipschitz"=stetig nach Satz \ref{diff-compact-Lipschitz-cont}.
Dann ist auch die Einschränkung von $f$ auf
$U_\varepsilon(x_0)\subseteq [a,b]$ Lipschitz"=stetig.\;\qedsymbol
\end{Beweis}

\begin{Satz}
Es gibt differenzierbare Funktionen, die nicht überall lokal
Lipschitz"=stetig sind.
\end{Satz}
\begin{Beweis}
Aus Satz \ref{diff-bounded-Lipschitz-cont} ergibt sich also
Kontraposition, dass eine Funktion mit unbeschränkter Ableitung
nicht Lipschitz"=stetig sein kann.

Ist $f\colon D\to\R$ an jeder Stelle differenzierbar und ist $f'$
in jeder noch so kleinen Umgebung der Stelle $x_0$ unbeschränkt, dann
kann $f$ also in der Nähe dieser Stelle auch nicht lokal
Lipschitz"=stetig sein.

Ein Beispiel für eine solche Funktion ist $f\colon{}[0,\infty)\to\R$
mit
\[f(0):=0\quad \text{und}\quad f(x):=x^{3/2}\cos\Big(\tfrac{1}{x}\Big).\]
Einerseits gilt
\[f'(0) = \lim_{h\to 0}\frac{f(0+h)-f(0)}{h} = \lim_{h\to 0}\frac{f(h)}{h}
= \lim_{h\to 0} (h^{1/2}\cos\Big(\tfrac{1}{h}\Big)) = 0.\]
Die Funktion ist also an der Stelle $x=0$ differenzierbar.
Andererseits gilt nach den Ableitungsregeln%
\[f'(x) = \frac{3}{2}\sqrt{x}\cos\Big(\tfrac{1}{x}\Big)+\frac{1}{\sqrt{x}}\sin\Big(\tfrac{1}{x}\Big).\]
für $x>0$. Der Term $\tfrac{1}{\sqrt{x}}$ erwirkt für $x\to 0$ immer
größere Maxima von $|f'(x)|$. Daher kann $f$ in der Nähe von $x=0$ nicht
lokal Lipschitz"=stetig sein.\;\qedsymbol
\end{Beweis}

\begin{Satz}
Sei $f\colon\R\to\R$ differenzierbar und $f(x)$ konvergent
für $x\to\infty$. Ist außerdem $f'$  Lipschitz-stetig,
zieht dies $f'(x)\to 0$ für $x\to\infty$ nach sich.
\end{Satz}
\begin{Beweis}
Gemäß dem cauchyschen Konvergenzkriterium gibt es zu jedem
$\varepsilon>0$ eine Stelle $x_0$, so dass
\begin{equation}
|f(b)-f(a)| < \varepsilon
\end{equation}
für alle $a,b$ mit $x_0 < a \le b$. Nun ist $f'$ aufgrund
der Lipschitz-Stetigkeit erst recht stetig, womit
\begin{equation}
\bigg|\int_a^b f'(x)\,\mathrm dx\bigg| = |f(b)-f(a)|
\end{equation}
laut dem Fundamentalsatz gilt. Gezeigt wird nun, dass $|f'(a)|$
beschränkt ist. Sei dazu $L$ die Lipschitz-Konstante. Ohne
Beschränkung der Allgemeinheit sei $f'(a)>0$. Fallen darf $f'$ maximal
mit dem Anstieg $-L$. Geschieht dies linear bis zur Nullstelle $b$,
ergibt sich ein rechtwinkliges Dreieck mit dem Flächeninhalt
\begin{equation}
\frac{1}{2L} f'(a)^2 = \int_a^b f'(x)\,\mathrm dx < \varepsilon.
\end{equation}
Demnach ist $f'(a) < \sqrt{2L\varepsilon}$. Weil dies für alle $a>x_0$
gilt, muss $f'$ jede Beschränkung unterbieten, womit
der Beweis der Behauptung erbracht ist.\;\qedsymbol
\end{Beweis}

\noindent
Die Diskussion Gegenbeispiels $f(0):=0$, $f(x):=\sin(x^2)/x$ macht
ersichtlich, dass die Aussage ohne Lipschitz-Stetigkeit nicht einmal
für glatte Funktionen gilt.

\newpage
\section{Differentialrechnung}

\subsection{Ableitungsregeln}

\begin{Definition}[diff: differenzierbar, Ableitung]%
\label{diff}\index{differenzierbar}\index{Ableitung}
Eine Funktion $f\colon D\to\R$ heißt differenzieraber an der Stelle
$x_0\in D$, wenn der Grenzwert%
\[f'(x_0) = \lim_{x\to x_0}\frac{f(x)-f(x_0)}{x-x_0}
= \lim_{h\to 0}\frac{f(x_0+h)-f(x_0)}{h}\]
existiert. Man nennt $f'(x_0)$ die Ableitung von $f$ an der Stelle
$x_0$.
\end{Definition}

\begin{Satz}\index{Produktregel}
Sei $I$ ein Intervall und $f,g\colon I\to\R$. Sind $f,g$
differenzierbar an der Stelle $x\in I$, dann ist auch%
\begin{align}
f+g&\;\text{dort differenzierbar mit}\;(f+g)'(x)=f'(x)+g'(x),\\
f-g&\;\text{dort differenzierbar mit}\;(f-g)'(x)=f'(x)-g'(x),\\
\label{eq:diff-mul}
fg&\;\text{dort differenzierbar mit}\;(fg)'(x)=f'(x)g(x)+f(x)g'(x).
\end{align}
\end{Satz}

\begin{Beweis} Es gilt
\begin{gather}
(f+g)'(x)
= \lim_{h\to 0}\frac{(f+g)(x+h)-(f+g)(x)}{h}\\
= \lim_{h\to 0}\frac{(f(x+h)+g(x+h))-(f(x)+g(x))}{h}\\
= \lim_{h\to 0}\bigg(\frac{f(x+h)-f(x)}{h}+\frac{g(x+h)-g(x)}{h}\bigg)\\
= \lim_{h\to 0}\frac{f(x+h)-f(x)}{h}+\lim_{h\to 0}\frac{g(x+h)-g(x)}{h}
= f'(x)+g'(x).
\end{gather}
Da die Grenzwerte auf der rechten Seite nach Voraussetzung existieren,
muss auch der Grenzwert der Summe existieren.
Die Rechnung für die Subtraktion ist analog.

Bei der Multiplikation wird ein Nullsummentrick angewendet:
\begin{gather}
g(x)f'(x)+f(x)g'(x)
= g(x)\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}
+ f(x)\lim_{h\to 0}\frac{g(x+h)-g(x)}{h}\\
= \lim_{h\to 0}\bigg[g(x+h)\frac{f(x+h)-f(x)}{h}\bigg]
+ \lim_{h\to 0}\bigg[f(x)\frac{g(x+h)-g(x)}{h}\bigg]\\
= \lim_{h\to 0}\frac{f(x+h)g(x+h)-f(x)g(x+h)}{h}
+ \lim_{h\to 0}\frac{f(x)g(x+h)-f(x)g(x)}{h}\\
= \lim_{h\to 0}\frac{f(x+h)g(x+h)-f(x)g(x+h)+f(x)g(x+h)-f(x)g(x)}{h}\\
= \lim_{h\to 0}\frac{f(x+h)g(x+h)-f(x)g(x)}{h}
= \lim_{h\to 0}\frac{(fg)(x+h)-(fg)(x)}{h}
= (fg)'(x).
\end{gather}
Hierbei wurde $\lim_{h\to 0}g(x+h)=g(x)$ benutzt, was richtig ist,
weil $g$ an der Stelle $x$ differenzierbar ist und dort somit ganz
sicher stetig.\;\qedsymbol
\end{Beweis}

\begin{Satz}
Sei $I$ ein Intervall. Sind $f,g\colon I\to\R$ an der Stelle
$x$ differenzierbar und ist $g(x)\ne 0$, dann
ist auch $f/g$ differenzierbar und es gilt
\begin{equation}
\bigg(\frac{f}{g}\bigg)'(x) = \frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}.
\end{equation}
\end{Satz}
\begin{Beweis}
Nach der Produktregel \eqref{eq:diff-mul} gilt
\begin{equation}
0 = 1' = \bigg(g\cdot\frac{1}{g}\bigg)'
= g'\cdot\frac{1}{g}+g\cdot \bigg(\frac{1}{g}\bigg)'.
\end{equation}
Umstellen bringt $(1/g)'(x)=-g'(x)/g(x)^2$. Nochmalige Anwendung der
Produktregel \eqref{eq:diff-mul} bringt
\begin{align}
\bigg(\frac{f}{g}\bigg)'(x)
&= \bigg(f\cdot\frac{1}{g}\bigg)'(x)
= f'(x)\cdot\frac{1}{g(x)}+f(x)\bigg(\frac{1}{g}\bigg)'(x)\\
&= \frac{f'(x)}{g(x)}-\frac{f(x)g'(x)}{g(x)^2}
= \frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}.\;\qedsymbol
\end{align}
\end{Beweis}

\begin{Satz}\label{diff-power}
Für $f\colon\R\to\R$, $f(x):=x^n$ mit $n\in\N$ gilt
$f'(x)=nx^{n-1}$.
\end{Satz}
\begin{Beweis}[Beweis 1]
Heranziehung des binomischen Lehrsatzes bringt
\begin{align}
f'(x) &= \lim_{h\to 0}\frac{(x+h)^n-x^n}{h}
= \lim_{h\to 0}\frac{\sum_{k=0}^n\binom{n}{k}x^{n-k} h^k-x^n}{h}\\
&= \lim_{h\to 0}\bigg(nx^{n-1}+\sum_{k=2}^n\binom{n}{k}x^{n-k}h^{k-1}\bigg)
= nx^{n-1}.\;\qedsymbol
\end{align}
\end{Beweis}
\begin{Beweis}[Beweis 2]
Induktiv. Der Induktionsanfang $\tfrac{\mathrm d}{\mathrm dx}x=1$ ist klar.
Induktionsschritt mittels Produktregel \eqref{eq:diff-mul}:
\begin{align}
\tfrac{\mathrm d}{\mathrm dx} x^n = \tfrac{\mathrm d}{\mathrm dx} (x\cdot x^{n-1})
= x^{n-1}+x\tfrac{\mathrm d}{\mathrm dx}x^{n-1}
= x^{n-1}+(n-1)x^{n-1} = nx^{n-1}.\;\qedsymbol
\end{align}
\end{Beweis}

\begin{Satz}
Für $f\colon\R{\setminus}\{0\}\to\R$, $f(x):=x^n$ mit $n\in\Z$
gilt $f'(x)=nx^{n-1}$.
\end{Satz}
\begin{Beweis}
Der Fall $n=0$ ist trivial und $n\ge 1$ wurde schon in Satz
\ref{diff-power} gezeigt. Sei nun $a\in\N$ und $n=-a$. Nach der
Produktregel \eqref{eq:diff-mul} und Satz \ref{diff-power} gilt
\begin{equation}
0 = \tfrac{\mathrm d}{\mathrm dx} 1
= \tfrac{\mathrm d}{\mathrm dx} (x^a x^{-a})
= x^{-a}\tfrac{\mathrm d}{\mathrm dx} x^a+x^a\tfrac{\mathrm d}{\mathrm dx} x^{-a}
= x^{-a}ax^{a-1}+x^a\tfrac{\mathrm d}{\mathrm dx} x^{-a}.
\end{equation}
Dividiert man nun durch $x^a$ und formt um, dann ergibt sich
\begin{equation}
\tfrac{\mathrm d}{\mathrm dx} x^{-a} = -ax^{-a-1}
\implies \tfrac{\mathrm d}{\mathrm dx} x^n = nx^{n-1}.\;\qedsymbol
\end{equation}
\end{Beweis}

\newpage
\subsection{Glatte Funktionen}

\begin{Satz}
Sei $f\colon\R\to\R$ eine Funktion mit der Eigenschaft
$f(x)=0$ für $x\le 0$ und $f(x)>0$ für $x>0$. Es gibt glatte Funktionen
mit dieser Eigenschaft, jedoch keine analytischen.
\end{Satz}

\begin{Beweis}
Wegen $f(x)=0$ für $x\le 0$ muss die linksseitige $n$-te Ableitung
an der Stelle $x=0$ immer verschwinden. Wenn die $n$-te Ableitung
stetig sein soll, muss auch die rechtsseitige Ableitung bei $x=0$
verschwinden. Da die Funktion glatt sein soll, muss das für jede
Ableitung gelten. Daher verschwindet die Taylorreihe an der Stelle
$x=0$. Da aber $f(x)>0$ für $x>0$, gibt es keine noch so kleine
Umgebung mit Übereinstimmung von $f$ und ihrer Taylorreihe.
Daher kann $f$ an der Stelle $x=0$ nicht analytisch sein.

Eine glatte Funktion lässt sich jedoch konstruieren:
\[f(x):=\begin{cases}
\ee^{-1/x}&\text{wenn}\;x>0,\\
0&\text{wenn}\;x\le 0.
\end{cases}\]
Ist nämlich $g(x)$ an einer Stelle glatt, dann ist
es nach Kettenregel, Produktregel und Summenregel auch $\ee^{g(x)}$.
Die $n$-te Ableitung lässt sich immer in der Form%
\[\sum\nolimits_k e^{g(x)}{r_k(x)}
= e^{g(x)}\sum\nolimits_k r_k(x) = e^{g(x)}r(x)\]
darstellen, wobei die $r_k(x)$ bzw. $r(x)$ in diesem Fall rationale
Funktionen mit Polstelle bei $x=0$ sind. Da aber $e^{-1/x}$ für
$x\to 0$ schneller fällt als jede rationale Funktion steigen kann,
muss die rechtsseitige Ableitung an der Stelle $x=0$ immer
verschwinden.\;\qedsymbol
\end{Beweis}

\section{Fixpunkt-Iterationen}%
\index{Fixpunkt-Iteration}

\begin{Definition}[Kontraktion]\index{Kontraktion}
Sei $(M,d)$ ein vollständiger metrischer Raum. Eine Abbildung
$\varphi\colon M\to M$ heißt Kontraktion, wenn sie
Lipschitz"=stetig mit Lipschitz"=Konstante $L<1$ ist, d.\,h.
\[d(\varphi(x),\varphi(y))<L\,d(x,y)\]
für alle $x,y\in M$.
\end{Definition}

\begin{Satz}[Fixpunktsatz von Banach]\label{Banach-fixed-point-theorem}%
\index{Fixpunktsatz von Banach}\index{Banach!Fixpunktsatz von}
Sei $(M,d)$ ein nichtleerer vollständiger metrischer Raum
und sei $\varphi\colon M\to M$ eine Kontraktion. Es gibt genau
einen Fixpunkt $x\in M$ mit $x=\varphi(x)$ und die Folge
$(x_n)\colon\N\to M$ mit $x_{n+1}=\varphi(x_n)$ konvergiert
gegen den Fixpunkt, unabhängig vom Startwert $x_0$.
\end{Satz}

\begin{Satz}[Hinreichendes Konvergenzkriterium]\label{diff-fixed-point-iter}
Sei $M=[a,b]$. Ist $\varphi\colon M\to M$ differenzierbar und gibt es
eine Zahl $r$ mit $|\varphi'(x)|<r<1$ für alle $x\in M$, dann
hat $\varphi$ genau einen Fixpunkt und die Folge $(x_n)$ mit $x_{n+1}=\varphi(x_n)$
konvergiert für jeden Startwert $x_0\in M$ gegen diesen Fixpunkt.
\end{Satz}
\begin{Beweis}
Nach Satz \ref{diff-bounded-Lipschitz-cont} ist eine differenzierbare
Funktion $\varphi$ mit beschränkter Ableitung auch Lipschitz"=stetig,
und $L=\sup_{x\in M}|\varphi'(x)|$ eine Lipschitz"=Konstante.
Wegen $|\varphi'(x)|<r$ muss $L\le r$ sein, und somit $L<1$.
D.\,h. $\varphi$ ist eine Kontraktion. Die Konvergenz der Folge
$(x_n)$ ist gemäß Satz \ref{Banach-fixed-point-theorem}
gewährleistet.\;\qedsymbol
\end{Beweis}
\newpage

\begin{Satz}[Hinreichendes Konvergenzkriterium zum Newton-Verfahren]%
\index{Newton-Verfahren}\mbox{}\\
Sei $f\colon [a,b]\to\R$ zweimal stetig differenzierbar und
$f'(x)\ne 0$ für alle $x$. Sei%
\[\varphi\colon [a,b]\to [a,b],\quad \varphi(x):=x-\frac{f(x)}{f'(x)}.\]
Man beachte $\varphi([a,b])\subseteq [a,b]$. Gilt für alle $x$ die Ungleichung%
\[|\varphi'(x)| = \bigg|\frac{f(x)f''(x)}{f'(x)^2}\bigg| < 1,\]
dann besitzt $f$ genau eine Nullstelle und die Folge $(x_n)$ mit
$x_{n+1}=\varphi(x_n)$ konvergiert gegen diese Nullstelle.
\end{Satz}

\begin{Beweis}
Gemäß den Ableitungsregeln ist $\varphi$ stetig differenzierbar
und es gilt%
\[\varphi'(x) = 1-\frac{f'(x)f'(x)-f(x)f''(x)}{f'(x)^2}
= \frac{f(x)f''(x)}{f'(x)^2}.\]
Da $|\varphi'(x)|$ stetig ist, gibt es nach dem Satz vom Minimum
und Maximum ein Maximum $M$ und nach Voraussetzung ist $M<1$.
Man setze nun $r:=(M+1)/2$. Dann ist $|\varphi'(x)|<r<1$.
Gemäß Satz \ref{diff-fixed-point-iter} konvergiert die Iteration
$(x_n)$ gegen den einzigen Fixpunkt von $\varphi$. Wegen $f'(x)\ne 0$
gilt dabei%
\[x = \varphi(x) = x-\frac{f(x)}{f'(x)} \iff \frac{f(x)}{f'(x)}=0\iff f(x)=0.\]
Der Fixpunkt von $\varphi$ ist also die einzige Nullstelle von $f$.\;\qedsymbol
\end{Beweis}

