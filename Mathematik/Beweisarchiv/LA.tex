
\chapter{Lineare Algebra}
\section{Matrizen}
\subsection{Definitionen}
\begin{Definition}[Transponierte Matrix]%
\index{transponierte Matrix}
Sei $R$ ein Ring und $A\in R^{m\times n}$ eine Matrix.
Die Matrix $A^T\in R^{n\times m}$ mit $(A^T)_{ij} := A_{ji}$
heißt Transponierte von $A$.
\end{Definition}

\begin{Definition}[Konjugierte Matrix]%
\index{konjugierte Matrix}
Sei $A\in\C^{m\times n}$. Die Matrix $\overline A$ mit
$(\overline A)_{ij} := \overline{A_{ij}}$
heißt konjugierte Matrix zu $A$. Mit $\overline{A_{ij}}$ ist
die Konjugation der komplexen Zahl $A_{ij}$ gemeint.
\end{Definition}

\begin{Definition}[Adjungierte Matrix]%
\index{adjungierte Matrix}
Sei $A\in\C^{m\times n}$. Die Adjungierte zu $A$ ist
definiert als $A^H:=(\overline A)^T$, d.\,h.
die Transponierte der konjugierten Matrix zu $A$.
\end{Definition}

\begin{Definition}[Inverse Matrix]%
\label{matrix-inv}\index{inverse Matrix}
Sei $K$ ein Körper und $A\in K^{n\times n}$ eine quadratische
Matrix. Man nennt $A$ invertierbar, wenn es eine Matrix $B$ gibt,
mit $AB=BA=E_n$, wobei $E_n$ die Einheitsmatrix ist. Die Matrix
$A^{-1}:=B$ heißt dann inverse Matrix zu $A$.
\end{Definition}

\subsection{Rechenregeln}

\begin{Satz}\label{matrix-mul-tp}
Sei $R$ ein kommutativer Ring. Für Matrizen $A\in R^{m\times n}$ und
$B\in R^{n\times p}$ gilt
\[(AB)^T = B^T A^T.\]
\end{Satz}
\begin{Beweis} Es gilt:
\begin{align}
(AB)^T &= \bigg(\sum_{k=1}^n A_{ik}B_{kj}\bigg)^T
= \bigg(\sum_{k=1}^n A_{jk}B_{ki}\bigg)
= \bigg(\sum_{k=1}^n B_{ki} A_{jk}\bigg)\\
&= \bigg(\sum_{k=1}^n (B^T)_{ik} (A^T)_{kj}\bigg)
= B^T A^T.\;\qedsymbol
\end{align}
\end{Beweis}

\begin{Satz}
Sei $A\in K^{n\times n}$ eine invertierbare Matrix. Dann ist auch
$A^T$ invertierbar und es gilt $(A^{-1})^T = (A^T)^{-1}$.
\end{Satz}
\begin{Beweis}
Aus $E=A^{-1}A=AA^{-1}$ und Satz \ref{matrix-mul-tp} folgt
\begin{equation}
E = E^T = (A^{-1}A)^T = A^T (A^{-1})^T = (AA^{-1})^T = (A^{-1})^T A^T.
\end{equation}
Dann muss $A^T$ nach Def. \ref{matrix-inv} die inverse Matrix
zu $(A^{-1})^T$ sein.\;\qedsymbol
\end{Beweis}

\newpage
\begin{Satz}\label{Verschiebungseigenschaft}
Sei $v\in\R^n$ und $w\in\R^m$. Sei $A\in\R^{m\times n}$. Es gilt
$\langle Av,w\rangle = \langle v,A^Tw\rangle$,
wobei links das Standardskalarprodukt auf dem $\R^m$ und
rechts das auf dem $\R^n$ ausgewertet wird.
\end{Satz}
\begin{Beweis}
Identifiziert man die Vektoren $x,y\in\R^k$ mit den
Matrizen $x,y\in\R^{k\times 1}$, dann
ist $\langle x,y\rangle = x^T y$. Gemäß Satz
\ref{matrix-mul-tp} darf man rechnen:
\[\langle Av,w\rangle
= (Av)^T w = v^T A^T w = \langle v,A^Tw\rangle.\;\qedsymbol\]
\end{Beweis}

\subsection{Rechenregeln für komplexe Matrizen}
\begin{Satz}\label{matrix-mul-conj}
Für Matrizen $A\in\C^{m\times n}$ und $B\in\C^{n\times p}$ gilt
\[\overline{AB} = \overline A\cdot\overline B\]
\end{Satz}
\begin{Beweis}
Es gilt
\begin{align*}
\overline{AB}
&= \overline{\bigg(\sum_{k=1}^n A_{ik}B_{kj}\bigg)}
= \bigg(\sum_{k=1}^n \overline{A_{ik}B_{kj}}\bigg)
= \bigg(\sum_{k=1}^n \overline{A_{ik}}\cdot\overline{B_{kj}}\bigg)
= \bigg(\sum_{k=1}^n (\overline A)_{ik}(\overline B)_{kj}\bigg)
= \overline A\cdot\overline B.\;\qedsymbol
\end{align*}
\end{Beweis}

\begin{Satz}\label{matrix-mul-tp-conj}
Für Matrizen $A\in\C^{m\times n}$ und $B\in\C^{n\times p}$ gilt
\[(AB)^H = B^H A^H.\]
\end{Satz}
\begin{Beweis}
Gemäß Satz \ref{matrix-mul-conj} und \ref{matrix-mul-tp} gilt
\[(AB)^H = (\overline{AB})^T = (\overline A\cdot\overline B)^T
= (\overline B)^T(\overline A)^T = B^H A^H.\]
\end{Beweis}

\begin{Satz}\label{Verschiebungseigenschaft-komplex}
Sei $v\in\C^n$ und $w\in\C^m$. Sei $A\in\C^{m\times n}$.
Es gilt $\langle Av,w\rangle = \langle v,A^H w\rangle$,
wobei links das Standardskalarprodukt auf dem $\C^m$ ausgewertet
wird und rechts das auf dem $\C^n$.
\end{Satz}
\begin{Beweis}
Identifiziert man die Vektoren $x,y\in\C^k$ mit den Matrizen
$x,y\in\C^{k\times 1}$, dann gilt $\langle x,y\rangle = x^H y$.
Gemäß Satz \ref{matrix-mul-tp-conj} darf man rechnen
\[\langle Av,w\rangle = (Av)^H w = v^H A^H w
= \langle v,A^Hw\rangle.\;\qedsymbol\]
\end{Beweis}

%\newpage
\section{Eigenwerte}

\begin{Satz}
Gegeben sei eine quadratische Matrix $A\in\R^{n\times n}$.
Dann ist die Matrix $M=A^T A$ symmetrisch und besitzt nur
nichtnegative Eigenwerte, speziell bei $\det(A)\ne 0$ nur positive.
\end{Satz}

\begin{Beweis}
Gemäß Satz \ref{matrix-mul-tp} gilt
\begin{equation}
M^T = (A^T A)^T = A^T (A^T)^T = A^T A = M.
\end{equation}
Ist nun $\lambda$ ein Eigenwert von $M$ und $v$ ein Eigenvektor dazu,
dann gilt $Mv=\lambda v$. Unter Anwendung von Satz
\ref{Verschiebungseigenschaft} folgt daraus
\begin{equation}
\lambda |v|^2 = \langle\lambda v,v\rangle
= \langle Mv,v\rangle = \langle A^T Av,v\rangle
= \langle Av,Av\rangle = |Av|^2\ge 0.
\end{equation}
Ergo ist $\lambda|v|^2\ge 0$. Unter der Voraussetzung $v\ne 0$ ist
$|v|>0$. Dann muss auch $\lambda\ge 0$ sein. Wenn nun $\det(A)\ne 0$
ist, also $A$ eine reguläre Matrix, dann hat $A$ trivialen Kern,
also $Av=0$ nur im Fall $v=0$. Da $v\ne 0$ vorausgesetzt wurde,
muss auch $Av\ne 0$, und damit $|Av|>0$ sein. Dann ist auch
$\lambda>0$. Alternativ folgt $\lambda>0$ daraus, dass $\det(A)$
das Produkt der Eigenwerte ist.\;\qedsymbol
\end{Beweis}

\begin{Satz}
Gegeben sei eine Matrix $A\in\C^{m\times n}$.
Dann ist die Matrix $M=A^H A$ hermitisch und besitzt nur
nichtnegative reelle Eigenwerte.
\end{Satz}
\begin{Beweis}
Gemäß Satz \ref{matrix-mul-tp-conj} gilt
\begin{equation}
M^H = (A^H A)^H = A^H (A^H)^H = A^H A = M.
\end{equation}
Ist nun $\lambda$ ein Eigenwert von $M$ und $v$ ein Eigenvektor dazu,
dann gilt $Mv=\lambda v$. Unter Anwendung von Satz
\ref{Verschiebungseigenschaft-komplex} folgt daraus
\begin{equation}
\lambda |v|^2 = \langle\lambda v,v\rangle
= \langle Mv,v\rangle = \langle A^H Av,v\rangle
= \langle Av,Av\rangle = |Av|^2\ge 0.
\end{equation}
Ergo ist $\lambda|v|^2\ge 0$. Unter der Voraussetzung $v\ne 0$
ist $|v|>0$. Dann muss auch $\lambda\ge 0$ sein.\;\qedsymbol
\end{Beweis}

\begin{Definition}[Unitäre Matrix]\index{unitäre Matrix}\newlinefirst
Eine quadratische Matrix $A$ heißt unitär, wenn $A^H A=E$ gilt.
\end{Definition}

\begin{Satz}
Ist $A$ unitär, dann gilt $|Av|=|v|$ für jeden Vektor $v$.
\end{Satz}
\begin{Beweis}
Laut Satz \ref{Verschiebungseigenschaft-komplex} gilt
\[|Av|^2 = \langle Av,Av\rangle = \langle v,A^H Av\rangle
= \langle v,Ev\rangle = \langle v,v\rangle = |v|^2.\]
Radizieren ergibt $|Av|=|v|$.\;\qedsymbol
\end{Beweis}

\begin{Satz}
Für jeden Eigenwert $\lambda$ einer unitären Matrix gilt $|\lambda|=1$.
\end{Satz}
\begin{Beweis}
Sei $v$ ein Eigenvektor zum Eigenwert $\lambda$. Laut Satz
\ref{Verschiebungseigenschaft-komplex} ist dann
\[|v|^2 = \langle v,v\rangle = \langle v,Ev\rangle
= \langle v,A^H Av\rangle = \langle Av,Av\rangle
= |Av|^2 = |\lambda v|^2
= |\lambda|^2 |v|^2.\]
Daher ist $|\lambda|^2=1$, und wegen $|\lambda|\ge 0$ folglich $|\lambda|=1$.\;\qedsymbol
\end{Beweis}



\newpage
\subsection{Quadratische Matrizen}
\begin{Satz}
Sei
\[I:=\begin{bmatrix}
0 & -1\\
1 & 0
\end{bmatrix},\quad
aE+bI = a\begin{bmatrix}1 & 0\\ 0 & 1\end{bmatrix}
+b\begin{bmatrix}0 & -1\\ 1 & 0\end{bmatrix}
=
\begin{bmatrix}
a & -b\\
b & a
\end{bmatrix}.\]
Die Menge $M := \{aE+bI\mid a,b\in\R\}$ bildet bezüglich
Matrizenaddition und Matrizenmultiplikation einen
Körper $(M,+,\cdot)$. Die Abbildung
\[\Phi\colon\C\to M,\quad \Phi(a+b\ui):= aE+bI\]
ist ein Isomorphismus zwischen Körpern.
\end{Satz}
\begin{Beweis}
Bei $(M,+)$ handelt es sich um eine Untergruppe der
kommutativen Gruppe $(\R^{2\times 2},+)$, denn gemäß
\begin{equation}\label{eq:matrix-repr-complex-add}
(aE+bI)+(cE+dI) = (a+b)E+(b+d)I \in M
\end{equation}
und
\begin{equation}
-(aE+bI) = (-a)E+(-b)I\in M
\end{equation}
ist das Untergruppenkriterium erfüllt. Die Abgeschlossenheit
bezüglich Multiplikation:
\begin{equation}\label{eq:matrix-repr-complex-mul}
\begin{split}
&(aE+bI)(cE+dI) = aEcE+aEdI+bIcE+bIdI\\
&= acE+adI+bcI+bdI^2
= (ac-bd)E+(ad+bc)I \in M.
\end{split}
\end{equation}
Das Kommutativgesetz:
\begin{equation}
\begin{split}
&(aE+bI)(cE+dI) = (ac-bd)E+(ad+bc)I\\
&= (ca-db)E+(cb+da)I = (cE+dI)(aE+bI).
\end{split}
\end{equation}
Das Assoziativgesetz ist für Matrizen allgemeingültig. Das multiplikativ
neutrale Element ist die Einheitsmatrix $E$. Wird nun $aE+bI\ne 0$
vorausgesetzt, dann ist $a\ne 0\lor b\ne 0$. Daher ist
$\det(aE+bI)=a^2+b^2\ne 0$. Demnach besitzt $aE+bI$ eine Inverse.
Somit muss $(M,+,\cdot)$ ein Körper sein.

Die Abbildung $\Phi$ ist invertierbar, denn jedes Bild $A$ kann
auf eindeutige Art in $A=aE+bI$ zerlegt werden, wodurch $a,b$
eindeutig bestimmt sind. Die Eigenschaften
\begin{equation}
\Phi((a+b\ui)+(c+d\ui)) = \Phi(a+b\ui)+\Phi(c+d\ui)
\end{equation}
und
\begin{equation}
\Phi((a+b\ui)(c+d\ui)) = \Phi(a+b\ui)\Phi(c+d\ui)
\end{equation}
ergeben sich aus den Rechnungen \eqref{eq:matrix-repr-complex-add}
und \eqref{eq:matrix-repr-complex-mul}.\;\qedsymbol
\end{Beweis}

\newpage
\section{Lineare Abbildungen}

\begin{Definition}\label{def:linear-independence}
Sei $(\mathbf a_1,\ldots, \mathbf a_n)$ ein System von Vektoren.
Man nennt es linear unabhängig, wenn
\[\textstyle\sum_{k=1}^n \lambda_k\bv a_k = \bv 0
\implies \forall k\colon\lambda_k = 0.\]
\end{Definition}

\begin{Satz}
Sei $K$ einer Körper und $A\in K^{m\times n}$ eine Matrix. Es besitzt
$A$ genau dann den trivialen Kern, wenn ihre Spalten linear
unabhängig sind.
\end{Satz}
\begin{Beweis} Die Spalten seien die $\mathbf a_k := A\mathbf e_k$.
Für die Aussage, trivialen Kern zu haben, findet sich die
äquivalente Umformung
\[\operatorname{Kern}(A) = \{\mathbf v\mid A\bv v = \bv 0\} = \{\bv 0\}
\iff (A\bv v = \bv 0 \Rightarrow \bv v = \bv 0).\]
Dies stellt mit $\lambda_k = v_k$ nun lediglich eine Kurzschreibweise
für die Eigenschaft
\[\textstyle\sum_{k=1}^n \lambda_k\bv a_k = \bv 0\implies \forall k\colon\lambda_k = 0.\]
dar, die die lineare Unabhängigkeit Def. \ref{def:linear-independence}
definiert.\,\qedsymbol
\end{Beweis}

\begin{Satz}
Eine lineare Abbildung ist genau injektiv, wenn sie einen trivalen
Kern besitzt.
\end{Satz}
\begin{Beweis} Sei $L$ die lineare Abbildung. Für die Aussage,
trivialen Kern zu haben, findet sich die äquivalente Umformung
\begin{align*}
\operatorname{Kern}(L) = \{0\}
&\iff L(v) = 0\Rightarrow v = 0\\
&\iff L(a-b) = 0\Rightarrow a - b = 0 && (v = a - b)\\
&\iff L(a) - L(b) = 0\Rightarrow a - b = 0 && (\text{weil $L$ linear ist})\\
&\iff L(a) = L(b) \Rightarrow a = b.
\end{align*}
Die letzte Aussage ist die definierende Eigenschaft
der Injektivität Def. \ref{def:inj}.\,\qedsymbol
\end{Beweis}

\begin{Satz}\label{gram-det-nonzero}
Die Spaltenvektoren einer Matrix $A\in\R^{m\times n}$ sind
genau dann linear unabhängig, wenn $\det(G)\ne 0$ gilt, wobei
$G=A^T A$ die gramsche Matrix ist.
\end{Satz}
\begin{Beweis}
Existiert ein $v\ne 0$ mit $Av = 0$, folgt sofort $Gv = A^T 0 = 0$,
womit $G$ keine reguläre Matrix sein kann, was gleichbedeutend
mit $\det(G)=0$ ist.

Sei umgekehrt $\det(G)=0$. Da $G$ hiermit keine reguläre Matrix ist,
existiert ein $v\ne 0$ mit $Gv=0$, und somit $\langle Gv,v\rangle = 0$.
Laut Satz \ref{Verschiebungseigenschaft}
ist $\langle Av,Av\rangle = \langle A^T A v,v\rangle$, was zur äquivalenten
Umformung
\[Av = 0\iff |Av| = 0\iff 0 = |Av|^2 = \langle Av,Av\rangle = \langle Gv,v\rangle\]
führt. Demzufolge muss $Av = 0$ sein.\,\qedsymbol
\end{Beweis}

\begin{Satz}
Sei $K$ ein geordneter Körper. Die Spalten einer Matrix
$A\in K^{m\times n}$ sind genau dann linear unabhängig, wenn
$\det(G)\ne 0$ gilt, wobei $G=A^T A$ die gramsche Matrix ist.
\end{Satz}
\begin{Beweis}
Man betrachte zu $v,w\in K^n$ die Bilinearform $B(v,w):=v^T w$.
Laut Satz \ref{of-sum-of-sq-positive} gilt $B(v,v)>0$ für
$v\ne 0$. Analog zu Satz \ref{Verschiebungseigenschaft} gilt
$B(v,Aw) = B(A^T v,w)$, weshalb $B(Av,Av) = B(A^T A v,v)$  gilt.
Mit den genannten Betrachtungen ist die zum Beweis von Satz
\ref{gram-det-nonzero} analoge Argumentation
durchführbar.\,\qedsymbol
\end{Beweis}

\begin{Satz}
Die Spaltenvektoren einer Matrix $A\in\C^{m\times n}$ sind
genau dann linear unabhängig, wenn $\det(G)\ne 0$ gilt, wobei
$G=A^H A$ die gramsche Matrix ist.
\end{Satz}
\begin{Beweis}
Die Argumentation verläuft analog zum Beweis von Satz
\ref{gram-det-nonzero}, wobei Satz
\ref{Verschiebungseigenschaft-komplex} anstelle von
Satz \ref{Verschiebungseigenschaft} zur Anwendung kommt.\,\qedsymbol
\end{Beweis}

\begin{Satz}
Zu einer linearen Abbildung $f\colon V\to W$ ist die Bildmenge $f(V)$
ein Untervektorraum von $W$.
\end{Satz}
\begin{Beweis}
Wir ziehen das Untervektorraumkriterium heran. Es ist $f(V)$ nichtleer,
weil der Definitionsbereich $V$ nichtleer ist, da dieser mindestens
den Nullvektor enthalten muss. Es gelte $w\in f(V)$ und $w'\in f(V)$.
Dann existieren Zeugen $v$ mit $w=f(v)$ und $v'$ mit $w'=f(v')$.
Infolge gilt $w+w' = f(v)+f(v') = f(v+v')$, womit $v+v'$ ein
Zeuge für $w+w'\in f(V)$ ist. Gleichermaßen gilt $\lambda w = \lambda f(v)
= f(\lambda v)$, womit $\lambda v$ ein Zeuge für $\lambda w\in f(V)$
ist. Die Forderungen des Kriteriums sind also erfüllt.\,\qedsymbol
\end{Beweis}

\begin{Satz}
Zu einer linearen Abbildung $f\colon V\to W$ ist Graph von $f$
ein Untervektorraum von $V\times W$.
\end{Satz}
\begin{Beweis}
Der Graph von $f$ ist die Menge
\[G = \{(v,w)\in V\times W\mid w=f(v)\}.\]
Der Raum $V\times W$ hat die Addition $(v,w)+(v',w'):=(v+v',w+w')$ und
die Skalarmultiplikation $\lambda (v,w) := (\lambda v,\lambda w)$.

Wir ziehen das Untervektorraumkriterium heran. Der Graph $G$ ist
nichtleer, weil der Definitionsbereich $V$ nichtleer ist, da dieser
mindestens den Nullvektor enthalten muss. Zu zeigen ist nun
$(v+v',w+w')\in G$, sofern $(v,w)\in G$ und $(v',w')\in G$. Aufgrund
der Prämissen ist $w=f(v)$ und $w'=f(v')$, womit
\[w+w' = f(v)+f(v') = f(v+v'), \iff (w+w',v+v')\in G.\]
Zu zeigen ist schließlich $(\lambda v,\lambda w)\in G$, sofern
$(v,w)\in G$. Aufgrund der Prämisse ist $w=f(v)$, womit
\[\lambda w = \lambda f(v) = f(\lambda v), \iff (\lambda w,\lambda v)\in G.\qedsymbol\]
\end{Beweis}

\newpage
\section{Bilinearformen}

\begin{Definition}[Nicht-ausgeartete Bilinearform]%
\label{def:degenerate}\newlinefirst
Sei $B\colon V\times W\to K$ eine Bilinearform, sei
\begin{align*}
B_1\colon V\to W^*,\quad B_1(v)(w):=B(v,w),\\
B_2\colon W\to V^*,\quad B_2(w)(v):=B(v,w).
\end{align*}
Man nennt $B$ nicht"=ausgeartet, wenn $B_1$ und $B_2$ injektiv sind.
\end{Definition}

\begin{Satz}\label{degenerate-char}
Eine Bilinearform $B\colon V\times W\to K$ ist genau dann
nicht"=ausgeartet, wenn $B_1(v)$ für alle $v\ne 0$ und $B_2(w)$ für
alle $w\ne 0$ nicht die Nullabbildung ist. Die Abbildungen $B_1,B_2$
aus Def. \ref{def:degenerate}.
\end{Satz}
\begin{Beweis}
Die lineare Abbildung $B_1$ ist genau dann injektiv, wenn
\begin{equation}
\{0\} = \operatorname{Kern}(B_1) := \{v\mid B_1(v)=0\}
\end{equation}
ist. Wegen $B_1(0)=0$ ist $B_1$ schon dann injektiv, wenn
\begin{equation}
B_1(v)=0\implies v=0,
\end{equation}
was per Kontraposition äquivalent ist zu $v\ne 0\implies B_1(v)\ne 0$.
Für $B_2$ gilt eine analoge Argumentation.\;\qedsymbol
\end{Beweis}
\begin{Satz}\label{sym-degenerate-char}
Eine symmetrische Bilinearform $B\colon V\times V\to K$ ist genau
dann nicht"=ausgeartet, wenn es für alle $v\ne 0$ ein $w$ gibt, so
dass $B(v,w)\ne 0$.
\end{Satz}
\begin{Beweis}
Da $B$ symmetrisch ist, ist $B_1=B_2$ in Def. \ref{def:degenerate}.
Es genügt also, $B_1$ zu
betrachten. Nun gilt%
\begin{equation}
B_1(v)=0 \iff (\forall w\colon B_1(v)(w)=0(w)) \iff (\forall w\colon B(v,w)=0).
\end{equation}
Aus Satz \ref{degenerate-char} ergibt sich dann die Behauptung,
d.\,h. die Äquivalenz zu
\begin{equation}
v\ne 0\implies \exists w\colon B(v,w)\ne 0.\;\qedsymbol
\end{equation}
\end{Beweis}
\begin{Satz}
Ein reelles Skalarprodukt $\langle v,w\rangle$ ist nicht"=ausgeartet.
\end{Satz}
\begin{Beweis}
In Satz \ref{sym-degenerate-char} setze
$B(v,w):=\langle v,w\rangle$. Wegen
\begin{equation}
\langle v,v\rangle = 0 \iff v=0
\end{equation}
kann man für $v\ne 0$ immer $w:=v$ setzen, dann ist
$B(v,w)=\langle v,v\rangle\ne 0$.\;\qedsymbol
\end{Beweis}

\begin{Satz}
Sind $V,W$ endlichdimensional, dann sind bei einer nicht"=ausgearteten
Bilinearform $B\colon V\times W\to K$ die Abbildungen $B_1,B_2$ aus
Def. \ref{def:degenerate} Isomorphismen.
\end{Satz}
\begin{Beweis}
Es gilt $\dim B_1(V)\le\dim W^*$ und $\dim B_2(W)\le\dim V^*$. Gemäß
Rangsatz erhält man $\dim V=\dim B_1(V)$ und $\dim W=\dim B_2(W)$,
da $B_1,B_2$ nach Voraussetzung injektiv sind. Demnach ist
\begin{equation}
\dim V \le \dim W^* = \dim W \le \dim V^* = \dim V.
\end{equation}
Folglich muss $\dim V=\dim W=\dim V^*=\dim W^*$ sein. Somit haben
$B_1,B_2$ vollen Rang, sind also surjektiv.\;\qedsymbol
\end{Beweis}

\newpage
\section{Euklidische Geometrie}

\begin{Satz}[Satz des Thales]\newlinefirst
Gegeben seien zwei Punkte $A,B$, deren Strecke ein Durchmesser
des Kreises ist. Sei $C$ ein beliebiger weiterer Punkt auf dem
Kreis. Dann ist das Dreieck $\triangle ABC$ rechtwinklig.
\end{Satz}
\begin{Beweis}
Wählt man den Mittelpunkt des Kreises als Ursprung aus, wird die
Ebene zu einem euklidischen Vektorraum. Jeder Punkt kann nun mit
seinem Ortsvektor identifiziert werden, setze $\mathbf a := A$,
$\mathbf b := B$, $\mathbf c := C$. Zu zeigen ist, dass
$\mathbf v:=\mathbf c-\mathbf a$ rechtwinklig auf
$\mathbf w:=\mathbf c-\mathbf b$  steht. Das ist genau dann der Fall,
wenn $\langle\mathbf v,\mathbf w\rangle = 0$ ist.
Man beachte $\mathbf b = -\mathbf a$. Aufgrund der Bilinearität und
Symmetrie des Skalarproduktes ergibt sich%
\begin{align}
\langle\mathbf v,\mathbf w\rangle
&= \langle\mathbf c-\mathbf a,\mathbf c+\mathbf a\rangle
= \langle\mathbf c,\mathbf c\rangle
+ \langle\mathbf c,\mathbf a\rangle
- \langle\mathbf c,\mathbf a\rangle
- \langle\mathbf a,\mathbf a\rangle\\
&= |\mathbf c|^2-|\mathbf a|^2 = 0.
\end{align}
Die letzte Gleichung gilt wegen $|\mathbf a|=|\mathbf c|$.\;\qedsymbol
\end{Beweis}

\begin{Satz}[Kosinussatz]\newlinefirst
Gegeben ist ein Dreieck $\triangle ABC$. Sei $\gamma$ der
Winkel $\angle ACB$. Dann gilt
\[c^2=a^2+b^2-2ab\cos\gamma.\]
\end{Satz}
\begin{Beweis}
Sei $\mathbf a := \overrightarrow{CB}$,
$\mathbf b := \overrightarrow{CA}$,
und $\mathbf c := \mathbf a-\mathbf b$.
Dann gilt $a=|\mathbf a|$, $b=|\mathbf b|$ und $c=|\mathbf c|$.
Die Rechenregeln des Skalarproduktes gestatten nun die
folgende Rechnung:
\begin{align}
c^2 &= |\mathbf a-\mathbf b|^2
= \langle\mathbf a-\mathbf b,\mathbf a-\mathbf b\rangle
= \langle\mathbf a,\mathbf a\rangle
+ \langle\mathbf b,\mathbf b\rangle
- 2\langle\mathbf a,\mathbf b\rangle\\
&= a^2+b^2-2ab\cos\gamma.\;\qedsymbol
\end{align}
\end{Beweis}

\begin{figure}[b!]
\centering
\includegraphics{img/Thales.pdf}
\caption{Zeichnung zum Satz des Thales}
\end{figure}

\begin{figure}[b!]
\centering
\includegraphics{img/Kosinussatz.pdf}
\caption{Zeichnung zum Kosinussatz}
\end{figure}

\newpage
\begin{Satz}[Sinussatz]
Für jedes Dreieck $\triangle ABC$ gilt
\[\frac{a}{\sin\alpha} = \frac{b}{\sin\beta} = \frac{c}{\sin\gamma}
= \frac{abc}{2A},\]
wobei $A$ der Flächeninhalt ist.
\end{Satz}
\begin{Beweis}
Sei $\mathbf a := \overrightarrow{CB}$, $\mathbf b:= \overrightarrow{CA}$
und $\mathbf c := \mathbf a-\mathbf b$. Dann gilt
\begin{align*}
ab\sin\gamma\;\mathbf e_1\wedge\mathbf e_2 &= \mathbf b\wedge\mathbf a,\\
bc\sin\alpha\;\mathbf e_1\wedge\mathbf e_2 &= \mathbf c\wedge(-\mathbf b)
= \mathbf b\wedge\mathbf c = \mathbf b\wedge (\mathbf a-\mathbf b)
= \mathbf b\wedge\mathbf a,\\
ac\sin\beta\;\mathbf e_1\wedge\mathbf e_2 &= (-\mathbf a)\wedge (-\mathbf c)
= \mathbf a\wedge\mathbf c
= \mathbf a\wedge(\mathbf a-\mathbf b)
= -\mathbf a\wedge\mathbf b = \mathbf b\wedge\mathbf a
\end{align*}
und $\mathbf b\wedge\mathbf a = 2A\,\mathbf e_1\wedge\mathbf e_2$.
Demnach gilt
\[ab\sin\gamma = bc\sin\alpha = ac\sin\beta = 2A.\]
Umformung der Gleichung führt zur Behauptung.\;\qedsymbol
\end{Beweis}

\begin{Lemma}
Sei durch $\bv p(t):=\bv a+t\bv v$ mit $\bv a,\bv v\in\R^n$
eine Parametergerade gegeben und sei $\bv b\in\R^n$ nicht auf der Gerade.
Wir betrachten die Abstandsvektoren $\bv d(t) = \bv b - \bv p(t)$. Man
erhält den kürzesten Abstand, wenn $\bv p(t)$ der Lotfußpunkt ist.
\end{Lemma}
\begin{Beweis}
Man ermittelt die Ableitung
\[d'(t) = \tfrac{\partial}{\partial t} |\bv d(t)|
= \frac{1}{2|\bv d(t)|} 2\langle\bv d(t),\bv d'(t)\rangle
= -\frac{\langle\bv d(t),\bv v\rangle}{|\bv d(t)|}.\]
Aus dem notwendigen Kriterium $d'(t)=0$ ergibt sich
$\langle\bv d,\bv v\rangle = 0$, womit $\bv d$ rechtwinklig auf
$\bv v$ stehen muss. Es verbleibt zu zeigen, dass es sich um ein
Minimum handelt. Wir bestimmen dazu die zweite Ableitung.
Mit $d'(t)d(t) = -\langle\bv d(t),\bv v\rangle$ findet sich
\[\tfrac{\partial}{\partial t}(d'(t)d(t)) = d''(t)d(t) + d'(t)^2
= -\tfrac{\partial}{\partial t}\langle\bv d(t),\bv v\rangle
= -\langle\bv d'(t),\bv v\rangle = \langle\bv v,\bv v\rangle.\]
Man erhält an der kritischen Stelle also
\[d''(t) = \frac{|\bv v|^2}{d(t)} > 0,\]
womit dort ein Minimum befindlich sein muss.\,\qedsymbol
\end{Beweis}

\begin{Satz}
Sei durch $\bv p(t):=\bv a+ t\bv v$ mit $\bv a, \bv v\in\R^n$ eine
Parametergerade gegeben und sei $\bv b\in\R^n$ ein weiterer Punkt.
Der Abstand von $\bv b$ zur Gerade beträgt
\[d = \bigg|\bv b-\bv a - \frac{\langle\bv b-\bv a,\bv v\rangle}{|\bv v|^2}\bv v\bigg|.\]
\end{Satz}
\begin{Beweis}[Beweis 1]
Es ist der Parameter $t$ gesucht, bei dem der Abstandsvektor
$\bv d := \bv b-\bv p(t)$ rechtwinklig zum Richtungsvektor $\bv v$ steht.
Allgemein stehen zwei Vektoren genau dann rechtwinklig, wenn ihr
Skalarprodukt verschwindet. Es muss also gelten
\[0 = \langle\bv d,\bv v\rangle
= \langle\bv b - \bv a-t\bv v, \bv v\rangle
= \langle\bv b - \bv a, \bv v\rangle - t\langle\bv v,\bv v\rangle,
\iff t = \frac{\langle\bv b-\bv a,\bv v\rangle}{|\bv v|^2}.\]
Es findet sich der Abstand
\[d = |\bv d| = |\bv b-\bv p|
= \Big|\bv b - \bv a - \tfrac{\langle\bv b-\bv a,\bv v\rangle}
{|\bv v|^2}\bv v\Big|.\,\qedsymbol\]
\end{Beweis}
\begin{Beweis}[Beweis 2]
Wir verschieben alle Ortsvektoren um $-\bv a$, damit die Gerade
durch den Ursprung verläuft. Aus dem verschobenen Punkt
$\bv b' = \bv b-\bv a$ lässt sich nun der Lotfußpunkt vermittels der
orthogonalen Projektion $P_{\bv v}(\bv b')$ erhalten. Der gesuchte
Abstand ist die zwischen $\bv b'$ und dem Lotfußpunkt befindliche
Distanz. Man erhält
\[d = |\bv b' - P_{\bv v}(\bv b')|
= \Big|\bv b-\bv a - \tfrac{\langle\bv b-\bv a,\bv v\rangle}
{\langle\bv v,\bv v\rangle}\bv v\Big|.\,\qedsymbol\]
\end{Beweis}

\begin{Satz}
Seien $\bv a,\bv v\in\R^n$ fest, $n\ge 1$ und $\bv v\ne\bv 0$.
Die beiden Mengen
\begin{gather*}
\{\bv p\mid \exists t\colon\bv p = \bv a+t\bv v\},\quad
\{\bv p\mid (\bv p-\bv a)\wedge\bv v = 0\}
\end{gather*}
beschreiben dieselbe Gerade.
\end{Satz}
\begin{Beweis}
Zu zeigen ist die Äquivalenz
\[(\exists t\colon\bv p-\bv a = t\bv v)\iff (\bv p-\bv a)\wedge\bv v = 0.\]
Die linke Seite verlangt, dass $\bv p-\bv a$ kollinear zu $\bv v$ sein
soll. Das äußere Produkt zweier Vektoren verschwindet im Allgemeinen
genau dann, wenn diese kollinear sind. Mithin ist die rechte Seite
äquivalent zur linken.\,\qedsymbol
\end{Beweis}

\begin{Satz}
Sei $\bv a\in\R^n$, genannt Basispunkt. Sei $\bv v_1,\dots,\bv v_m\in\R^n$
mit $m\le n$ linear unabhängig, genannt Richtungsvektoren. Die beiden Mengen
\[\{\bv p\mid\exists\lambda_1,\ldots, \lambda_m\colon \bv p
= \bv a+\sum_{k=1}^m \lambda_k\bv v_k\},\quad
\{\bv p\mid (\bv p-\bv a)\wedge\bv v_1\wedge\ldots\wedge\bv v_m = 0\}\]
beschreiben denselben affinen Unterraum.
\end{Satz}
\begin{Beweis}
Die linke Seite verlangt, dass sich $\bv p-\bv a$ als Linearkombination
der $\bv v_k$ darstellen lässt. Damit gleichbedeutend ist, dass das
System aus $\bv p-\bv a$ und den $\bv v_k$ linear abhängig ist.
Ein System von Vektoren ist genau dann linear abhängig, wenn das
äußere Produkt der Vektoren verschwindet.\,\qedsymbol
\end{Beweis}

\begin{Satz}[Lagrange-Identität]\newlinefirst
Für zwei Vektoren $\bv a,\bv b\in\R^n$ gilt
\[|\bv a\wedge\bv b|^2 = \sum_{i,j:i<j}(a_i b_j - a_j b_i)^2 
= |\bv a|^2|\bv b|^2 - \langle\bv a,\bv b\rangle^2.\]
\end{Satz}
\begin{Beweis}
Es findet sich die Umformung
\begin{gather*}
\sum_{i,j:i<j}(a_i b_j - a_j b_i)^2 = \frac{1}{2}\sum_{i,j}(a_i b_j - a_j b_i)^2
= \frac{1}{2}\sum_{i,j}(a_i^2 b_j^2 - 2a_i b_i a_j b_j + a_j^2 b_i^2)\\
= \frac{1}{2}\bigg(2\sum_{i,j} a_i^2 b_j^2 - 2\sum_{i,j}a_i b_i a_j b_j\bigg)
= \bigg(\sum_{i=1}^n a_i^2\bigg)\bigg(\sum_{j=1}^n b_j^2\bigg)
- \bigg(\sum_{i=1}^n a_i b_i\bigg)\bigg(\sum_{j=1}^n a_j b_j\bigg)\\
= |\bv a|^2 |\bv b|^2 - \langle\bv a,\bv b\rangle^2.\,\qedsymbol
\end{gather*}
\end{Beweis}

