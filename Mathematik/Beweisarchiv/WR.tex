
\chapter{Wahrscheinlichkeitsrechnung}

\section{Diskrete Wahrscheinlichkeitsräume}

\begin{Definition}[Diskreter Wahrscheinlichkeitsraum]\mbox{}\\*
Sei $\Omega$ eine höchstens abzählbare Menge. Das
Paar $(\Omega,P)$ nennt man diskreten Wahrscheinlichkeitsraum,
wenn
\[P\colon 2^\Omega\to [0,1],\quad P(A):=\sum_{\omega\in A} P(\{\omega\})\]
die Eigenschaft $\sum_{\omega\in\Omega} P(\{\omega\}) = 1$ besitzt.
\end{Definition}
Bemerkung: Man schreibt auch $P(\omega):=P(\{\omega\})$.

\begin{Definition}[Reelle Zufallsgröße]\mbox{}\\*
Sei $(\Omega,P)$ ein diskreter Wahrscheinlichkeitsraum.
Eine Funktion $X\colon\Omega\to\R$ nennt man Zufallsgröße.
Die Verteilung von $X$ ist definiert gemäß $P_X(A):=P(X^{-1}(A))$.
\end{Definition}

\begin{Definition}[Erwartungswert]%
\label{def:expected-value}\mbox{}\\*
Sei $(\omega_k)$ eine beliebige Abzählung von $\Omega$.
Ist die Reihe $\sum_{k=0}^{|\Omega|} X(\omega_k)P(\{\omega_k\})$
absolut konvergent, dann nennt man
\[E(X) := \sum_{\omega\in\Omega} X(\omega)P(\{\omega\})\]
den Erwartungswert von $X$.
\end{Definition}

\begin{Satz} Es gilt
\[E(X) = \sum_{x\in X(\Omega)} xP(X^{-1}(x))
= \sum_{x\in X(\Omega)} xP(X=x).\]
\end{Satz}
\strong{Beweis.} Zunächst gilt
\begin{gather*}
\sum_{\substack{\omega\in\Omega\\ X(\omega)=x}}P(\omega)
= P(\bigcup_{\substack{\omega\in\Omega\\ X(\omega)=x}} \{\omega\})
= P(\{\omega\in\Omega\mid X(\omega)=x\})
= P(X^{-1}(x)).
\end{gather*}
Da die Reihe zu $E(X)$ nach Def. \ref{def:expected-value}
absolut konvergent ist, darf sie beliebig umgeordnet werden und
man bekommt
\begin{gather*}
E(X) = \sum_{\omega\in\Omega}X(\omega)P(\omega)
= \sum_{x\in X(\Omega)}\sum_{\substack{\omega\in\Omega\\ X(\omega)=x}} xP(\omega)
= \sum_{x\in X(\Omega)} x\sum_{\substack{\omega\in\Omega\\ X(\omega)=x}}P(\omega)\\
= \sum_{x\in X(\Omega)} xP(X^{-1}(x)).\;\qedsymbol
\end{gather*}

\newpage
\begin{Korollar} Der Erwartungswertoperator ist ein lineares Funktional,
das heißt, es gilt $E(aX)=aE(X)$ und $E(X+Y)=E(X)+E(Y)$. 
\end{Korollar}
\strong{Beweis.} Aufgrund der Konvergenz der Reihen gilt
\[E(aX) = \sum_{\omega\in\Omega}aX(\omega)P(\omega)
= a\sum_{\omega\in\Omega}X(\omega)P(\omega) = aE(X)\]
und
\begin{align*}
E(X+Y) &= \sum_{\omega\in\Omega} (X(\omega)+Y(\omega))P(\omega)
= \sum_{\omega\in\Omega} (X(\omega)P(\omega)+Y(\omega)P(\omega))\\
&= \sum_{\omega\in\Omega} X(\omega)P(\omega)
+ \sum_{\omega\in\Omega} Y(\omega)P(\omega) = E(X)+E(Y).\;\qedsymbol
\end{align*}

\begin{Korollar} Ist $X\le Y$, dann ist auch $E(X)\le E(Y)$.
\end{Korollar}
\strong{Beweis.} Gemäß $P(\omega)\ge 0$ ist
\begin{gather*}
X\le Y\iff X(\omega)\le Y(\omega)\iff 0\le Y(\omega)-X(\omega)
\iff 0\le (Y(\omega)-X(\omega))P(\omega).
\end{gather*}
Somit hat man
\begin{gather*}
X\le Y\implies 0\le E(Y-X) = \sum_{\omega\in\Omega} (Y(\omega)-X(\omega))P(\omega),
\end{gather*}
und gemäß Linearität daher
\begin{gather*}
X\le Y\implies 0\le E(Y-X) = E(Y)-E(X) \iff E(X)\le E(Y).\;\qedsymbol
\end{gather*}

\begin{Definition}[Unabhängige Ereignisse]\mbox{}\\*
Zwei Ereignisse $A,B$ heißen unabhängig, falls $P(A\cap B)=P(A)P(B)$.
\end{Definition}

\begin{Definition}[Unabhängige Zufallsgrößen]\mbox{}\\*
Zwei Zufallsgrößen $X,Y\colon\Omega\to\R$ heißen unabhängig, wenn
die Ereignisse $\{X\in A\}$ und $\{X\in B\}$
für alle Mengen $A,B\subseteq\R$ unabhängig sind.
\end{Definition}

\begin{Satz}
Zwei Zufallsgrößen $X,Y\colon\Omega\to\R$ sind genau dann unabhängig,
wenn für alle $x\in X(\Omega)$ und $y\in Y(\Omega)$ gilt:
\[P(X=x,Y=y)= P(X=x)P(Y=y).\]
\end{Satz}
\strong{Beweis.} Sind $X,Y$ unabhängig, dann ist
\begin{align*}
P(X=x,Y=y) &= P(\{X\in\{x\}\}\cap\{Y\in\{y\}\})
= P(\{X\in\{x\}\})P(\{Y\in\{y\}\})\\
&= P(X=x)P(Y=y).
\end{align*}
Umgekehrt gelte nun $P(X=x,Y=y)=P(X=x)P(Y=y)$, dann ist
\begin{gather*}
P(\{X\in A\}\cap\{Y\in B\})
= P(\bigcup_{x\in A}\{X=x\}\cap\bigcup_{y\in B}\{Y=y\})\\
= P(\bigcup_{x\in A}\bigcup_{y\in B}(\{X=x\}\cap\{Y=y\}))
= \sum_{x\in A}\sum_{y\in B}P(\{X=x\}\cap\{Y=y\})\\
= \sum_{x\in A}\sum_{y\in B}P(X=x)P(Y=y)
= \sum_{x\in A}P(X=x)\sum_{y\in B}P(Y=y)\\
= P(\bigcup_{x\in A}\{X=x\})P(\bigcup_{y\in B}\{Y=y\})
= P(X\in A)P(Y\in B).\;\qedsymbol
\end{gather*}

\begin{Definition}[Bedingter Erwartungswert]%
\label{def:cond-expected-value}\mbox{}\\*
\[E(X\mid A) = \frac{E(1_A X)}{P(A)} = \frac{1}{P(A)}\sum_{\omega\in A} X(\omega)P(\{\omega\}).\]
\end{Definition}

\begin{Satz} Es gilt
\[E(X\mid A) = \frac{1}{P(A)}\sum_x xP(\{X=x\}\cap A)
= \sum_x xP(X=x\mid A),\]
wobei sich die Summe über alle $x\in X(\Omega)$ erstreckt.
\end{Satz}
\strong{Beweis.} Man kann rechnen
\begin{align*}
E(1_A X) &= \sum_{\omega\in\Omega} 1_A(\omega) X(\omega) P(\{\omega\})
= \sum_x\sum_{\omega\in X^{-1}(x)} 1_A(\omega) X(\omega) P(\{\omega\})\\
&= \sum_x x\sum_{\omega\in X^{-1}(x)} 1_A(\omega) P(\{\omega\})
= \sum_x x\sum_{\omega\in X^{-1}(x)\cap A} P(\{\omega\})\\
&= \sum_x x P(X^{-1}(x)\cap A),
\end{align*}
wobei $X^{-1}(x) = \{X=x\}$.\;\qedsymbol

\begin{Korollar}\label{prob-as-expected-value}
Es gilt $P(A) = E(1_A)$, wobei $1_A$ die Indikatorfunktion ist.
\end{Korollar}
\strong{Beweis.} Gemäß Definition des Erwartungswertes ist
\[E(1_A) = \sum_{\omega\in\Omega} 1_A(\omega)P(\{\omega\})
= \sum_{\omega\in A}P(\{\omega\}) = P(A).\;\qedsymbol\]

\begin{Korollar}
Es gilt $P(A\mid B) = E(1_A\mid B)$, wobei $1_A$ die Indikatorfunktion ist.
\end{Korollar}
\strong{Beweis.} Gemäß Definition \ref{def:cond-expected-value}
und Korollar \ref{prob-as-expected-value} ist
\[E(1_A\mid B) = \frac{E(1_A 1_B)}{P(B)} = \frac{E(1_{A\cap B})}{P(B)}
= \frac{P(A\cap B)}{P(B)} = P(A\mid B).\qedsymbol\]

\section{Allgemeine Wahrscheinlichkeitsräume}

\begin{Satz}\label{rv-transform-pdf}
Sei $g\colon\R\to\R$ eine streng monotone Funktion. Seien
$X,Y$ Zufallsgrößen mit Dichten $f_X,f_Y$. Ist $Y=g(X)$,
dann gilt
\[f_Y(y) = \frac{f_X(g^{-1}(y))}{|g'(g^{-1}(y))|}.\]
\end{Satz}
\strong{Beweis.} Sei $g$ streng monoton steigend. Dann kann man rechnen
\[F_Y(y) = P(Y\le y) = P(g(X)\le y) = P(X\le g^{-1}(y)) = F_X(g^{-1}(y)).\]
Gemäß der Kettenregel findet man
\[f_Y(y) = \frac{\mathrm d}{\mathrm dy}F_Y(y)
= f_X(g^{-1}(y))\frac{\mathrm d}{\mathrm dy}g^{-1}(y) = \frac{f_X(g^{-1}(y))}{g'(g^{-1}(y))}.\]
Sei $g$ nun streng monoton fallend. Dann kann man rechnen
\[F_Y(y) = P(g(X)\le y) = P(X\ge g^{-1}(y)) = 1 - P(X < g^{-1}(y))
= 1 - F_X(g^{-1}(y)).\]
Entsprechend findet man
\[f_Y(y) = -\frac{f_X(g^{-1}(y))}{g'(g^{-1}(y))}.\]
Nun ist $g'$ in beiden Fällen frei von Nullstellen. Demnach ist
$\sgn(g'(x))$ konstant für alle $x$ und wir haben allgemein
\[f_Y(y) = \sgn(g'(x))\frac{f_X(x)}{g'(x)} = \frac{f_X(x)}{|g'(x)|}\]
mit $x=g^{-1}(y)$.\;\qedsymbol

\begin{Satz}[»LOTUS: Law of the unconscious statistican«]\mbox{}\\*
Ist $g\colon\R\to\R$ streng monoton, dann muss gelten
\[E(g(X)) = \int_{-\infty}^\infty g(x)f_X(x)\,\mathrm dx.\]
\end{Satz}
\strong{Beweis.} Sei $Y=g(X)$. Mit Satz \ref{rv-transform-pdf}
und Substitution $y=g(x)$ kann man rechnen
\begin{align*}
E(g(X)) &= E(Y) = \int_{-\infty}^\infty yf_Y(y)\,\mathrm dy
= \int_{-\infty}^\infty y\frac{f_X(g^{-1}(y))}{|g'(g^{-1}(y))|}\,\mathrm dy\\
&= \int_{g^{-1}(-\infty)}^{g^{-1}(\infty)} g(x)
\frac{f_X(x)}{|g'(x)|}g'(x)\,\mathrm dx\\
&= \sgn(g')\int_{-\sgn(g')\infty}^{\sgn(g')\infty}g(x) f_X(x)\mathrm dx
= \int_{-\infty}^\infty g(x) f_X(x)\mathrm dx.\;\qedsymbol
\end{align*}

