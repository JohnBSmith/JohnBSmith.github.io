<!DOCTYPE html>
<html lang="de">
<head>
  <meta charset="UTF-8">
  <title>Blog</title>
  <link href="blog.css" rel="stylesheet">
</head>
<body>

<div id="content">
<header id="header">
<h1>Blog</h1>
<a href="../de.htm">Home</a><!--
--><a href="../Rest/Portal.htm">Portal</a>
</header>

<div id="main-aside">
<main id="main">

<h2>Inhaltsverzeichnis</h2>
<ul>
<li><span class="date">[2015-12-12]</span>
  <a href="#Passwoerter">Kryptische Passwörter</a>
<li><span class="date">[2017-02-22]</span>
  <a href="#Index">Index clientseitig durchsuchen</a>
<li><span class="date">[2017-03-19]</span>
  <a href="#Midi">Probleme mit Midi auf Linux</a>
<li><span class="date">[2017-05-09]</span>
  <a href="#Int">Ganzzahldatentypen</a>
<li><span class="date">[2017-10-26]</span>
  <a href="#Rust">Gedanken zu Rust</a>
<li><span class="date">[2017-10-31]</span>
  <a href="#Rechenaufwand">Rechenaufwand</a>
<li><span class="date">[2017-12-23]</span>
  <a href="#SVG-und-Fonts">SVG und Fonts</a>
<li><span class="date">[2018-01-21]</span>
  <a href="#Drachen-und-Zombies">Drachen und Zombies</a>
<li><span class="date">[2018-03-13]</span>
  <a href="#GCHQ-TLS">GCHQ murrt gegen TLS-1.3</a>
<li><span class="date">[2018-03-23]</span>
  <a href="#Strukturelle-Typisierung">Strukturelle Typisierung in Rust</a>
<li><span class="date">[2018-04-21]</span>
  <a href="#Wikipedia">Wikipedia ist überholt</a>
<li><span class="date">[2018-08-06]</span>
  <a href="#Backpulver">Backpulver</a>
<li><span class="date">[2018-10-15]</span>
  <a href="#Spulenfiepen">Spulenfiepen</a>
<li><span class="date">[2018-12-09]</span>
  <a href="#Mykro">Mykro</a>
<li><span class="date">[2018-12-12]</span>
  <a href="#Absurder-Speicherverbrauch">Absurder Speicherverbrauch</a>
<li><span class="date">[2019-04-07]</span>
  <a href="#Allgemeinheitsthese">Allgemeinheitsthese</a>
</ul>

<h2 id="Passwoerter">Kryptische Passwörter</h2>
<div class="date">12. Dezember 2015</div>

<p>Ich las gerade einen Zeitungsartikel über Passwörter. Dort wurden
einem wie üblich Tipps genannt, um sich solche sicher zu erstellen.
Die Darstellung war simplifiziert und hob unwesentliche Dinge hervor.
Es wurde dann empfohlen, mindestens 12 Zeichen mit
Zufallskombinationen zu verwenden. Aber was soll das
<em>mindestens</em>? Warum nicht einfach
eine exakte Zahl von Zeichen? Viele Dienste wollen dann auch noch
unbedingt Sonderzeichen und große Buchstaben, andernfalls wird
das Passwort abgewiesen. Das ist natürlich ein ziemlicher Bullshit.
Ich will erklären warum.

<p>Gehen wir zunächst einmal von dem aus, was sicher ist. Man kann
durch theoretische Überlegungen leicht feststellen, dass ein
128bit-Passwort sicher ist. Falls Quantencomputer verwendet werden,
so soll man nach Meinung von Forschern die Passwortlänge verdoppeln,
also ein 256bit-Passwort verwenden.

<p>Nun kann man so ein Passwort zu einer bestimmten Basis
darstellen, wenn man es als Zahl im Stellenwertsystem auffasst.
Die Basis <code>B</code> ist dabei eine geordnete Menge von Ziffern.
Die Länge der Basis, d.&nbsp;h. die Anzahl der Ziffern, bezeichnen wir
mit <code>b</code>. Im Dezimalsystem hat man folgende Basis:

<pre>
  B = {0,1,2,3,4,5,6,7,8,9}
  b = |B| = 10
</pre>


<p>Eigentlich ist ein Passwort keine Zahl, sondern nur
ein Symbolwort. Man nennt die Basis dann
Symbolalphabet. Aber das sind abstraktere
Begriffe, die nur für Informatiker verständlich sind. Die
nötige Passwortlänge wollen wir nun mit <code>L</code> bezeichnen,
präziser mit <code>L(b)</code>, da es sich um eine von <code>b</code>
abhängige Funktion handelt.

<p>Für eine einzelne Ziffer gibt es nun <code>b</code> Möglichkeiten,
für zwei sind es <code>b*b</code> und für <code>L</code> Ziffern sind
es <code>b<sup>L</sup></code>. Die Zahl
<pre>
  S = b<sup>L</sup>
</pre>
<p>bezeichnet man als Schlüsselraumgröße.
Ein bit hat zwei Möglichkeiten. Dann hat ein
128bit Passwort also <code>2<sup>128</sup></code> Möglichkeiten.
Damit erhält man die Gleichung
<pre>
  b<sup>L</sup> = 2<sup>128</sup>.
</pre>
<p>Logarithmieren auf beiden Seiten und Umformung bringt
<pre>
  L = 128 lg(2)/lg(b).
</pre>
<p>Zur Sicherheit wollen wir Aufrunden. Somit erhält man
für die notwendige Passwortlänge die Formel
<pre>
  L = ceil(128 lg(2)/lg(b)).
</pre>
<p>Hier sind nun einige Basen bzw. Alphabete wie sie praktisch
verwendet werden.
<table class="bt">
<tr><th>b<th>Basis
<tr><td>10<td>dezimale Ziffern
<tr><td>16<td>hexadezimale Ziffern
<tr><td>26<td>kleine Buchstaben
<tr><td>36<td>Buchstaben und Ziffern
<tr><td>62<td>kleine und große Buchstaben sowie Ziffern
<tr><td>90<td>druckbare ASCII-Zeichen
</table>

<p>Nun eine Tabelle mit der notwendigen Passwortlänge:
<pre>
    b |   L
    2 | 128
    3 |  81
    4 |  64
    6 |  50
   10 |  39
   16 |  32
   26 |  28
   36 |  25
   62 |  22
   90 |  20
  100 |  20
  128 |  19
  256 |  16
  999 |  13
</pre>
<p>Man sieht, dass die notwendige Passwortlänge erst schnell und
dann immer langsamer abnimmt. Nur um fünf Zeichen zu sparen, muss
man aber nicht große Buchstaben und Sonderzeichen verwenden.
Selbst wenn man 1000 Hanzi verwenden würde, wäre die Passwortlänge
immer noch 13 Zeichen.

<p>Es ist nun am sichersten, (exakt gleichverteilte) zufällige Zeichen
zu benutzen. Diese ordnet man zur Lesbarkeit in Vierergruppen
an. Bei 24 kleinen Buchstaben und Ziffern sind das sechs Gruppen.
Ein Passwort sieht dann z.&nbsp;B. so aus:

<pre>
  b4ma 2kwq wmco piht ujx8 r37g
</pre>

<p>Leerzeichen sollten vom Eingabeparser
entfernt werden. Ich finde es sehr fehleranfällig, wenn
Eingabeparser so etwas nicht machen. 

<p>Das mit den nicht notwendigen Sonderzeichen lässt sich auch
so veranschaulichen: wenn man zwei hexadezimale Ziffern zu einem
Symbol zusammenfasst, hat man schon <code>b=256</code>.
Anstelle noch so kryptische Zeichen zu verwenden, kann man also auch
einfach die Passwortlänge verdoppeln und dabei nur hexadezimale
Ziffern benutzen.

<p>Für einen Account bei einem Server sollten sich
<code>2<sup>128</sup></code> Schlüssel aber nicht Brute-Force
durchprobieren lassen. So etwas wäre ja höchst auffällig. Angenommen,
jemand loggt sich maximal 100 mal am Tag ein und der Account
besteht maximal für 100 Jahre. Dann ist die maximale Anzahl der
Versuche 100*365*100. Man will nun eine Wahrscheinlichkeit von eins
zu einer Mio., dass jemand hierbei Zufällig den richtigen
Schlüssel findet. Man benötigt daher einen Schlüsselraum von
<pre>
  100*356*100*10<sup>6</sup> = 3.65*10<sup>12</sup>
  = 1.66*2<sup>41</sup>.
</pre>

<p>Bei <code>2<sup>42</sup></code> ergibt sich ein Passwort von
neun Buchstaben und Ziffern. Zur Sicherheit kann man 12 verwenden.
Ein Passwort wie
<pre>
  b4ma 2kwq wmco
</pre>
<p>sollte also ausreichend sicher sein. Ein Problem welches
sich jetzt ergibt, ist, dass von Passwörtern nur gesalzene
Hashwerte gespeichert werden sollten. Es kann möglich sein,
dass dabei zwei unterschiedliche Passwörter zum gleichen Hashwert
führen. Bei SHA-256-Hashwerten ist so etwas aber fast
ausgeschlossen.

<p>Und damit ist nun klar, dass man weder Sonderzeichen
noch Großbuchstaben verwenden muss. Verängstigte können
die Passwortlänge einfach von 12 auf 16 erhöhen,
das bringt mehr als noch so kryptische Sonderzeichen.
Warum? Weil <code>36<sup>16</sup></code> größer ist als
<code>90<sup>12</sup></code>.

<p>Um es mathematisch zu präzisieren: Wächst die Basis, so
ergibt sich für die Schlüsselraumgröße die Potenzfunktion
<code>S(x)=x<sup>L</sup></code>. Wächst jedoch die Schlüssellänge, so
ergibt sich für die Schlüsselraumgröße die Exponentialfunktion
<code>S(x)=b<sup>x</sup></code>. Exponentialfunktionen wachsen aber
wesentlich schneller als Potenzfunktionen.


<h2 id="Index">Index clientseitig durchsuchen</h2>
<div class="date">22. Februar 2017</div>

<p>Interessanterweise lässt sich unter Verwendung von clientseitigem
Javascript auch ein Index auf einem Server durchsuchen.
Das Problem ist zunächst, dass der Index irgendwann größer wird als
ca. vier MB. Als Lösung wird der Index nun jedoch als Baumstruktur
gespeichert, wobei die Knoten der ersten Ebenen in unterschiedlichen
Dateien stehen. Über <code>XMLHttpRequest</code> lassen sich die
Dateien dann per Lazy Evaluation lesen. Angenommen man hat einen
1&nbsp;GB großen Suchbaum. Bei zwei Ebenen mit je 26 Buchstaben lässt
sich der Index dann auf 1,5&nbsp;MB große Dateien verteilen.

<p>Ein weiteres Problem, welches sich dabei ergibt, ist nun aber
die Suche von Teilzeichenketten im Index. Dazu müsste der gesamte
Index geladen und durchsucht werden. Man könnte nun auf die Idee
kommen, aus dem Index einen Meta-Index zu erzeugen. Aber die
Teilzeichenketten sind im Normalfall über den gesamten Index verteilt,
sodass man doch wieder alle Dateien laden müsste.

<p>Stattdessen ist es vorteilhaft, gleich einen Index zu erzeugen,
der auch die Teilzeichenketten umfasst. Dieser wird dann wohl
etwas größer werden. Aber das ist nicht so schlimm, auch wenn der Index
so groß werden würde wie der gesamte Text der Website. Solange beim
Auswerten des Suchbaumes nur zwei bis drei kleine Dateien geladen
werden müssen, ist das kein Nachteil. Man beachte, dass das
Text-Daten sind, das ist eigentlich sehr wenig Festplattenplatz.

<p>Ich hätte gerne eine Volltextsuche für
<a href="https://de.wikipedia.org/wiki/Project_Gutenberg"
>Project Gutenberg</a>. Die Website verwendet seit einiger Zeit
schon TLS. Man muss aber einen https-Link verwenden.

<h2 id="Midi">Probleme mit Midi auf Linux</h2>
<div class="date">19. März 2017</div>
<p>Zurzeit gibt es noch einige Probleme mit Midi auf Linux
(Ubuntu 16.04 LTS). Linthesia in Version <code>0.4.3</code>
funktionierte, stürzte aber nach einiger Zeit ab. Nach Software-Updates
scheint es jetzt in (immer noch Version <code>0.4.3</code>) nicht mehr
abzustürzen, aber man hört nichts mehr, weil TiMidity irgendwie nicht
richtig angesprochen wird. TiMidity selbst funktioniert, jedoch war es
mir nicht möglich, alternative Soundfonts zu installieren.

<p>Musescore hat auch mal gesponnen, sodass man nichts hörte.
Es scheint jetzt aber wieder korrekt zu funktionieren. Das Verhalten
kann sich nach Software-Updates ja verändern, und ich verfolge nicht
alle Software-Updates. Musescore wandelt Midi in einigen Fällen
sogar in einigermaßen korrekte Partituren um und scheint sogar
Auftakte und Tonarten erkennen zu können.

<p>Das <code>fluidsynth</code>-Plugin für den VLC funktioniert,
das ist schön. Wenn man eine Midi-Datei im Firefox anklickt, dann
öffnet sich VLC und spielt die ab. Die Zukunftsvorstellung ist dabei
jetzt natürlich, dass sich gleich Linthesia öffnen könnte.
Jedenfalls braucht jemand in Zukunft nicht mehr unbedingt Noten
schreiben. Es geht wesentlich schneller, mit einem Keyboard eine
Midi-Datei aufzunehmen und die dann mit Linthesia abspielen zu lassen.

<h2 id="Int">Ganzzahldatentypen</h2>
<div class="date">9. Mai 2017</div>
<p>Wart ihr euch schon einmal unsicher, welchen der vielen
Ganzzahldatentypen ihr in C benutzen sollt? Meines Erachtens
können die eingebauten Datentypen von einem Compiler sinnvollerweise
so definiert werden:
<pre>
  unsigned char  == uint_fast8_t
  unsigned short == uint_fast16_t
  unsigned int   == uint_fast16_t
  unsigned long  == uint_fast32_t
</pre>
<p>Prinzipiell könnte ein Compiler auch so konfiguriert werden,
dass er möglichst viel Speicher spart (meistens nicht sinnvoll).
Zum Beispiel könnte man auch auf solches stoßen:
<pre>
  unsigned char  == uint_least8_t
  unsigned short == uint_least16_t
  unsigned int   == uint_fast16_t
  unsigned long  == uint_least32_t
</pre>
<p>Angenommen man will nun eine UTF-8- oder eine UTF-32-Zeichenkette
speichern. Man könnte jetzt auf die Idee kommen, dafür ein Feld von
<code>uint8_t</code> bzw. <code>uint32_t</code> zu verwenden.
Es kann aber auf seltenen Architekturen der Fall sein, dass das nicht
sinnvoll oder nicht möglich ist. Vielmehr möchte man für diesen Zweck
eigentlich <code>uint_least8_t</code> bzw. <code>uint_least32_t</code>
verwenden.

<p>Die Dinge fangen nun an kompliziert zu werden, wenn man ein
<code>uint32_t</code> als Feld von vier <code>uint8_t</code>
betrachten möchte:
<pre>
  uint32_t x = 0xcafebabe;
  uint8_t* p = (uint8_t*)&amp;x;
  printf("%02x %02x %02x %02x\n",p[3],p[2],p[1],p[0]);
</pre>
<p>Möchte man Daten aus einer Datei lesen oder in eine Datei
schreiben, so kommt dabei aber <code>uint8_t*</code> nicht vor.
Vielmehr stößt man auf <code>char*</code>, welcher zunächst in
<code>unsigned char*</code> umgecastet werden sollte. Es muss
jedoch nicht unbedingt <code>sizeof uint8_t == sizeof unsigned char</code>
sein. Das ist problematisch, wenn man z.&nbsp;B. eine
<code>uint_least32_t</code>-Zahl aus einer Datei auslesen möchte.
Die Zahl ist über ein Feld von vier <code>unsigned char</code>
verteilt. Der naive Ansatz wäre folgender:
<pre>
  unsigned char* Daten;
  uint8_t* pu8 = (unsigned char*)Daten; // Aua! (1)
  unsigned long x = *(uint32_t*)(pu8+POSITION);
</pre>
<p>Nicht weniger naiv ist folgender Ansatz:
<pre>
  unsigned char* Daten;
  unsigned char* p = Daten+POSITION;
  unsigned long x;
  x = *(unsigned long*)p; // Aua! (2)
  x = *(uint32_t*)p; // Aua! (3)
</pre>
<p>Wenn z.&nbsp;B. <code>sizeof unsinged char == sizeof unsigned
long</code> ist, dann geht alles schief. Das Feld schaut dann
folgendermaßen aus:
<pre>
  B X X X B X X X B X X X B X X X ...
  ^       ^       ^       ^
  p[0]    p[1]    p[2]    p[3]
</pre>
<p>wobei die <code>B</code> die Bytes der Zahl sind und <code>X</code>
beliebiger Datenmüll ist. Schauen wir uns an, was mit <code>pu8</code>
passiert:
<pre>
  B == pu8[0]
  X == pu8[1]
  X == pu8[2]
  X == pu8[3]
  B == pu8[4]
  X == pu8[5]
  usw.
</pre>
<p>Bei (3) schaut es so aus:
<pre>
  B X X X == *(uint32_t*)(p+POSITION)
</pre>

<p>Bei (2) nehmen wir an, dass <code>unsigned long == uint64_t</code>
gilt:
<pre>
  B X X X B X X X == *(unsigned long*)(p+POSITION)
</pre>
<p>Der Ausdruck (2) ist ausgesprochen schlimm, denn selbst
wenn <code>unsigned char</code> einem Byte entspräche, wäre
<pre>
  B B B B X X X X == *(unsigned long*)(p+POSITION)
</pre>
<p>wo Datenmüll <code>XXXX</code> im Datum liegt. Das Problem ist hier,
dass das nicht portabel ist und Leute es eventuell erst bemerken,
wenn das Programm nach dem Portieren von <code>ILP32</code> auf
<code>LP64</code> an irgendeiner Stelle abstürzt oder sinnlose
Ergebnisse liefert.

<p>Dass alles nicht schon genug ist, ergibt sich außerdem noch
das Problem, dass die Daten in der Datei im Little-Endian-Format
gespeichert sind, die Computer-Architektur aber die
Big-Endian-Konvention verwenden könnte.

<p>Eine portable Lösung schaut wie folgt aus:
<pre>
  unsigned char* Daten;
  unsinged char* p = Daten+POSITION;
  unsigned long x;
  x = ((unsigned long)p[3] &amp; 0xff)&lt;&lt;24
    | ((unsigned long)p[2] &amp; 0xff)&lt;&lt;16
    | ((unsigned long)p[1] &amp; 0xff)&lt;&lt; 8
    | ((unsigned long)p[0] &amp; 0xff);
</pre>
<p>Zur Optimierung verwendet man ein Makro oder eine Inline-Funktion.
Die high-end-Compiler <code>gcc</code> und <code>clang</code>
beherrschen natürlich das Inlinen von Funktionen. Für billige
Compiler ist ein Makro aber besser.

<p>Falls <code>unsigned char</code>
mit <code>uint8_t</code> koinzidiert, kann <code>&amp; 0xff</code>
entfallen. Das ist eine ganz banale Optimierung. Wenn ein Compiler
das nicht wegoptimiert, solltet ihr den Compiler besser in den
Mülleimer werfen. Man darf diese Optimierung auch für
Embedded-Compiler erwarten, weil dort die Verwendung von
Bitarithmetik maßgeblich ist.

<p>Die Programmiersprache&nbsp;C kann als Mischung aus Hochsprache
und Assembler betrachtet werden. Programmierung in&nbsp;C sollte
zu Gunsten von typsicheren Programmiersprachen möglichst vermieden
werden. Als nächstes wird man sich also fragen, wie das Lesen von
Daten in den effizienten Programmiersprachen Ada, D, Rust und OCaml
ermöglicht wird.

<h2 id="Rust">Gedanken zu Rust</h2>
<div class="date">26. Oktober 2017</div>

<p>Standard ML ist eine strenge, typsichere und mathematisch
außerordentlich reine Programmiersprache.
In Standard ML wurden Theorembeweiser geschrieben.
An Robin Milner wurde 1991 für den Entwurf von Standard ML
der Turing-Award verliehen. Nur eines ist Standard ML nicht:
im Mainstream angekommen. Die Sprache konnte sich nicht
gegen die Effizienz von&nbsp;C behaupten. Verkettete Listen
erzeugen zu viele Speicherallokationen und rekursive Programmierung
ist langsamer als iterative. Solche Probleme lassen sich mit
Zusatzinformationen bis zu einem gewissen Grad wegoptimieren,
aber das kann außerordentlich schwierig werden. Zwar hatte
Standard ML auch Mittel zur iterativen Programmierung, aber diese
stand im Abseits.

<p>In Rust wurde nun das scharfe typtheoretische Modell von
Standard ML und Haskell mit dem iterativen Programmierstil kombiniert.
So gibt es die algebraischen Typkonstruktionen Produkt
(<code>struct</code> wie <code>struct</code> in&nbsp;C,
<code>record</code> in Pascal) und Summe (<code>enum</code>
wie tagged <code>union</code> in&nbsp;C und Pascal). Wenn
die <code>union</code> entfällt, bleibt das Tag übrig, das ist
dann eine einfache <code>enum</code> wie in&nbsp;C. Die Summe
kann man sich als disjunkte Vereinigung von Typen vorstellen,
wenn man sich die Typen als Mengen von zulässigen Werten vorstellt.
Das Produkt entspricht dem kartesischen Produkt.

<p>Während das Produkt von Typen seinen Weg in die klassischen
Programmiersprachen gefunden hat, sind typsichere
Tagged Unions bei den meisten Sprachen leider nicht möglich.
Zur Verwendung von Tagged Unions ist es notwendig, diese
wieder typsicher in eine Fallunterscheidung aufzuspalten.
Dafür wird eine extra Kontrollstruktur benötigt, die es
nur in Algol 68, Rust und den funktionalen Sprachen gibt.
In Java sind aus Gründen der Typsicherheit nun Tagged Unions
nicht möglich &mdash; das Verhalten wird durch Laufzeitpolymorphie
simuliert, was zu außerordentlich vielen Speicherallokationen,
Cache-unfreundlichen Indirektionen und damit scheußlichem
Laufzeitverhalten führt. Man versucht, das Problem durch
Verwendung eines Garbage-Kollektors leicht zu mildern.

<p>Auch das System zur Ausnahmebehandlung arbeitet in Rust
mit Summentypen. Der Grundgedanke ist, dass im Falle eines
Fehlers von einer Funktion ein Fehlerwert zurückgegeben
wird, der außerhalb der Menge der normalen Werte liegt.
Um ein solches Vorgehen zuverlässig zu machen, wird wieder eine
disjunkte Vereinigung benutzt. Die dafür normalerweise
vorgesehenen Summentypen sind <code>Option&lt;T&gt;</code>
und <code>Result&lt;T,E&gt;</code>, man kann sich aber auch
eigene Summentypen definieren. Bei C++ weiß man nie so genau
was hinter <code>try-catch</code> steckt, welche Laufzeitkosten
entstehen, wie komplex das intern arbeitet, und welche Programmaufrufe
eine Ausnahme werfen könnten. Bei einer Systemprogrammiersprache mag
man solches eigentlich nicht. Aus diesem Grund ist der Ansatz mit
Summentypen sehr viel eleganter.

<p>Das System <i>Scope, Ownership, Lifetime, Borrowing, Mutability</i>
ist sehr ausgeklügelt, auch wenn die formalen Regeln des Borrow
Checkers noch nicht zu 100% ausgearbeitet worden sind. In den meisten
Fällen genügt das System tatsächlich. In einer speziellen Situation
reichten mir Lebenszeitangaben nicht aus, so dass ich einen
Arena-Allokator benötigt hätte. Die Referenzen auf dynamisch erzeugte
Daten die ein Arena-Objekt zurückgibt, haben die selbe Lebenszeit
wie das Arena-Objekt selbst. Leider ist es nicht möglich, den Datentyp
Arena in typsicherem Rust zu programmieren und in der Standardbibliothek
ist dieser Datentyp auch nicht vorhanden.

<p>Man kann sich aber auch in solchen problematischen Fällen
mit <code>Rc&lt;T&gt;</code>
(ggf. als <code>Rc&lt;RefCell&lt;T&gt;&gt;</code>)
behelfen. Das sind Referenz-zählende Smart-Pointer, die eine
minimalistische Garbage-Collection darstellen.
Das eleganteste bei der Speicherallokation in Rust ist
die automatische Deallokation und der automatische Aufruf
von Destruktoren am Ende der Lebenszeit von Objekten, ein System
das schon erfolgreich in C++ eingebaut wurde.
Die Lebenszeit von Objekten endet immer dann, wenn sie ihren Scope
verlassen und keine Ownership-Übertrag an eine andere Variable
stattgefunden hat. Das gilt auch für die Smart-Pointer: Die
Lebenszeit des Objektes, auf das die Pointer zeigen, endet genau
dann, wenn die Lebenszeit aller Pointer vorbei ist.

<p>Man muss sich mal vor Augen führen dass solche Referenz-zählenden
Zeiger eigentlich unheimlich performant sind. Zumal beim
bloßen Borrowing nicht am Referenz-Zähler herumgespielt werden
muss, es sei denn man benötigt verändliche Borrows, die durch
<code>RefCell&lt;T&gt;</code> modeliert werden. Selbst wenn das
Programm dann 1% langsamer wird oder 1% mehr
Energie verbraucht, ist das eigentlich nicht weiter schlimm.
Für große Objekte, aus denen eine Menge Inhalt ausgelsen werden
soll, sind die Referenz-Zähler eigentlich
zu vernachlässen. Performance-Einbußen treten erst bei Feldern von sehr
feinen kleinen Datenobjekten auf. In einem solchen Fall sollte man
sowieso möglichst mit Summentypen arbeiten um unnötige
Speicherallokationen zu vermeiden.

<p>Momentan gibt es in Rust ein kleineres Problem mit
<i>Code Bloat</i>, der bei generischen Algorithmen auftritt.
Zum Beispiel wird der Maschinencode beim Datentyp
<code>Vec&lt;T&gt;</code> für jeden Typ <code>T</code> neu erzeugt,
was als Monomorphisation oder automatische Spezialisierung
bezeichnet wird.
Tatsächlich ist es aber möglich, die internen Algorithmen polymorph
zu schreiben, indem sie nur den Speicher per Shallow Copy kopieren.
Nur das Interface wird generisch ausgelegt, was aber eine
Zero-Cost Abstraktion darstellt. Für die Laufzeit ist es
natürlich <i>etwas</i> günstiger, wenn die Algorithmen für
jeden Typ <code>T</code> extra spezialisiert
werden. Bei einem Optionstyp <code>Option&lt;T&gt;</code> mit
großen Inhalt muss z.&nbsp;B. nur das Tag kopiert werden, falls die
Option <code>None</code> ist.

<p>Auch <code>Vec&lt;Box&lt;T&gt;&gt;</code> für unterschiedliche
Typen <code>T</code> erzeugt momentan Code Bloat. Das ist natürlich
unschön, da der Maschinencode in diesem Fall identisch sein sollte.
Diese Idee lässt sich aber vernünftig und in mehr Ausdrucksstärke
als allgemeines Prinzip in Rust umsetzen. Die Konstruktion
dazu ist:
<pre class="code indent">
<b>trait</b> Item{}

<b>struct</b> A; <b>impl</b> Item <b>for</b> A{}
<b>struct</b> B; <b>impl</b> Item <b>for</b> B{}

<b>fn</b> main(){
  <b>let</b> vA: Vec&lt;Box&lt;Item&gt;&gt; = vec![Box::new(A), Box::new(A)];
  <b>let</b> vB: Vec&lt;Box&lt;Item&gt;&gt; = vec![Box::new(B), Box::new(B)];
}
</pre>
<p>Auf diese Art ist auch Laufzeit-Polymorphie möglich.
Wenn <code>Item</code> nämlich Methoden-Signaturen enthält
und die Strukturen diese implementieren, werden die
Variablen vom Typ <code>Box&lt;Item&gt;</code> automatisch zu
<i>Fat Pointern</i> die neben einem Zeiger auf die Daten noch
einen Zeiger auf die jeweilige Tabelle virtueller Methoden
enthalten. Für Variablen vom Typ <code>Item</code> entfallen alle
Indirektionen und die Methoden können direkt aufgerufen werden.
Die selben Methoden können somit nach belieben statisch an ein
Objekt oder dynamisch an einen Objekt-Zeiger gebunden werden.
Dieser Ansatz ist außerordentlich elegant und ausgeklügelt,
und ich würde sagen dass das der kanonische Ansatz für eine
performante Programmiersprache ist.

<p>Eine kleine Schwäche die mir noch aufgefallen ist, ist,
dass bis jetzt eine Typumwandlung von <code>&amp;[T]</code>
zu <code>&amp;[T;N]</code> nicht möglich ist. Die Idee ist,
dass in
<pre class="code indent">
<b>fn</b> compose_u32(a: &amp;[u8;4]) &minus;&gt; u32 {
  (a[3] <b>as</b> u32)&lt;&lt;24 | (a[2] <b>as</b> u32)&lt;&lt;16 |
  (a[1] <b>as</b> u32)&lt;&lt; 8 | (a[0] <b>as</b> u32)
}
</pre>
<p>bei den Feld-Zugriffen keine Bereichsprüfung notwendig ist.
Diese Bereichsprüfung würde schon bei
<code>compose_u32(&amp;a[index..index+4])</code> ausgeführt
werden. Aber <code>&amp;a[index..index+4] as &amp;[u8;4]</code> ist
in Rust bis jetzt nicht möglich.

<p>Was Rust m.&nbsp;E. noch fehlt ist ein System zum
maschinengestützten Beweisen von sicher abstrahierten unsafe-Blöcken.
Damit lässt sich die Korrektheit von Alogrithmen, welche
unsafe-Blöcke enthalten, im Nachhinein sicherstellen. Mit
Korrektheit ist hier gemeint, dass es in keinem Fall zu einer
Speicherzugriffs-Verletzung und damit undefiniertem Verhalten
kommen kann, denn jedes undefinierte Verhalten stellt eine
potentielle Sicherheitslücke dar. Es ist ja so, dass
die meisten unsafe-Blöcke keine globalen Auswirkungen
besitzen, sondern lokal in ihren Modulen abgekapselt sind.
Es ist nun zu zeigen dass diese Kapselung in ihrer Gesamtheit
wieder typsicher ist. Wir sprechen von einer typsicheren Abstraktion.

<p>Zusammenfassend ergibt sich für mich, dass Rust das Potenzial
besitzt, C/C++ abzulösen. Falls es nicht Rust selbst ist,
ist es eine Sprache die Rust sehr ähnlich ist und fast die gleichen
Mechanismen besitzt.

<h2 id="Rechenaufwand">Rechenaufwand</h2>
<div class="date">31. Oktober 2017</div>

<p>Beim Nachdenken über Naturkonstanten und Einheitensysteme bin
ich zu einer interssanten Überlegung gekommen. Es ist ja so,
dass das internationale Einheitensytem bald auf Naturkonstanten
umgestellt werden soll. Der physikalisch gebildete Laie könnte sich
daraufhin die Frage stellen, wieso die Einheiten nicht gleich im Bezug
zum plancksche Einheitsystem definiert werden, da dieses doch
dem State of the Art in der Physik entspricht. Der Grund hierfür
liegt natürlich in der großen Unsicherheit des Messungen für
die Gravitationskonstante G. Diese Messunsicherheit überträgt sich
dem Wesen des planckschen Einheitensystems nach auf die wichtigsten
Basiseinheiten des internationalen Einheitensystems.
Ein solcher Ansatz ist also praktisch nicht durchfürbar, solange
nicht ein äußerst genauer Wert der Graviationskonstante bekannt ist.

<p>Die klassische Bestimmung der Gravitationskonstante über
eine Gravitationswaage ist zu ungenau. Einen genauen Wert der
Gravitationskonstante über die Bahnparameter von astronomischen
Objekten zu schlussfolgern, steht im Zusammenhang mit der genauen
Kenntnis der Masse eine Planeten. Die Massen der Planeten sind
offensichtlich auch nicht so genau bekannt.

<p>Eigentlich ist die Gravitationskonstante aber doch eine
inhärente Eigenschaft unserer Welt. Somit könnte es doch möglich
sein, alles Wissen über unsere Welt zusammenzunehmen und dann die
Information über die Gravitationskonstante aus diesem Wissen
herauszuprojizieren. Praktisch könnte sich dieser Ansatz wie
folgt gestalten: Man verändert die Gravitationskonstante um einen
winzigen Bruchteil, belässt aber alle anderen Naturkonstanten bei
ihrem bekannten Wert. Hierzu wird eine physikalische Simulation
durchgeführt und mit unserem Universum verglichen. Stellt man dabei
irgendwann eine Abweichung fest, müsste ein fehlerhafter Wert der
Gravitationskonstante vorliegen.

<p>Wir legen nun alle anderen aktuellen physikalischen Probleme
beiseite und stellen uns die Frage, warum ein solcher Ansatz nicht
durchführbar ist, auch wenn wir die Welt hypothetisch genau verstehen
würden.

<p>Der Kern des Problemes liegt in der Simulation eines <i>dynamischen
Systems</i>, die zuviel Rechenaufwand benötigt. Es ist so,
dass dynamische Systeme exakt, einschließlich exaktem Anfangszustand,
festgelegt werden können. Durch Simulation können nun die Strukturen,
welche das System entwickelt (besitzt, schon immer bessen hat,
siehe <i>laplacescher Dämon</i> etc.),
beschaut werden. Tatsächlich ist die Physik von diesem Problem
durchsiebt. Eine Simulation ist immer bei Unkenntnis
der anlytischen Lösung der Differentialgleichung, durch
welche das System beschrieben wird, notwendig. Aktuelle Beispiele
sind Simulationen der theoretischen Chemie und Proteinfaltungen.

<p>Da es mit der Gravitationskonstante viel zu kompliziert ist,
beschäftiget man sich am besten zuerst mit etwas einfacherem.
Z.&nbsp;B. sind die Eigenschaften des Wasserstoffatoms
analytisch berechenbar. Wenn zusätzliche Elektronen ins Spiel kommen,
kommt aber zunächst nur die numerische Lösung in Frage.

<p>Auch wenn man nur eine bestimmte Frequenz des Spektrums wissen
möchte, ist dafür die numerische Simulation des gesamten Systems
notwendig. Muss das so sein? Ein Trick könnte doch eine
Berechnung unnötig machen oder zumindest den Rechenaufwand drastisch
veringern? Könnte man die Frequenz nicht mathematisch vorhersagen,
ohne die vollständige analytische Lösung zu besitzen?

<p>Das große Problem hierbei ist, dass alle Teilzustände des
Systems gekoppelt sind und gegenseitigen Einfluss auf sich
ausüben. Manchmal ist es möglich bei schwachem Einfluss auf
Berechnungen zu verzichten. Ein Beispiel wo dieser Ansatz zum
Erfolg führt, ist der Barnes-Hut-Algorithmus.


<h2 id="SVG-und-Fonts">SVG und Fonts</h2>
<div class="date">23. Dezember 2017</div>

<p>Das SVG-Format ist ja bis jetzt überhaupt noch nicht
benutzbar, finde ich. Etwa gefühlt jede zweite SVG-Datei enthält doch
auch Text. Zum Rendern des Textes muss der gewählte Font verfügbar
sein, was aber nicht gewährleistet werden kann.

<p>Die erste Lösung die man gewählt hat, ist die Umwandlung der Glyphen
in Pfade an Ort und Stelle. Das ist so ziemlich die kurzsichtigste
Lösung auf die man kommen kann. Da die Glyphen immer wieder in Pfade
umgewandelt werden, erzeugt man Bloat, total unelegant. Außerdem lassen
sich die Texte danach nicht mehr bearbeiten, nicht mehr kopieren,
und das Verzeichnis lässt sich nicht mehr nach Text durchsuchen.
Man muss an der Stelle eigentlich mal ganz laut sagen, »so einen Mist
will ich nicht«, aber alle akzeptieren es.

<p>Besser wäre die Einbettung des Fonts in die SVG-Datei.
Hat man auch vorgesehen, es heißt »SVG fonts«. Aus Chrome hat man
es wieder ausgebaut, weil die da zu nichts fähig sind, und bei
Firefox wurde es auf unbestimmte Zeit verschoben.

<p>Jetzt will man dafür das neue Format WOFF benutzen, was im
Wesentlichen einen komprimierenden Container für TTF-Dateien
darstellt. Es mag ja sein, dass WOFF für sich sinnvoll ist und
gewisse Probleme löst. Aber dann müsste man ja so einen WOFF-Blob in
die SVG-Datei einbetten und hat wieder Bloat, oder aber man setzt
einen Link auf die WOFF-Datei und hat Dependency-Hell. Jedenfalls
kommt man irgendwie zum Schluss dass das hinten und forne nicht
funktioniert.

<p>Der vernünftige Ansatz schaut wie folgt aus. Wie bei SVG fonts
gehört zu jedem Buchstaben des Textes ein Glyph für den ein
Pfad vorliegt. So Dinge wie Kerning lassen wir jetzt einfach mal
unberücksichtigt. Jeder Buchstabe ist wie ein Klon des Glyphen.
Der Vorteil: Man kann nun einen Buchstaben unter der Lupe betrachten
und dort den Pfad verändern. Damit verändern sich der selbe
Buchstabe auch an allen anderen Stellen. Möchte man nur einen Text
manipulieren, dann erzeugt man einfach eine Kopie der Glyphen.

<p>Ich finde jetzt nicht, dass das besonders viel Speicher kostet,
der Pfad für einen Glyphen ist nicht allzu groß. Wenn man jetzt
schon einen SVG-Renderer geschrieben hat, warum geht da nicht
auch SVG fonts? Da nimmt man den Pfad, führt eine Transformation
aus und schiebt die Position um <code>horiz-adv-x</code> weiter,
lassen wir Unicode features mal unberücksichtigt. 

<p>Die müssen sich da alle mal am Kopf fassen. Die bekommen ja
garnichts auf die Reihe. Das ist irgendwie so wie beim schmalen
geschützten Leerzeichen. Da gibt es heute noch keine Entität für.
Einfach  <code>&amp;nbts;</code> für <i>non-breaking thin space</i>
und gut ist. Aber nein, da muss man ja ein halbes Jahrhundert
drauf warten.


<h2 id="Drachen-und-Zombies">Drachen und Zombies</h2>
<div class="date">21. Januar 2018</div>

<p>Vor einer Weile habe ich mich gefragt, wie einige
Schauergeschichten und mythische Wesen wohl zustandegekommen sind.
Meine Überlegung dabei ist, dass die Leute damals nicht auf den
Kopf gefallen waren, und viele von ihnen nicht leicht abergläubig
wurden. Um Leute so richtig von etwas zu überzeugen, so dass es
sich noch ein Jahrhundert später rumspricht, benötigt man mehrere
Zeugen.

<p>Bei der Vorstellung von Drachen ist es vielleicht so gewesen,
dass Skelette von Dinosauriern, vielleicht sogar Flugsauriern
gefunden wurden. Die Leute müssen dann auch dazu fähig gewesen sein,
die Skelette durch Vergleich mit anderen Tierskeletten zu analysieren.

<p>Die Vorstellung von Zombies und Vampiren rührt vielleicht von
Tollwut her. Wenn die Leute Angst vor so etwas bekamen, war es nur
gut. Tollwut ist heute noch sehr gefährlich. Nach einem Biss ist
sofort eine postexpositionelle Prophylaxe notwendig, andernfalls
verläuft die Krankheit fast immer tödlich. Wurde man damals von einem
tollwütigen Tier gebissen, war es bereits zu spät.

<p>Man muss dem entgegenhalten, dass einige Vorstellungen erst
spät, ich meine etwa ab dem 19. Jhd., entwickelt wurden.
Das müsste man beim Zombie-Motiv recherchieren.
Das wiederkehrende Zombie-Motiv besagt, dass Menschen nach einem
Biss von einem Zombie selbst zu einem Zombie werden. Hierdurch
wird aber die Tollwut genau charakterisiert.


<h2 id="GCHQ-TLS">GCHQ murrt gegen TLS-1.3</h2>
<div class="date">13. März 2018</div>

<p>Wenn ihr bei TLS 1.3 von der Diskussion um die Middleboxen
gehört habt, hier ist mal ein aktuelles Beispiel wo ich das erklären
will. Das GCHQ vermeldet:<sup><a href="#GCHQ-Blog-post">[1]</a></sup>

<blockquote>
The challenge is that these protections will also make the enterprise
security model much, much harder. There’s two specific things
that I think will have a negative impact on enterprise security.
<br><br>
The first is that it’s impossible to whitelist sites anymore because
server certificates (the things that authenticate a site) are encrypted.
So, your appliance will be unable to work out (for example) whether
you’re communicating with your bank, or if malware on your machine is
talking to its criminal masters, without breaking the connection.
That wouldn't be a problem if you could break the start of a connection
and then drop out when you find out it’s a low risk (or sensitive site).
But that brings us to the second problem; you can’t do this in
TLS 1.3. Once you proxy a connection, you have to proxy it
until it’s done.
</blockquote>

<p>Nun ist der rationale Gedankengang beim Entwurf eines Kryptosystems
von folgender Gestalt. Ein System nach dem State of the Art sieht
spontanen Verbindungsaufbau mit DH-Schlüsseltausch und Perfect Forward
Secrecy vor. So Geschichten mit RSA könnt ihr in die Tonne werfen,
das taugt nur noch für Zertifikate. Man verifiziert seine Identität
dabei am besten nach dem DH-Schlüsseltausch und nicht davor.

<p>Die Argumentation des GCHQ ist nun, dass alle Computer so dermaßen
von Malware verseucht sind, dass jedes Socket vor der Benutzung
erstmal vom lokalen Proxy verifiziert werden muss. Natürlich überlegt
man sich jetzt, ob das nicht auch ohne Zertifikate vor dem
Schlüsseltausch möglich ist, wenn man die feste IP überprüft.
Das ist aber nicht sicher,
weil ein Router dazwischen gehen kann und das Paket irgendwo anders
hin leiten. Nach dem Schlüsseltausch verifizieren geht auch nicht,
weil dabei schon eine Verbindung stehen muss und die Malware dann
bereits alle möglichen Dinge sendet.

<p>Den letzten Satz, »Once you proxy a connection, you have to proxy it
until it’s done«, seh ich aber nicht ein. Warum kann der Proxy nicht
die Verbindung aufbauen und den Client dann zu einem Web of Trust
mit dem Server einladen? Vielleicht müsste da eine Protokollerweiterung
vorgenommen werden. Aber ich kann keinen fundamentalen Grund finden,
warum man so ein Modell nicht umsetzen kann.

<p>Außerdem bin ich von der Sicherheit von Proxys nicht überzeugt.
Es wäre doch möglich, dass Router die Daten, welche an einen
Server gehen sollen, verzweigen. Um das Aussickern von ungewollten
Daten in einer unsicheren Umgebung zu verhindern, müssten die Daten
jedoch durch einen sicheren Flaschenhals geleitet werden (den
lokalen Proxy), wo eine Überprüfung des kryptologischen Hashwerts
der Daten vorgenommen wird. Vorausgesetzt das Tool auf dem Client,
das den Hashwert erzeugt und an den Proxy sendet, ist nicht auch schon
verseucht.

<p>Und was sagt uns TLS 1.3 laut GCHQ da? Dass die Verbindung
über den lokalen Proxy bis zum Ende laufen muss. Vielleicht ist
das ein Indiz dafür, dass das der einzige Weg ist, das zugrundliegende
mathematische Modell zu erfüllen.

<p>Am besten man entpackt die Daten und schaut sich die auf dem
lokalen Proxy manuell Byte für Byte an. Das ist natürlich unrealistisch.
Was ich daraus auch schließen kann ist: Sicherheit nach
Kompromittierung zu bieten ist für mich sehr schwierig bis unmöglich,
auch wenn es Überprüfungen an sicheren Flaschenhälsen gibt.

<ul>
<li id="GCHQ-Blog-post">
  [1] »<a href="https://www.ncsc.gov.uk/blog-post/tls-13-better-individuals-harder-enterprises"
  >TLS 1.3: better for individuals &ndash; harder for enterprises</a>«.
  In: GCHQ NCSC Blog post (9. März 2018).
<li>[2] Jürgen Schmidt:
  »<a href="https://www.heise.de/security/meldung/Verschluesselung-GCHQ-kritisiert-die-kommende-TLS-Version-3991653.html"
  >Verschlüsselung: GCHQ kritisiert die kommende TLS-Version</a>«.
  In: Heise online (13. März 2018).
</ul>

<h2 id="Strukturelle-Typisierung">Strukturelle Typisierung in Rust</h2>
<div class="date">23. März 2018</div>

<p>In Rust geht das folgende:
<pre class="code indent">
<b>struct</b> PN{x: i32, y: i32};
<b>type</b> PS = (i32,i32);
</pre>

<p>Während der Typ <code>PN</code> nomial typisiert ist, ist
<code>PS</code> als anonymer Produkttyp strukturell typisiert.
Prinzipiell müsste dann auch folgendes möglich sein:
<pre class="code indent">
<b>enum</b> EN{x: i32, y: i32};
<b>type</b> ES = <b>enum</b>{i32,i32};
</pre>
<p>Die letzte Zeile existiert in Rust aber bisweilen nicht.
Richtige strukturelle Typisierung würde nun aber auch
gerne Isomorphien wie z.&nbsp;B.
<pre class="indent">
(i32,(i32,i32)) == ((i32,i32),i32) == (i32,i32,i32)
</pre>
durchsetzen. Man fragt sich jetzt wie das durchgesetzt werden soll,
wenn man es eigentlich verbieten will. Hier ergibt sich für mich
die Notwendigkeit von Funktionen zwischen Typen:
<pre class="indent">
flatten&lt;(i32,(i32,i32))&gt; == (i32,i32,i32)
</pre>
<p>Hier ordnet <code>flatten</code> den Tupeln über die Isomorphie
den Standard-Repräsentanten der Äquivalenzklasse zu, erzeugt
also eine Normalform. Weiterhin ist eine Funktion denkbar,
die einen nominalen Typen auf seinen strukturellen runterprojiziert.
Das ist aber irgendwie auch problematisch, weil jetzt nicht
klar ist, ob man auch die Indexmenge (die inneren Variablen)
auf die natürlichen Zahlen runterprojiziert, z.&nbsp;B. per
lexikographischer Anordnung. Das führt nicht zu ineffizientem
Maschinencode, weil es mit der Erzeugung von Maschinencode ausdrücklich
noch nichts zu tun hat. Eine Umordnung der Elemente und Padding
kommt erst ganz zum Schluss. Allerdings ist das für die Praxis
höchst unergnomisch. Konsequenterweise benötigt man auch noch
unbenannte Strukturen, wobei man die Indexmenge aber zum
Bestandteil des strukturellen Typen macht.

<p>Eine Frage die sich mir auch stellt, ist, ob sich ein solches
hypothetisches strukturelles Typsystem so ausdrucksstark formulieren
lässt, dass damit das Template-System von C++ modelliert werden kann.

<p>Die nächste Frage ist, ob das überhaupt für irgendwas von
Vorteil ist. Man ist in der Anfangsphase von Rust auch
von der strukturellen zur nominellen Typisierung gewechselt,
weil so Probleme vermieden werden konnten<sup>[1]</sup>.
Dass nominelle Typisierung für die typsichere Programmierung
konstitutiv ist, ist natürlich klar.

<ul>
<li>[1] Diskussion
  »<a href="https://internals.rust-lang.org/t/why-were-structural-records-removed/1553"
  >Why were structural records removed?</a>«. In: »Rust Internals«
  Forum (15. Feb. 2015).
</ul>


<h2 id="Wikipedia">Wikipedia ist überholt</h2>
<div class="date">21. April 2018</div>

<p>Man kann Wikipedia noch so toll finden, aus einer vernünftigen
Sicht ist es überholt. Es folgt eine Liste von Gründen,
die dafür sprechen. 

<ul>
<li>Mediawiki ist in PHP geschrieben. Es wäre sinnvoller, man
  würde eine neue Wiki-Software in einer statisch typisierten
  Programmiersprache schreiben.
<li>Die Syntax für Vorlagen soll wohl Lisp-Ausdrücke nachbilden,
  bloß mit geschweiften anstelle von runden Klammern. Allerdings
  hat man bei der Modellierung den Fehler gemacht, dass Tabellen
  innerhalb von Vorlagen nicht erlaubt sind. Es gibt zwar ein
  Escape-System, das ist aber sehr unelegant.
<li>Style sheets in Vorlagen produzieren schlechtes HTML. Man will
  eigentlich Klassenangaben in den Vorlagen und dann am Anfang der Seite
  hinzufügungen zum Style sheet.
<li>Die Artikel sind so lizensiert, dass bei Kopien die Autoren
  angegeben werden müssen. Bei einer so großen Anzahl von Autoren
  erscheint mir das irrsinnig. Es wäre besser, man würde die Artikel
  neu schreiben und Creative Commons CC0 lizensieren. Davon hätten
  alle mehr.
<li>Das Kategoriensystem ist historisch gewachsen, wobei man
  einige Fehler gemacht hat. Z.&nbsp;B. gibt es Kategorien
  »Film über Thematik X« und »Serie über Thematik X«.
  Es wäre sinnvoller, man würde in die Facetten »Film«, »Serie«
  und »Thematik X« aufsplitten. Dann gibt es noch das Problem,
  dass es überlange Kategorien nicht geben soll. Man sortiert dann
  in Unterkategorien ein, prinzipiell gut und richtig. Aber es müsste
  bei der Suche auch Mengenoperationen geben, das geht bisher nur
  mit externer Software. Außerdem ergibt sich das Problem, dass sich
  aus dem Titel einer Unterkategorie nicht immer die Titel der
  Oberkategorien ableiten lassen. Dann weiß man nicht, ob ein Artikel
  schon in einer bestimme Kategorie einsortiert wurde, oder nicht.
<li>Man müsste das Kategoriensystem feiner gestalten und so
  modellieren, dass man es auch als Schlagwortkatalog nutzen kann.
<li>Man müsste die Suche bezüglich Kategorien bzw. Facetten mit
  einer Volltextsuche koppeln können.
<li>Die Relevanzkriterien sind für ein elektronisches Medium nicht
  sinnvoll. Man könnte jedem Artikel einfach ein Relevanzattribut
  geben, so wie man ihn in eine bestimmte Kategorie einsortiert.
  Dann lassen sich mit dem Datenbanksystem irrelevante Artikel einfach
  ausfiltern. 
</ul>

<h2 id="Backpulver">Backpulver</h2>
<div class="date">06. August 2018</div>

<p>Hab gestern mal Backpulver für Mürbeteigkeksbrötchen benutzt.
Und danach eine Cola getrunken, was ich normalerweise nicht mache.
Ganz widerliches Zeug. Die Cola allein macht die
Zähne weich. Aber das Backpulver ist irgendwie noch viel schlimmer,
das macht euch den Zahnschmelz kaputt dass es sich rau anfühlt.
Wie wenn ihr euch ewig lange die Zähne geputzt habt oder Zahnstein
entfernen lassen wart. Außerdem waren die Keksbrötchen etwas salzig
geworden. In der Inhaltsangabe steht Natriumcarbonate als Triebmittel,
Diphosphate als Säuerungsmittel und Maisstärke als Trennmittel.

<p>Kann ja sein dass ich etwas falsch gemacht habe und ihr damit
super klarkommt. Ich mag aber erst einmal lieber ohne Backpulver backen.

<h2 id="Spulenfiepen">Spulenfiepen</h2>
<div class="date">15. Oktober 2018</div>

<p>Vor einiger Zeit habe ich einen Laptop von Asus, auf dem ich auch
gerade diesen Text schreibe, mehr oder weniger Geschenk bekommen.
Das Gerät ist eigentlich für meine Zwecke eigentlich ganz in Ordnung,
abgesehen davon dass das DVD-Laufwerk keinen Brenner enthält.
Es ist nun so, dass es beim Bewegen der Maus oder beim gedrückthalten
einer Taste zu Spulenfiepen kommt, was mich aber nie gestört hat.
Das Geräusch ist relativ leise und klingt auch eher wie ein Rattern.

<p>Nach einem Betriebsystemupgrade von Ubuntu 16 auf 18 hat sich
aber nun ein surrender Ton im Leerlauf eingestellt, der klingt wie
eine leise schnarrende Violine auf H6 (1975 Hz). Das ist nicht so
häßlich wie hochfrequenter Tinnitus, aber dennoch ausgesprochen nervig
wenn man gerade etwas liest oder schreibt.

<p>Als erstes hab ich gedacht, das neue System würde krampfhaft
versuchen Energie zu sparen und daher die Prozessorleistung nach
unten zu drücken. Ein Blick in die Systemüberwachung hat aber gezeigt
dass das Gegenteil der Fall ist. Die beiden Kerne waren ständig
ausgelastet, also 40% und 100% im Tauschwechsel. Ich dachte nun,
es müsse am hungrigen GNOME3 liegen und hab das dann wieder
gegen Unity ausgetauscht. Das GNOME3 ist schon ein bisschen gefräßig,
beim bewegen eines Fenster bricht mir die Framerate ein, fühlt sich
alles nicht richtig flüssig an. Nun war das Fiepen auf einem Benutzerkonto
fort, auf dem anderem aber noch vorhanden. Ein Blick in
<code>top</code> zeigte mir nun, dass <code>baloo_file</code>
mit 15% und <code>baloo_file+</code> mit 90% im Hintergrund laufen.
Das ist so ein schrottes Suchprogramm für Dateisuche. Die
Hintergrundprozesse sollen wohl nebenbei einen größeren Index
aufbauen. Normalerweise sollen die nach wenigen Minuten fertig
sein, bei mir lief das aber schon Stunden.

<p>Bevor ich die ausgeschaltet habe, lief Unity auch nicht stabil.
Nach dem öffnen eines Texteditors hat der das Fenster angefangen
zu flackern, wenn man den Mauszeiger ganz nach unten bewegt hat.
Total seltsam. Könnte auch eine Koinzidenz sein. Mich interessiert
aber trotzdem wo das herkommt. Vermutlich hat irgendjemand
Multithreading verkackt und dann ist da so eine Race condition
die irgendwo Müll reinschreibt oder zu einem Puffüberlauf führt.
Nur Spekulation.

<h2 id="Mykro">Mykro</h2>
<div class="date">09. Dezember 2018</div>

<p>Beim Lehrpersonal hab ich immer öfter die Aussprache Mykro gehört,
z.B. Mykrofarad. Das ist mir auch bei Harald Lesch aufgefallen. Ich
glaub jetzt ja irgendwie, die halbe Wissenschaftler-Community ist davon
befallen. Natürlich handelt es sich dabei um Pseudo-Griechisch.

<p>Nun, ich bin nicht der einzige der das so sieht.

<pre class="indent">
<b>Mykroprozessor</b>
Das höre ich leider immer öfter in Vorträgen von
Doktoranden. Natürlich verwenden manche Leute die
Abkürzung »μP« für den Mikroprozessor und natürlich
heißt der Buchstabe »My«, aber das ändert nichts
daran, dass das hier verwendete Wort μικρὸς mit
einem »i« ist.
-- Prof. Dr. Herbert Klaeren:
   »Ansichten eines Programmierers«
</pre>


<h2 id="Absurder-Speicherverbrauch">Absurder Speicherverbrauch</h2>
<div class="date">12. Dezember 2018</div>

<p>Es ist ja so, dass eine PDF-Datei mit Times New Roman viel kleiner
ist als eine mit Latin Modern. Der Grund dafür kann doch nur darin
liegen, dass Times New Roman eine der in Postscript standardmäßig
verfügbaren Schriftarten ist und demnach nicht in die PDF-Datei
eingebettet werden muss. Bei der Menge an wissenschaftlichen
Dokumenten fragt man sich, warum man nicht auch Latin Modern und einige
andere wie Linux Libertine zu dieser Menge hinzufügt.

<p>Das Thema hat, denke ich, schon eine sehr starke praktische
Relevanz. Der Faktor kann nämlich bei bis zu 10 liegen. Aktuell soll
das gesamte Archiv von SciHub etwa 55TB groß sein, wobei die
PDF-Dateien zu zip-Dateien gebündelt wurden. Man stelle sich nun mal
vor, das könnte auf 6TB reduziert werden. Mit RAID auf drei Festplatten
ergäben sich 18TB. Momentan kosten 4TB ca. 100 EUR. Demnach könnte
man eine Kopie für ca. 600 EUR speichern.

<p>Würde man Quelltexte in LaTeX2e speichern, ergäben sich vielleicht
sogar nur 200GB. Demanch könnte man eine Kopie mit RAID auf fünf
Festplatten für ca. 200 EUR haben.

<p>Letztendlich kostet die Herstellung von Festplatten Energie und
Ressourcen die man gerne sparen würde. Außerdem lassen sich Quelltexte
schneller mit <code>grep</code> durchsuchen, demnach lässt sich auch
schneller ein Index zur Volltextsuche erstellen. Mein eigenes Archiv
liegt jedenfalls in Form von Quelltext vor.

<h2 id="Allgemeinheitsthese">Allgemeinheitsthese</h2>
<div class="date">4. April 2019</div>

<p>Zufälligerweise macht die Physik immer genau von einer Reihe
praktischer mathematischer Theorien gebrauch. Das sind in etwa
die Analysis, die Vektorrechnung bzw. allg. die lineare Algebra,
die Vektoranalysis bzw. allg. die Differentialgeometrie,
die Variationsrechnung, die Funktionalanalysis, die
Wahrscheinlichkeitstheorie.

<p>Warum ist die uns umgebende Wirklichkeit so kompliziert, dass all
dies benötigt wird? Meine These ist zunächst, dass diese mathematischen
Werkzeuge allgemeiner Natur sind. Z.&nbsp;B. verallgemeinern
Mannigfaltigkeiten Koordinatenräume und die darauf definierbaren
Konzepte wie dynamische Systeme. Denkt man sich nun irgendeinen
beliebigen Raum, dann ist dieser sicher nicht flach, sondern irgendwie
gekrümmt.

<p>Die Idee ist nun, dass die Beschreibung einer in sich konsistenten
Welt zwangsläufig höhere mathematische Mittel erfordern würde.
Wenn die Welt semi-kontinuierlich sein soll, muss Differentialgeometrie
und Funktionalanalysis enthalten sein, einfach weil dies die
allgemeinsten mathematischen Theorien zur Beschreibung kontinuierlicher
Strukturen sind.

<p>Um die Rätsel der Physik zu lösen, müsste man demnach nur einen
noch reichhaltigeren mathematischen Apparat haben, der dann ganz
automatisch ein tiefgreifenderes Verständnis und tiefgreifendere
Ansätze ermöglicht.

<p>Das wirkt jetzt etwas tautologisch, enthält aber unter der
Oberfläche ein respektloses induktives Verfahren zur Schlussfolgerung.
Der Gedankengang ist, dass sich ein mathematisches
Objekt oft auf ausgezeichnete Art als Speziallfall oder Approximation
eines allgemeineren angeben lässt. Z.&nbsp;B. ist eine quadratische
Funktion die lokale Approxmation einer beliebigen differenzierbaren
Funktion. Man kann es jetzt seltsam finden, dass die Welt mit einer
gewissen Güte immer durch quadratischen Funktionen beschrieben werden
kann. Das ist jedoch nur die lokale Approximation nichtlinearer
Vorgänge.

<p>Analog mag es seltsam erscheinen dass die moderne Physik so viele
Feldtheorien verwendet. Das muss aber nicht unbedingt bedeuten dass
die Welt auf ganz spezielle Art aufgebaut ist. Es kann schlicht sein,
dass es ein mathematisches Prinzip gibt, welches diese Feldtheorien
erzwingt, wenn das Ziel eine einigermaßen vernünftige approximative
Beschreibung sein soll.

<p>Anderes Beispiel, die spezielle Relativitätstheorie. Warum so und
nicht anders? Nun, es ist doch nur eine der wenigen möglichen
Geometrien. Der Minkowskiraum ist dabei auch nur die tangentiale
Approximation einer Mannigfaltigkeit.

<p>Vielleicht ist die Welt nur die emergente Erscheinung die sich
gemäß eines bestimmten Systems ergibt. Die vorfindbaren Strukturen
sind nicht Speziell für dieses System, sindern finden sich bei
vielen anderen Systemen auch. Wo findet man denn Symmetrien?
Die findet man doch überall. Dass die Eichtheorien mit kontinuierlichen
Symmetriegruppen arbeiten kann doch kein Zufall sein, wenn Symmetrien
erstens überall zu finden sind, und zweitens kontinuierliche Strukturen
enthalten sein sollen, die wohl dann auch überall zu finden sind.

<p>Tatsächlich beeindruckend wäre eine holistische kosmologische
Theorie, nach der die mikroskopischen Vorgänge das Ergebnis bestimmter
topologischer, globaler und informationstheoretischer Postulate
sind.

<p>Jedenfalls ergibt sich für mich, dass die allgemeinen mathematischen
Eigenschaften von Eichtheorien vielleicht für das Verständnis
wichtiger sind als die speziellen Theorien der Physik. Mir scheint,
dass man sich zu wenig auf die mathematisch allgemeingültigen
Zusammenhänge konzentriert und deshalb nicht voran kommt.

</main>

<aside id="right-aside">
</aside>
</div>

</div>

</body>
</html>
